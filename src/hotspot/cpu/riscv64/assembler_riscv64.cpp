/*
 * Copyright (c) 1997, 2012, Oracle and/or its affiliates. All rights reserved.
 * Copyright (c) 2014, Red Hat Inc. All rights reserved.
 * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.
 * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
 *
 * This code is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License version 2 only, as
 * published by the Free Software Foundation.
 *
 * This code is distributed in the hope that it will be useful, but WITHOUT
 * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 * version 2 for more details (a copy is included in the LICENSE file that
 * accompanied this code).
 *
 * You should have received a copy of the GNU General Public License version
 * 2 along with this work; if not, write to the Free Software Foundation,
 * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 *
 * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 * or visit www.oracle.com if you need additional information or have any
 * questions.
 */

#include <stdio.h>
#include <sys/types.h>

#include "precompiled.hpp"
#include "asm/assembler.hpp"
#include "asm/assembler.inline.hpp"
#include "compiler/disassembler.hpp"
#include "interpreter/interpreter.hpp"
#include "memory/resourceArea.hpp"
#include "runtime/interfaceSupport.inline.hpp"
#include "runtime/sharedRuntime.hpp"

extern "C" void test_assembler_entry(CodeBuffer *cb);

#define __ _masm.

#ifdef ASSERT
static void asm_check(const unsigned int *insns, const unsigned int *insns1, size_t len) {
  assert_cond(insns != NULL && insns1 != NULL);
  bool ok = true;
  for (unsigned int i = 0; i < len; i++) {
    if (insns[i] != insns1[i]) {
      ok = false;
      printf("Ours:\n");
      Disassembler::decode((address)&insns1[i], (address)&insns1[i+1]);
      printf("Theirs:\n");
      Disassembler::decode((address)&insns[i], (address)&insns[i+1]);
      printf("\n");
    }
  }
  assert(ok, "Assembler smoke test failed");
}
#endif

void test_assembler_entry(CodeBuffer *cb) {
  MacroAssembler _masm(cb);
  address entry = __ pc();

#ifdef ASSERT
// BEGIN  Generated code -- do not edit
// Generated by riscv64-asmtest.py
    Label back, forth;
    __ bind(back);

    // ArithOp
    __ add(x5, x28, x15);                              // add  x5, x28, x15
    __ sub(x23, x30, x14);                             // sub  x23, x30, x14
    __ addw(x18, x27, x17);                            // addw  x18, x27, x17
    __ subw(x10, x31, x15);                            // subw  x10, x31, x15
    __ orr(x11, x22, x10);                             // or  x11, x22, x10
    __ xorr(x14, x31, x26);                            // xor  x14, x31, x26
    __ mul(x18, x24, x28);                             // mul  x18, x24, x28
    __ mulh(x7, x8, x14);                              // mulh  x7, x8, x14
    __ mulhsu(x14, x21, x22);                          // mulhsu  x14, x21, x22
    __ mulhu(x20, x15, x5);                            // mulhu  x20, x15, x5
    __ div(x6, x5, x9);                                // div  x6, x5, x9
    __ divu(x10, x17, x29);                            // divu  x10, x17, x29
    __ rem(x15, x5, x23);                              // rem  x15, x5, x23
    __ remu(x31, x15, x21);                            // remu  x31, x15, x21
    __ mulw(x20, x29, x27);                            // mulw  x20, x29, x27
    __ divw(x20, x11, x21);                            // divw  x20, x11, x21
    __ divuw(x7, x23, x15);                            // divuw  x7, x23, x15
    __ remw(x17, x25, x29);                            // remw  x17, x25, x29
    __ remuw(x20, x14, x5);                            // remuw  x20, x14, x5
    __ andr(x20, x12, x25);                            // and  x20, x12, x25

    // TwoRegImmedOp
    __ addi(x31, x12, 1494);                           // addi  x31, x12, 1494
    __ addiw(x6, x21, -1996);                          // addiw  x6, x21, -1996
    __ ori(x27, x28, -1047);                           // ori  x27, x28, -1047
    __ xori(x22, x21, -544);                           // xori  x22, x21, -544
    __ andi(x26, x22, 495);                            // andi  x26, x22, 495
    __ slti(x19, x27, -154);                           // slti  x19, x27, -154
    __ jalr(x20, x7, 1174);                            // jalr  x20, x7, 1174

    // TwoRegUnsignedImmedOp
    __ sltiu(x25, x5, 509u);                           // sltiu  x25, x5, 509

    // AbsOp
    __ j(__ pc());                                     // j  .
    __ j(back);                                        // j  back
    __ j(forth);                                       // j  forth
    __ jal(__ pc());                                   // jal  .
    __ jal(back);                                      // jal  back
    __ jal(forth);                                     // jal  forth

    // LoadImmedOp
    __ lui(x16, 0xdaf8000);                            // lui  x16, 0xdaf8
    __ auipc(x23, 0x7068c000);                         // auipc  x23, 0x7068c

    // RegAndAbsOp
    __ bnez(x26, __ pc());                             // bnez  x26, .
    __ bnez(x26, back);                                // bnez  x26, back
    __ bnez(x26, forth);                               // bnez  x26, forth
    __ beqz(x31, __ pc());                             // beqz  x31, .
    __ beqz(x31, back);                                // beqz  x31, back
    __ beqz(x31, forth);                               // beqz  x31, forth

    // TwoRegAndAbsOp
    __ bne(x29, x28, __ pc());                         // bne  x29, x28, .
    __ bne(x29, x28, back);                            // bne  x29, x28, back
    __ bne(x29, x28, forth);                           // bne  x29, x28, forth
    __ beq(x20, x27, __ pc());                         // beq  x20, x27, .
    __ beq(x20, x27, back);                            // beq  x20, x27, back
    __ beq(x20, x27, forth);                           // beq  x20, x27, forth
    __ bge(x21, x9, __ pc());                          // bge  x21, x9, .
    __ bge(x21, x9, back);                             // bge  x21, x9, back
    __ bge(x21, x9, forth);                            // bge  x21, x9, forth
    __ bgeu(x12, x30, __ pc());                        // bgeu  x12, x30, .
    __ bgeu(x12, x30, back);                           // bgeu  x12, x30, back
    __ bgeu(x12, x30, forth);                          // bgeu  x12, x30, forth
    __ blt(x17, x28, __ pc());                         // blt  x17, x28, .
    __ blt(x17, x28, back);                            // blt  x17, x28, back
    __ blt(x17, x28, forth);                           // blt  x17, x28, forth
    __ bltu(x21, x25, __ pc());                        // bltu  x21, x25, .
    __ bltu(x21, x25, back);                           // bltu  x21, x25, back
    __ bltu(x21, x25, forth);                          // bltu  x21, x25, forth

    // ShiftRegOp
    __ sll(x31, x25, x6);                              // sll  x31, x25, x6
    __ srl(x7, x27, x18);                              // srl  x7, x27, x18
    __ sra(x29, x13, x21);                             // sra  x29, x13, x21
    __ sraw(x20, x23, x22);                            // sraw  x20, x23, x22
    __ sllw(x31, x25, x22);                            // sllw  x31, x25, x22
    __ srlw(x24, x19, x9);                             // srlw  x24, x19, x9

    // ShiftImmOp
    __ slli(x27, x5, 6u);                              // slli  x27, x5, 6
    __ srli(x22, x19, 4u);                             // srli  x22, x19, 4
    __ srai(x5, x16, 15u);                             // srai  x5, x16, 15
    __ slliw(x5, x6, 10u);                             // slliw  x5, x6, 10
    __ srliw(x21, x29, 3u);                            // srliw  x21, x29, 3
    __ sraiw(x28, x17, 9u);                            // sraiw  x28, x17, 9

    // Op
    __ nop();                                          // nop
    __ ecall();                                        // ecall
    __ ebreak();                                       // ebreak
    __ fence_i();                                      // fence.i

    // SystemOp
    __ fence(1u, 15u);                                 // fence  w, iorw

    // AtomOp
    __ sc_w(x19, x19, x30, Assembler::aq);             // sc.w.aq  x19, x19, (x30)
    __ amoswap_w(x11, x11, x12, Assembler::aq);        // amoswap.w.aq  x11, x12, (x11)
    __ amoadd_w(x22, x29, x27, Assembler::aq);         // amoadd.w.aq  x22, x27, (x29)
    __ amoxor_w(x22, x21, x6, Assembler::aq);          // amoxor.w.aq  x22, x6, (x21)
    __ amoand_w(x22, x31, x14, Assembler::aq);         // amoand.w.aq  x22, x14, (x31)
    __ amoor_w(x7, x29, x25, Assembler::aq);           // amoor.w.aq  x7, x25, (x29)
    __ amomin_w(x16, x6, x11, Assembler::aq);          // amomin.w.aq  x16, x11, (x6)
    __ amomax_w(x28, x23, x17, Assembler::aq);         // amomax.w.aq  x28, x17, (x23)
    __ amominu_w(x14, x31, x18, Assembler::aq);        // amominu.w.aq  x14, x18, (x31)
    __ amomaxu_w(x25, x31, x6, Assembler::aq);         // amomaxu.w.aq  x25, x6, (x31)
    __ lr_w(x26, x14, Assembler::aq);                  // lr.w.aq  x26, (x14)
    __ sc_w(x6, x29, x12, Assembler::rl);              // sc.w.rl  x6, x29, (x12)
    __ amoswap_w(x17, x8, x15, Assembler::rl);         // amoswap.w.rl  x17, x15, (x8)
    __ amoadd_w(x22, x30, x28, Assembler::rl);         // amoadd.w.rl  x22, x28, (x30)
    __ amoxor_w(x13, x25, x6, Assembler::rl);          // amoxor.w.rl  x13, x6, (x25)
    __ amoand_w(x16, x19, x26, Assembler::rl);         // amoand.w.rl  x16, x26, (x19)
    __ amoor_w(x27, x11, x6, Assembler::rl);           // amoor.w.rl  x27, x6, (x11)
    __ amomin_w(x30, x15, x24, Assembler::rl);         // amomin.w.rl  x30, x24, (x15)
    __ amomax_w(x12, x7, x26, Assembler::rl);          // amomax.w.rl  x12, x26, (x7)
    __ amominu_w(x28, x28, x8, Assembler::rl);         // amominu.w.rl  x28, x8, (x28)
    __ amomaxu_w(x26, x17, x27, Assembler::rl);        // amomaxu.w.rl  x26, x27, (x17)
    __ lr_w(x31, x24, Assembler::rl);                  // lr.w.rl  x31, (x24)
    __ sc_d(x22, x10, x27, Assembler::aq);             // sc.d.aq  x22, x10, (x27)
    __ amoswap_d(x6, x6, x26, Assembler::aq);          // amoswap.d.aq  x6, x26, (x6)
    __ amoadd_d(x17, x29, x30, Assembler::aq);         // amoadd.d.aq  x17, x30, (x29)
    __ amoxor_d(x17, x15, x8, Assembler::aq);          // amoxor.d.aq  x17, x8, (x15)
    __ amoand_d(x31, x24, x5, Assembler::aq);          // amoand.d.aq  x31, x5, (x24)
    __ amoor_d(x11, x12, x28, Assembler::aq);          // amoor.d.aq  x11, x28, (x12)
    __ amomin_d(x26, x20, x26, Assembler::aq);         // amomin.d.aq  x26, x26, (x20)
    __ amomax_d(x7, x9, x18, Assembler::aq);           // amomax.d.aq  x7, x18, (x9)
    __ amominu_d(x18, x10, x15, Assembler::aq);        // amominu.d.aq  x18, x15, (x10)
    __ amomaxu_d(x21, x23, x10, Assembler::aq);        // amomaxu.d.aq  x21, x10, (x23)
    __ lr_d(x21, x16, Assembler::aq);                  // lr.d.aq  x21, (x16)
    __ sc_d(x8, x20, x11, Assembler::rl);              // sc.d.rl  x8, x20, (x11)
    __ amoswap_d(x20, x12, x15, Assembler::rl);        // amoswap.d.rl  x20, x15, (x12)
    __ amoadd_d(x19, x30, x6, Assembler::rl);          // amoadd.d.rl  x19, x6, (x30)
    __ amoxor_d(x5, x29, x24, Assembler::rl);          // amoxor.d.rl  x5, x24, (x29)
    __ amoand_d(x5, x8, x29, Assembler::rl);           // amoand.d.rl  x5, x29, (x8)
    __ amoor_d(x8, x15, x7, Assembler::rl);            // amoor.d.rl  x8, x7, (x15)
    __ amomin_d(x16, x9, x24, Assembler::rl);          // amomin.d.rl  x16, x24, (x9)
    __ amomax_d(x28, x25, x13, Assembler::rl);         // amomax.d.rl  x28, x13, (x25)
    __ amominu_d(x15, x29, x28, Assembler::rl);        // amominu.d.rl  x15, x28, (x29)
    __ amomaxu_d(x28, x31, x20, Assembler::rl);        // amomaxu.d.rl  x28, x20, (x31)
    __ lr_d(x24, x14, Assembler::rl);                  // lr.d.rl  x24, (x14)

    // OneRegOp
    __ frflags(x7);                                    // frflags  x7
    __ frrm(x18);                                      // frrm  x18
    __ frcsr(x10);                                     // frcsr  x10
    __ rdtime(x9);                                     // rdtime  x9
    __ rdcycle(x22);                                   // rdcycle  x22
    __ rdinstret(x7);                                  // rdinstret  x7

    // TwoRegOp
    __ mv(x13, x23);                                   // mv  x13, x23
    __ notr(x22, x24);                                 // not  x22, x24
    __ neg(x9, x14);                                   // neg  x9, x14
    __ negw(x30, x8);                                  // negw  x30, x8
    __ sext_w(x19, x26);                               // sext.w  x19, x26
    __ seqz(x22, x23);                                 // seqz  x22, x23
    __ snez(x22, x7);                                  // snez  x22, x7
    __ sltz(x30, x14);                                 // sltz  x30, x14
    __ sgtz(x15, x12);                                 // sgtz  x15, x12
    __ fscsr(x12, x27);                                // fscsr  x12, x27
    __ fsrm(x18, x25);                                 // fsrm  x18, x25
    __ fsflags(x17, x23);                              // fsflags  x17, x23

    // ThreeRegOp
    __ slt(x25, x29, x13);                             // slt  x25, x29, x13
    __ sltu(x9, x29, x11);                             // sltu  x9, x29, x11

    // CsrxixOp
    __ csrrw(x9, 3831u, x20);                          // csrrw  x9, 0xef7, x20
    __ csrrs(x5, 1483u, x27);                          // csrrs  x5, 0x5cb, x27
    __ csrrc(x27, 4084u, x16);                         // csrrc  x27, 0xff4, x16

    // CsrxiiOp
    __ csrrwi(x27, 1835u, 16u);                        // csrrwi  x27, 0x72b, 16
    __ csrrsi(x8, 3287u, 25u);                         // csrrsi  x8, 0xcd7, 25
    __ csrrci(x19, 1229u, 18u);                        // csrrci  x19, 0x4cd, 18

    // CsrxiOp
    __ csrr(x26, 0xa1c);                               // csrr  x26, 0xa1c

    // CsrixOp
    __ csrw(0xe03, x8);                                // csrw  0xe03, x8
    __ csrs(0x890, x17);                               // csrs  0x890, x17
    __ csrc(0xdb9, x30);                               // csrc  0xdb9, x30

    // CsriiOp
    __ csrwi(0x9ffu, 14);                              // csrwi  0x9ff, 14
    __ csrsi(0x4f4u, 23);                              // csrsi  0x4f4, 23
    __ csrci(0x820u, 13);                              // csrci  0x820, 13

    // LoadStoreOp
    __ ld(x10, Address(x20, -1940));                   // ld  x10, -1940(x20)
    __ lw(x22, Address(x28, -1251));                   // lw  x22, -1251(x28)
    __ lwu(x16, Address(x25, -802));                   // lwu  x16, -802(x25)
    __ lh(x17, Address(x9, -871));                     // lh  x17, -871(x9)
    __ lhu(x25, Address(x10, -1321));                  // lhu  x25, -1321(x10)
    __ lb(x26, Address(x12, -1172));                   // lb  x26, -1172(x12)
    __ lbu(x24, Address(x27, -161));                   // lbu  x24, -161(x27)
    __ sd(x20, Address(x18, -1240));                   // sd  x20, -1240(x18)
    __ sw(x10, Address(x18, 264));                     // sw  x10, 264(x18)
    __ sh(x6, Address(x13, -137));                     // sh  x6, -137(x13)
    __ sb(x19, Address(x13, 100));                     // sb  x19, 100(x13)
    __ fld(f31, Address(x19, 687));                    // fld  f31, 687(x19)
    __ flw(f23, Address(x27, -613));                   // flw  f23, -613(x27)
    __ fsd(f23, Address(x30, 108));                    // fsd  f23, 108(x30)
    __ fsw(f24, Address(x19, -282));                   // fsw  f24, -282(x19)

    // Float2ArithOp
    __ fsqrt_s(f9, f23, Assembler::rdn);               // fsqrt.s  f9, f23, rdn
    __ fsqrt_d(f2, f4, Assembler::rdn);                // fsqrt.d  f2, f4, rdn

    // Float3ArithOp
    __ fadd_s(f4, f5, f15, Assembler::rup);            // fadd.s  f4, f5, f15, rup
    __ fsub_s(f25, f14, f0, Assembler::rup);           // fsub.s  f25, f14, f0, rup
    __ fadd_d(f17, f24, f5, Assembler::rup);           // fadd.d  f17, f24, f5, rup
    __ fsub_d(f16, f19, f28, Assembler::rup);          // fsub.d  f16, f19, f28, rup
    __ fmul_s(f22, f0, f20, Assembler::rup);           // fmul.s  f22, f0, f20, rup
    __ fdiv_s(f18, f16, f18, Assembler::rup);          // fdiv.s  f18, f16, f18, rup
    __ fmul_d(f31, f1, f30, Assembler::rup);           // fmul.d  f31, f1, f30, rup
    __ fdiv_d(f3, f28, f31, Assembler::rup);           // fdiv.d  f3, f28, f31, rup

    // Float4ArithOp
    __ fmadd_s(f27, f5, f0, f2, Assembler::rup);       // fmadd.s  f27, f5, f0, f2, rup
    __ fmsub_s(f25, f13, f12, f29, Assembler::rtz);    // fmsub.s  f25, f13, f12, f29, rtz
    __ fmadd_d(f28, f20, f27, f27, Assembler::rup);    // fmadd.d  f28, f20, f27, f27, rup
    __ fmsub_d(f14, f17, f1, f18, Assembler::rtz);     // fmsub.d  f14, f17, f1, f18, rtz
    __ fnmsub_s(f4, f1, f15, f14, Assembler::rmm);     // fnmsub.s  f4, f1, f15, f14, rmm
    __ fnmadd_s(f23, f10, f18, f8, Assembler::rtz);    // fnmadd.s  f23, f10, f18, f8, rtz
    __ fnmsub_d(f6, f28, f26, f3, Assembler::rmm);     // fnmsub.d  f6, f28, f26, f3, rmm
    __ fnmadd_d(f13, f12, f7, f16, Assembler::rtz);    // fnmadd.d  f13, f12, f7, f16, rtz

    // TwoRegFloatOp
    __ fclass_s(x23, f3);                              // fclass.s  x23, f3
    __ fmv_s(f2, f16);                                 // fmv.s  f2, f16
    __ fclass_d(x19, f25);                             // fclass.d  x19, f25
    __ fmv_d(f22, f3);                                 // fmv.d  f22, f3
    __ fabs_s(f25, f12);                               // fabs.s  f25, f12
    __ fneg_s(f15, f10);                               // fneg.s  f15, f10
    __ fabs_d(f22, f28);                               // fabs.d  f22, f28
    __ fneg_d(f7, f19);                                // fneg.d  f7, f19
    __ fmv_x_w(x23, f3);                               // fmv.x.w  x23, f3
    __ fmv_x_d(x5, f19);                               // fmv.x.d  x5, f19

    // ThreeRegFloatOp
    __ fsgnj_s(f9, f18, f0);                           // fsgnj.s  f9, f18, f0
    __ fsgnjn_s(f25, f23, f12);                        // fsgnjn.s  f25, f23, f12
    __ fsgnj_d(f3, f16, f0);                           // fsgnj.d  f3, f16, f0
    __ fsgnjn_d(f22, f1, f9);                          // fsgnjn.d  f22, f1, f9
    __ fsgnjx_s(f18, f12, f5);                         // fsgnjx.s  f18, f12, f5
    __ fmin_s(f18, f30, f28);                          // fmin.s  f18, f30, f28
    __ fsgnjx_d(f29, f19, f0);                         // fsgnjx.d  f29, f19, f0
    __ fmin_d(f9, f18, f28);                           // fmin.d  f9, f18, f28
    __ fmax_s(f4, f3, f21);                            // fmax.s  f4, f3, f21
    __ feq_s(x10, f1, f23);                            // feq.s  x10, f1, f23
    __ fmax_d(f17, f22, f26);                          // fmax.d  f17, f22, f26
    __ feq_d(x26, f1, f27);                            // feq.d  x26, f1, f27
    __ flt_s(x30, f28, f21);                           // flt.s  x30, f28, f21
    __ fle_s(x25, f6, f27);                            // fle.s  x25, f6, f27
    __ flt_d(x21, f0, f14);                            // flt.d  x21, f0, f14
    __ fle_d(x28, f1, f19);                            // fle.d  x28, f1, f19

    // FloatConvertOp
    __ fcvt_w_s(x11, f15, Assembler::rup);             // fcvt.w.s  x11, f15, rup
    __ fcvt_wu_s(x13, f13, Assembler::rne);            // fcvt.wu.s  x13, f13, rne
    __ fcvt_s_w(f17, x27, Assembler::rdn);             // fcvt.s.w  f17, x27, rdn
    __ fcvt_s_wu(f11, x27, Assembler::rtz);            // fcvt.s.wu  f11, x27, rtz
    __ fcvt_l_s(x22, f26, Assembler::rne);             // fcvt.l.s  x22, f26, rne
    __ fcvt_lu_s(x20, f5, Assembler::rmm);             // fcvt.lu.s  x20, f5, rmm
    __ fcvt_s_l(f21, x13, Assembler::rup);             // fcvt.s.l  f21, x13, rup
    __ fcvt_s_lu(f28, x25, Assembler::rtz);            // fcvt.s.lu  f28, x25, rtz
    __ fcvt_s_d(f12, f7, Assembler::rdn);              // fcvt.s.d  f12, f7, rdn
    __ fcvt_d_s(f15, f11, Assembler::rne);             // fcvt.d.s  f15, f11
    __ fcvt_w_d(x27, f20, Assembler::rdn);             // fcvt.w.d  x27, f20, rdn
    __ fcvt_wu_d(x22, f14, Assembler::rdn);            // fcvt.wu.d  x22, f14, rdn
    __ fcvt_d_w(f10, x25, Assembler::rne);             // fcvt.d.w  f10, x25
    __ fcvt_d_wu(f9, x20, Assembler::rne);             // fcvt.d.wu  f9, x20
    __ fcvt_l_d(x26, f25, Assembler::rdn);             // fcvt.l.d  x26, f25, rdn
    __ fcvt_lu_d(x10, f27, Assembler::rdn);            // fcvt.lu.d  x10, f27, rdn
    __ fcvt_d_l(f0, x29, Assembler::rdn);              // fcvt.d.l  f0, x29, rdn
    __ fcvt_d_lu(f14, x22, Assembler::rdn);            // fcvt.d.lu  f14, x22, rdn

    // AMOOp
    __ vamoswapei8_v(v25, x23, v22, true, Assembler::v0_t); // vamoswapei8.v  v25, (x23), v22, v25, v0.t
    __ vamoaddei8_v(v21, x25, v5, true, Assembler::v0_t); // vamoaddei8.v  v21, (x25), v5, v21, v0.t
    __ vamoxorei8_v(v18, x30, v9, true, Assembler::v0_t); // vamoxorei8.v  v18, (x30), v9, v18, v0.t
    __ vamoandei8_v(v7, x28, v10, true, Assembler::v0_t); // vamoandei8.v  v7, (x28), v10, v7, v0.t
    __ vamoorei8_v(v24, x6, v11, true, Assembler::v0_t); // vamoorei8.v  v24, (x6), v11, v24, v0.t
    __ vamominei8_v(v7, x19, v30, true, Assembler::v0_t); // vamominei8.v  v7, (x19), v30, v7, v0.t
    __ vamomaxei8_v(v28, x19, v5, true, Assembler::v0_t); // vamomaxei8.v  v28, (x19), v5, v28, v0.t
    __ vamominuei8_v(v11, x13, v4, true, Assembler::v0_t); // vamominuei8.v  v11, (x13), v4, v11, v0.t
    __ vamomaxuei8_v(v12, x10, v21, true, Assembler::v0_t); // vamomaxuei8.v  v12, (x10), v21, v12, v0.t
    __ vamoswapei8_v(v28, x22, v12, true);             // vamoswapei8.v  v28, (x22), v12, v28
    __ vamoaddei8_v(v29, x23, v9, true);               // vamoaddei8.v  v29, (x23), v9, v29
    __ vamoxorei8_v(v5, x28, v3, true);                // vamoxorei8.v  v5, (x28), v3, v5
    __ vamoandei8_v(v11, x7, v12, true);               // vamoandei8.v  v11, (x7), v12, v11
    __ vamoorei8_v(v16, x30, v0, true);                // vamoorei8.v  v16, (x30), v0, v16
    __ vamominei8_v(v20, x30, v22, true);              // vamominei8.v  v20, (x30), v22, v20
    __ vamomaxei8_v(v6, x21, v8, true);                // vamomaxei8.v  v6, (x21), v8, v6
    __ vamominuei8_v(v0, x25, v1, true);               // vamominuei8.v  v0, (x25), v1, v0
    __ vamomaxuei8_v(v22, x6, v30, true);              // vamomaxuei8.v  v22, (x6), v30, v22
    __ vamoswapei8_v(v11, x5, v22, false, Assembler::v0_t); // vamoswapei8.v  x0, (x5), v22, v11, v0.t
    __ vamoaddei8_v(v3, x16, v19, false, Assembler::v0_t); // vamoaddei8.v  x0, (x16), v19, v3, v0.t
    __ vamoxorei8_v(v24, x30, v6, false, Assembler::v0_t); // vamoxorei8.v  x0, (x30), v6, v24, v0.t
    __ vamoandei8_v(v15, x6, v11, false, Assembler::v0_t); // vamoandei8.v  x0, (x6), v11, v15, v0.t
    __ vamoorei8_v(v15, x9, v14, false, Assembler::v0_t); // vamoorei8.v  x0, (x9), v14, v15, v0.t
    __ vamominei8_v(v4, x23, v27, false, Assembler::v0_t); // vamominei8.v  x0, (x23), v27, v4, v0.t
    __ vamomaxei8_v(v24, x21, v19, false, Assembler::v0_t); // vamomaxei8.v  x0, (x21), v19, v24, v0.t
    __ vamominuei8_v(v12, x12, v28, false, Assembler::v0_t); // vamominuei8.v  x0, (x12), v28, v12, v0.t
    __ vamomaxuei8_v(v9, x18, v22, false, Assembler::v0_t); // vamomaxuei8.v  x0, (x18), v22, v9, v0.t
    __ vamoswapei8_v(v16, x26, v28, false);            // vamoswapei8.v  x0, (x26), v28, v16
    __ vamoaddei8_v(v19, x14, v31, false);             // vamoaddei8.v  x0, (x14), v31, v19
    __ vamoxorei8_v(v11, x17, v3, false);              // vamoxorei8.v  x0, (x17), v3, v11
    __ vamoandei8_v(v17, x8, v26, false);              // vamoandei8.v  x0, (x8), v26, v17
    __ vamoorei8_v(v21, x7, v14, false);               // vamoorei8.v  x0, (x7), v14, v21
    __ vamominei8_v(v5, x28, v26, false);              // vamominei8.v  x0, (x28), v26, v5
    __ vamomaxei8_v(v4, x26, v25, false);              // vamomaxei8.v  x0, (x26), v25, v4
    __ vamominuei8_v(v13, x25, v12, false);            // vamominuei8.v  x0, (x25), v12, v13
    __ vamomaxuei8_v(v1, x29, v17, false);             // vamomaxuei8.v  x0, (x29), v17, v1
    __ vamoswapei16_v(v3, x28, v20, true, Assembler::v0_t); // vamoswapei16.v  v3, (x28), v20, v3, v0.t
    __ vamoaddei16_v(v19, x12, v13, true, Assembler::v0_t); // vamoaddei16.v  v19, (x12), v13, v19, v0.t
    __ vamoxorei16_v(v14, x5, v11, true, Assembler::v0_t); // vamoxorei16.v  v14, (x5), v11, v14, v0.t
    __ vamoandei16_v(v5, x6, v1, true, Assembler::v0_t); // vamoandei16.v  v5, (x6), v1, v5, v0.t
    __ vamoorei16_v(v8, x9, v20, true, Assembler::v0_t); // vamoorei16.v  v8, (x9), v20, v8, v0.t
    __ vamominei16_v(v23, x5, v14, true, Assembler::v0_t); // vamominei16.v  v23, (x5), v14, v23, v0.t
    __ vamomaxei16_v(v16, x27, v10, true, Assembler::v0_t); // vamomaxei16.v  v16, (x27), v10, v16, v0.t
    __ vamominuei16_v(v20, x19, v12, true, Assembler::v0_t); // vamominuei16.v  v20, (x19), v12, v20, v0.t
    __ vamomaxuei16_v(v10, x15, v4, true, Assembler::v0_t); // vamomaxuei16.v  v10, (x15), v4, v10, v0.t
    __ vamoswapei16_v(v21, x26, v26, true);            // vamoswapei16.v  v21, (x26), v26, v21
    __ vamoaddei16_v(v26, x27, v10, true);             // vamoaddei16.v  v26, (x27), v10, v26
    __ vamoxorei16_v(v11, x21, v0, true);              // vamoxorei16.v  v11, (x21), v0, v11
    __ vamoandei16_v(v13, x21, v30, true);             // vamoandei16.v  v13, (x21), v30, v13
    __ vamoorei16_v(v7, x12, v8, true);                // vamoorei16.v  v7, (x12), v8, v7
    __ vamominei16_v(v30, x7, v4, true);               // vamominei16.v  v30, (x7), v4, v30
    __ vamomaxei16_v(v24, x8, v26, true);              // vamomaxei16.v  v24, (x8), v26, v24
    __ vamominuei16_v(v13, x18, v21, true);            // vamominuei16.v  v13, (x18), v21, v13
    __ vamomaxuei16_v(v8, x9, v4, true);               // vamomaxuei16.v  v8, (x9), v4, v8
    __ vamoswapei16_v(v23, x19, v18, false, Assembler::v0_t); // vamoswapei16.v  x0, (x19), v18, v23, v0.t
    __ vamoaddei16_v(v19, x25, v16, false, Assembler::v0_t); // vamoaddei16.v  x0, (x25), v16, v19, v0.t
    __ vamoxorei16_v(v23, x13, v21, false, Assembler::v0_t); // vamoxorei16.v  x0, (x13), v21, v23, v0.t
    __ vamoandei16_v(v11, x19, v19, false, Assembler::v0_t); // vamoandei16.v  x0, (x19), v19, v11, v0.t
    __ vamoorei16_v(v5, x22, v12, false, Assembler::v0_t); // vamoorei16.v  x0, (x22), v12, v5, v0.t
    __ vamominei16_v(v4, x24, v7, false, Assembler::v0_t); // vamominei16.v  x0, (x24), v7, v4, v0.t
    __ vamomaxei16_v(v30, x20, v30, false, Assembler::v0_t); // vamomaxei16.v  x0, (x20), v30, v30, v0.t
    __ vamominuei16_v(v3, x9, v11, false, Assembler::v0_t); // vamominuei16.v  x0, (x9), v11, v3, v0.t
    __ vamomaxuei16_v(v27, x7, v31, false, Assembler::v0_t); // vamomaxuei16.v  x0, (x7), v31, v27, v0.t
    __ vamoswapei16_v(v3, x7, v10, false);             // vamoswapei16.v  x0, (x7), v10, v3
    __ vamoaddei16_v(v12, x22, v1, false);             // vamoaddei16.v  x0, (x22), v1, v12
    __ vamoxorei16_v(v6, x6, v10, false);              // vamoxorei16.v  x0, (x6), v10, v6
    __ vamoandei16_v(v22, x27, v28, false);            // vamoandei16.v  x0, (x27), v28, v22
    __ vamoorei16_v(v4, x31, v24, false);              // vamoorei16.v  x0, (x31), v24, v4
    __ vamominei16_v(v11, x26, v13, false);            // vamominei16.v  x0, (x26), v13, v11
    __ vamomaxei16_v(v21, x29, v0, false);             // vamomaxei16.v  x0, (x29), v0, v21
    __ vamominuei16_v(v24, x22, v2, false);            // vamominuei16.v  x0, (x22), v2, v24
    __ vamomaxuei16_v(v17, x23, v28, false);           // vamomaxuei16.v  x0, (x23), v28, v17
    __ vamoswapei32_v(v21, x5, v30, true, Assembler::v0_t); // vamoswapei32.v  v21, (x5), v30, v21, v0.t
    __ vamoaddei32_v(v23, x6, v25, true, Assembler::v0_t); // vamoaddei32.v  v23, (x6), v25, v23, v0.t
    __ vamoxorei32_v(v10, x26, v0, true, Assembler::v0_t); // vamoxorei32.v  v10, (x26), v0, v10, v0.t
    __ vamoandei32_v(v6, x15, v28, true, Assembler::v0_t); // vamoandei32.v  v6, (x15), v28, v6, v0.t
    __ vamoorei32_v(v20, x19, v4, true, Assembler::v0_t); // vamoorei32.v  v20, (x19), v4, v20, v0.t
    __ vamominei32_v(v27, x22, v4, true, Assembler::v0_t); // vamominei32.v  v27, (x22), v4, v27, v0.t
    __ vamomaxei32_v(v27, x25, v11, true, Assembler::v0_t); // vamomaxei32.v  v27, (x25), v11, v27, v0.t
    __ vamominuei32_v(v26, x13, v1, true, Assembler::v0_t); // vamominuei32.v  v26, (x13), v1, v26, v0.t
    __ vamomaxuei32_v(v10, x28, v13, true, Assembler::v0_t); // vamomaxuei32.v  v10, (x28), v13, v10, v0.t
    __ vamoswapei32_v(v14, x30, v13, true);            // vamoswapei32.v  v14, (x30), v13, v14
    __ vamoaddei32_v(v12, x10, v21, true);             // vamoaddei32.v  v12, (x10), v21, v12
    __ vamoxorei32_v(v17, x17, v7, true);              // vamoxorei32.v  v17, (x17), v7, v17
    __ vamoandei32_v(v19, x23, v27, true);             // vamoandei32.v  v19, (x23), v27, v19
    __ vamoorei32_v(v8, x5, v10, true);                // vamoorei32.v  v8, (x5), v10, v8
    __ vamominei32_v(v1, x24, v0, true);               // vamominei32.v  v1, (x24), v0, v1
    __ vamomaxei32_v(v24, x22, v20, true);             // vamomaxei32.v  v24, (x22), v20, v24
    __ vamominuei32_v(v19, x31, v0, true);             // vamominuei32.v  v19, (x31), v0, v19
    __ vamomaxuei32_v(v21, x15, v0, true);             // vamomaxuei32.v  v21, (x15), v0, v21
    __ vamoswapei32_v(v4, x23, v5, false, Assembler::v0_t); // vamoswapei32.v  x0, (x23), v5, v4, v0.t
    __ vamoaddei32_v(v28, x24, v22, false, Assembler::v0_t); // vamoaddei32.v  x0, (x24), v22, v28, v0.t
    __ vamoxorei32_v(v26, x15, v23, false, Assembler::v0_t); // vamoxorei32.v  x0, (x15), v23, v26, v0.t
    __ vamoandei32_v(v29, x31, v14, false, Assembler::v0_t); // vamoandei32.v  x0, (x31), v14, v29, v0.t
    __ vamoorei32_v(v25, x22, v6, false, Assembler::v0_t); // vamoorei32.v  x0, (x22), v6, v25, v0.t
    __ vamominei32_v(v18, x21, v16, false, Assembler::v0_t); // vamominei32.v  x0, (x21), v16, v18, v0.t
    __ vamomaxei32_v(v11, x9, v11, false, Assembler::v0_t); // vamomaxei32.v  x0, (x9), v11, v11, v0.t
    __ vamominuei32_v(v1, x14, v0, false, Assembler::v0_t); // vamominuei32.v  x0, (x14), v0, v1, v0.t
    __ vamomaxuei32_v(v28, x14, v22, false, Assembler::v0_t); // vamomaxuei32.v  x0, (x14), v22, v28, v0.t
    __ vamoswapei32_v(v7, x25, v21, false);            // vamoswapei32.v  x0, (x25), v21, v7
    __ vamoaddei32_v(v0, x6, v21, false);              // vamoaddei32.v  x0, (x6), v21, v0
    __ vamoxorei32_v(v13, x15, v20, false);            // vamoxorei32.v  x0, (x15), v20, v13
    __ vamoandei32_v(v28, x9, v26, false);             // vamoandei32.v  x0, (x9), v26, v28
    __ vamoorei32_v(v3, x17, v3, false);               // vamoorei32.v  x0, (x17), v3, v3
    __ vamominei32_v(v13, x21, v30, false);            // vamominei32.v  x0, (x21), v30, v13
    __ vamomaxei32_v(v16, x25, v5, false);             // vamomaxei32.v  x0, (x25), v5, v16
    __ vamominuei32_v(v18, x12, v25, false);           // vamominuei32.v  x0, (x12), v25, v18
    __ vamomaxuei32_v(v3, x24, v6, false);             // vamomaxuei32.v  x0, (x24), v6, v3

    // R2vmOp
    __ vzext_vf2(v29, v8, Assembler::v0_t);            // vzext.vf2  v29, v8, v0.t
    __ vzext_vf4(v1, v16, Assembler::v0_t);            // vzext.vf4  v1, v16, v0.t
    __ vzext_vf8(v12, v26, Assembler::v0_t);           // vzext.vf8  v12, v26, v0.t
    __ vsext_vf2(v9, v9, Assembler::v0_t);             // vsext.vf2  v9, v9, v0.t
    __ vsext_vf4(v1, v27, Assembler::v0_t);            // vsext.vf4  v1, v27, v0.t
    __ vsext_vf8(v28, v9, Assembler::v0_t);            // vsext.vf8  v28, v9, v0.t
    __ vpopc_m(x24, v5, Assembler::v0_t);              // vpopc.m  x24, v5, v0.t
    __ vfirst_m(x8, v11, Assembler::v0_t);             // vfirst.m  x8, v11, v0.t
    __ vmsbf_m(v30, v17, Assembler::v0_t);             // vmsbf.m  v30, v17, v0.t
    __ vmsif_m(v13, v8, Assembler::v0_t);              // vmsif.m  v13, v8, v0.t
    __ vmsof_m(v11, v25, Assembler::v0_t);             // vmsof.m  v11, v25, v0.t
    __ viota_m(v3, v16, Assembler::v0_t);              // viota.m  v3, v16, v0.t
    __ vfcvt_xu_f_v(v1, v9, Assembler::v0_t);          // vfcvt.xu.f.v  v1, v9, v0.t
    __ vfcvt_x_f_v(v9, v21, Assembler::v0_t);          // vfcvt.x.f.v  v9, v21, v0.t
    __ vfcvt_f_xu_v(v31, v1, Assembler::v0_t);         // vfcvt.f.xu.v  v31, v1, v0.t
    __ vfcvt_f_x_v(v13, v13, Assembler::v0_t);         // vfcvt.f.x.v  v13, v13, v0.t
    __ vfcvt_rtz_xu_f_v(v10, v29, Assembler::v0_t);    // vfcvt.rtz.xu.f.v  v10, v29, v0.t
    __ vfcvt_rtz_x_f_v(v1, v8, Assembler::v0_t);       // vfcvt.rtz.x.f.v  v1, v8, v0.t
    __ vfwcvt_xu_f_v(v12, v28, Assembler::v0_t);       // vfwcvt.xu.f.v  v12, v28, v0.t
    __ vfwcvt_x_f_v(v13, v18, Assembler::v0_t);        // vfwcvt.x.f.v  v13, v18, v0.t
    __ vfwcvt_f_xu_v(v19, v20, Assembler::v0_t);       // vfwcvt.f.xu.v  v19, v20, v0.t
    __ vfwcvt_f_x_v(v21, v10, Assembler::v0_t);        // vfwcvt.f.x.v  v21, v10, v0.t
    __ vfwcvt_f_f_v(v27, v7, Assembler::v0_t);         // vfwcvt.f.f.v  v27, v7, v0.t
    __ vfwcvt_rtz_xu_f_v(v27, v3, Assembler::v0_t);    // vfwcvt.rtz.xu.f.v  v27, v3, v0.t
    __ vfwcvt_rtz_x_f_v(v19, v1, Assembler::v0_t);     // vfwcvt.rtz.x.f.v  v19, v1, v0.t
    __ vfncvt_xu_f_w(v20, v1, Assembler::v0_t);        // vfncvt.xu.f.w  v20, v1, v0.t
    __ vfncvt_x_f_w(v17, v10, Assembler::v0_t);        // vfncvt.x.f.w  v17, v10, v0.t
    __ vfncvt_f_xu_w(v24, v29, Assembler::v0_t);       // vfncvt.f.xu.w  v24, v29, v0.t
    __ vfncvt_f_x_w(v27, v28, Assembler::v0_t);        // vfncvt.f.x.w  v27, v28, v0.t
    __ vfncvt_f_f_w(v31, v29, Assembler::v0_t);        // vfncvt.f.f.w  v31, v29, v0.t
    __ vfncvt_rod_f_f_w(v9, v31, Assembler::v0_t);     // vfncvt.rod.f.f.w  v9, v31, v0.t
    __ vfncvt_rtz_xu_f_w(v19, v11, Assembler::v0_t);   // vfncvt.rtz.xu.f.w  v19, v11, v0.t
    __ vfncvt_rtz_x_f_w(v31, v27, Assembler::v0_t);    // vfncvt.rtz.x.f.w  v31, v27, v0.t
    __ vfsqrt_v(v30, v31, Assembler::v0_t);            // vfsqrt.v  v30, v31, v0.t
    __ vfclass_v(v1, v13, Assembler::v0_t);            // vfclass.v  v1, v13, v0.t
    __ vzext_vf2(v14, v2);                             // vzext.vf2  v14, v2
    __ vzext_vf4(v15, v8);                             // vzext.vf4  v15, v8
    __ vzext_vf8(v8, v16);                             // vzext.vf8  v8, v16
    __ vsext_vf2(v29, v25);                            // vsext.vf2  v29, v25
    __ vsext_vf4(v13, v23);                            // vsext.vf4  v13, v23
    __ vsext_vf8(v30, v29);                            // vsext.vf8  v30, v29
    __ vpopc_m(x29, v5);                               // vpopc.m  x29, v5
    __ vfirst_m(x23, v14);                             // vfirst.m  x23, v14
    __ vmsbf_m(v16, v18);                              // vmsbf.m  v16, v18
    __ vmsif_m(v1, v4);                                // vmsif.m  v1, v4
    __ vmsof_m(v7, v19);                               // vmsof.m  v7, v19
    __ viota_m(v9, v28);                               // viota.m  v9, v28
    __ vfcvt_xu_f_v(v13, v9);                          // vfcvt.xu.f.v  v13, v9
    __ vfcvt_x_f_v(v21, v2);                           // vfcvt.x.f.v  v21, v2
    __ vfcvt_f_xu_v(v26, v19);                         // vfcvt.f.xu.v  v26, v19
    __ vfcvt_f_x_v(v23, v26);                          // vfcvt.f.x.v  v23, v26
    __ vfcvt_rtz_xu_f_v(v9, v9);                       // vfcvt.rtz.xu.f.v  v9, v9
    __ vfcvt_rtz_x_f_v(v1, v29);                       // vfcvt.rtz.x.f.v  v1, v29
    __ vfwcvt_xu_f_v(v3, v3);                          // vfwcvt.xu.f.v  v3, v3
    __ vfwcvt_x_f_v(v30, v2);                          // vfwcvt.x.f.v  v30, v2
    __ vfwcvt_f_xu_v(v16, v16);                        // vfwcvt.f.xu.v  v16, v16
    __ vfwcvt_f_x_v(v3, v20);                          // vfwcvt.f.x.v  v3, v20
    __ vfwcvt_f_f_v(v24, v3);                          // vfwcvt.f.f.v  v24, v3
    __ vfwcvt_rtz_xu_f_v(v20, v6);                     // vfwcvt.rtz.xu.f.v  v20, v6
    __ vfwcvt_rtz_x_f_v(v17, v22);                     // vfwcvt.rtz.x.f.v  v17, v22
    __ vfncvt_xu_f_w(v11, v5);                         // vfncvt.xu.f.w  v11, v5
    __ vfncvt_x_f_w(v24, v13);                         // vfncvt.x.f.w  v24, v13
    __ vfncvt_f_xu_w(v4, v31);                         // vfncvt.f.xu.w  v4, v31
    __ vfncvt_f_x_w(v9, v19);                          // vfncvt.f.x.w  v9, v19
    __ vfncvt_f_f_w(v19, v30);                         // vfncvt.f.f.w  v19, v30
    __ vfncvt_rod_f_f_w(v2, v11);                      // vfncvt.rod.f.f.w  v2, v11
    __ vfncvt_rtz_xu_f_w(v18, v21);                    // vfncvt.rtz.xu.f.w  v18, v21
    __ vfncvt_rtz_x_f_w(v6, v31);                      // vfncvt.rtz.x.f.w  v6, v31
    __ vfsqrt_v(v27, v4);                              // vfsqrt.v  v27, v4
    __ vfclass_v(v18, v14);                            // vfclass.v  v18, v14

    // R2rdOp
    __ vmv1r_v(v29, v5);                               // vmv1r.v  v29, v5
    __ vmv2r_v(v31, v14);                              // vmv2r.v  v31, v14
    __ vmv4r_v(v13, v14);                              // vmv4r.v  v13, v14
    __ vmv8r_v(v25, v4);                               // vmv8r.v  v25, v4
    __ vfmv_f_s(f30, v26);                             // vfmv.f.s  f30, v26
    __ vmv_x_s(x11, v21);                              // vmv.x.s  x11, v21
    __ vfmv_s_f(v1, f12);                              // vfmv.s.f  v1, f12
    __ vmv_s_x(v27, x6);                               // vmv.s.x  v27, x6
    __ vfmv_v_f(v24, f15);                             // vfmv.v.f  v24, f15
    __ vmv_v_x(v21, x12);                              // vmv.v.x  v21, x12
    __ vmv_v_v(v2, v14);                               // vmv.v.v  v2, v14

    // OneVecRegImmOp
    __ vmv_v_i(v22, 7);                                // vmv.v.i  v22, 7

    // OneRegVOp
    __ vid_v(v27);                                     // vid.v  v27

    // LdStTwoRegOp
    __ vl1r_v(v28, x24);                               // vl1r.v  v28, (x24)
    __ vs1r_v(v5, x8);                                 // vs1r.v  v5, (x8)
    __ vle8_v(v0, x25, Assembler::v0_t);               // vle8.v  v0, (x25), v0.t
    __ vle16_v(v2, x28, Assembler::v0_t);              // vle16.v  v2, (x28), v0.t
    __ vle32_v(v13, x11, Assembler::v0_t);             // vle32.v  v13, (x11), v0.t
    __ vle64_v(v11, x16, Assembler::v0_t);             // vle64.v  v11, (x16), v0.t
    __ vse8_v(v29, x17, Assembler::v0_t);              // vse8.v  v29, (x17), v0.t
    __ vse16_v(v25, x16, Assembler::v0_t);             // vse16.v  v25, (x16), v0.t
    __ vse32_v(v27, x22, Assembler::v0_t);             // vse32.v  v27, (x22), v0.t
    __ vse64_v(v18, x10, Assembler::v0_t);             // vse64.v  v18, (x10), v0.t
    __ vle8ff_v(v12, x23, Assembler::v0_t);            // vle8ff.v  v12, (x23), v0.t
    __ vle16ff_v(v27, x30, Assembler::v0_t);           // vle16ff.v  v27, (x30), v0.t
    __ vle32ff_v(v7, x11, Assembler::v0_t);            // vle32ff.v  v7, (x11), v0.t
    __ vle64ff_v(v23, x11, Assembler::v0_t);           // vle64ff.v  v23, (x11), v0.t
    __ vle8_v(v22, x8);                                // vle8.v  v22, (x8)
    __ vle16_v(v14, x19);                              // vle16.v  v14, (x19)
    __ vle32_v(v29, x14);                              // vle32.v  v29, (x14)
    __ vle64_v(v15, x25);                              // vle64.v  v15, (x25)
    __ vse8_v(v24, x28);                               // vse8.v  v24, (x28)
    __ vse16_v(v18, x6);                               // vse16.v  v18, (x6)
    __ vse32_v(v19, x14);                              // vse32.v  v19, (x14)
    __ vse64_v(v23, x30);                              // vse64.v  v23, (x30)
    __ vle8ff_v(v8, x9);                               // vle8ff.v  v8, (x9)
    __ vle16ff_v(v11, x21);                            // vle16ff.v  v11, (x21)
    __ vle32ff_v(v6, x28);                             // vle32ff.v  v6, (x28)
    __ vle64ff_v(v14, x24);                            // vle64ff.v  v14, (x24)

    // LdStThreeoRegOp
    __ vlse8_v(v8, x15, x26, Assembler::v0_t);         // vlse8.v  v8, (x15), x26, v0.t
    __ vlse16_v(v31, x20, x14, Assembler::v0_t);       // vlse16.v  v31, (x20), x14, v0.t
    __ vlse32_v(v10, x28, x12, Assembler::v0_t);       // vlse32.v  v10, (x28), x12, v0.t
    __ vlse64_v(v28, x21, x29, Assembler::v0_t);       // vlse64.v  v28, (x21), x29, v0.t
    __ vsse8_v(v7, x5, x13, Assembler::v0_t);          // vsse8.v  v7, (x5), x13, v0.t
    __ vsse16_v(v31, x13, x22, Assembler::v0_t);       // vsse16.v  v31, (x13), x22, v0.t
    __ vsse32_v(v15, x12, x19, Assembler::v0_t);       // vsse32.v  v15, (x12), x19, v0.t
    __ vsse64_v(v29, x14, x24, Assembler::v0_t);       // vsse64.v  v29, (x14), x24, v0.t
    __ vlse8_v(v24, x17, x22);                         // vlse8.v  v24, (x17), x22
    __ vlse16_v(v14, x19, x20);                        // vlse16.v  v14, (x19), x20
    __ vlse32_v(v26, x17, x20);                        // vlse32.v  v26, (x17), x20
    __ vlse64_v(v22, x11, x18);                        // vlse64.v  v22, (x11), x18
    __ vsse8_v(v3, x26, x27);                          // vsse8.v  v3, (x26), x27
    __ vsse16_v(v27, x10, x5);                         // vsse16.v  v27, (x10), x5
    __ vsse32_v(v20, x13, x22);                        // vsse32.v  v20, (x13), x22
    __ vsse64_v(v0, x31, x10);                         // vsse64.v  v0, (x31), x10

    // ThreeVecRegD21VOp
    __ vfmerge_vfm(v31, v26, f4, v0);                  // vfmerge.vfm  v31, v26, f4, v0
    __ vmerge_vxm(v9, v17, x21, v0);                   // vmerge.vxm  v9, v17, x21, v0
    __ vmerge_vvm(v27, v5, v0, v0);                    // vmerge.vvm  v27, v5, v0, v0
    __ vsbc_vxm(v27, v2, x23, v0);                     // vsbc.vxm  v27, v2, x23, v0
    __ vsbc_vvm(v17, v26, v13, v0);                    // vsbc.vvm  v17, v26, v13, v0
    __ vadc_vxm(v20, v6, x25, v0);                     // vadc.vxm  v20, v6, x25, v0
    __ vadc_vvm(v0, v23, v17, v0);                     // vadc.vvm  v0, v23, v17, v0
    __ vmadc_vxm(v8, v17, x22, v0);                    // vmadc.vxm  v8, v17, x22, v0
    __ vmadc_vvm(v13, v25, v8, v0);                    // vmadc.vvm  v13, v25, v8, v0
    __ vmsbc_vxm(v10, v12, x9, v0);                    // vmsbc.vxm  v10, v12, x9, v0
    __ vmsbc_vvm(v30, v6, v23, v0);                    // vmsbc.vvm  v30, v6, v23, v0

    // ThreeVecRegD12Op
    __ vfwnmsac_vf(v9, f13, v11, Assembler::v0_t);     // vfwnmsac.vf  v9, f13, v11, v0.t
    __ vfwnmsac_vv(v12, v29, v9, Assembler::v0_t);     // vfwnmsac.vv  v12, v29, v9, v0.t
    __ vfwmsac_vf(v14, f29, v0, Assembler::v0_t);      // vfwmsac.vf  v14, f29, v0, v0.t
    __ vfwmsac_vv(v14, v24, v14, Assembler::v0_t);     // vfwmsac.vv  v14, v24, v14, v0.t
    __ vfwnmacc_vf(v16, f6, v0, Assembler::v0_t);      // vfwnmacc.vf  v16, f6, v0, v0.t
    __ vfwnmacc_vv(v25, v25, v31, Assembler::v0_t);    // vfwnmacc.vv  v25, v25, v31, v0.t
    __ vfwmacc_vf(v26, f2, v7, Assembler::v0_t);       // vfwmacc.vf  v26, f2, v7, v0.t
    __ vfwmacc_vv(v6, v19, v23, Assembler::v0_t);      // vfwmacc.vv  v6, v19, v23, v0.t
    __ vfnmsub_vf(v11, f18, v25, Assembler::v0_t);     // vfnmsub.vf  v11, f18, v25, v0.t
    __ vfnmsub_vv(v29, v17, v8, Assembler::v0_t);      // vfnmsub.vv  v29, v17, v8, v0.t
    __ vfmsub_vf(v8, f8, v1, Assembler::v0_t);         // vfmsub.vf  v8, f8, v1, v0.t
    __ vfmsub_vv(v8, v16, v25, Assembler::v0_t);       // vfmsub.vv  v8, v16, v25, v0.t
    __ vfnmadd_vf(v14, f14, v10, Assembler::v0_t);     // vfnmadd.vf  v14, f14, v10, v0.t
    __ vfnmadd_vv(v28, v7, v10, Assembler::v0_t);      // vfnmadd.vv  v28, v7, v10, v0.t
    __ vfmadd_vf(v11, f16, v7, Assembler::v0_t);       // vfmadd.vf  v11, f16, v7, v0.t
    __ vfmadd_vv(v19, v2, v10, Assembler::v0_t);       // vfmadd.vv  v19, v2, v10, v0.t
    __ vfnmsac_vf(v30, f23, v21, Assembler::v0_t);     // vfnmsac.vf  v30, f23, v21, v0.t
    __ vfnmsac_vv(v20, v6, v31, Assembler::v0_t);      // vfnmsac.vv  v20, v6, v31, v0.t
    __ vfmsac_vf(v24, f19, v19, Assembler::v0_t);      // vfmsac.vf  v24, f19, v19, v0.t
    __ vfmsac_vv(v14, v12, v26, Assembler::v0_t);      // vfmsac.vv  v14, v12, v26, v0.t
    __ vfmacc_vf(v23, f9, v3, Assembler::v0_t);        // vfmacc.vf  v23, f9, v3, v0.t
    __ vfmacc_vv(v7, v10, v31, Assembler::v0_t);       // vfmacc.vv  v7, v10, v31, v0.t
    __ vfnmacc_vf(v27, f13, v3, Assembler::v0_t);      // vfnmacc.vf  v27, f13, v3, v0.t
    __ vfnmacc_vv(v10, v4, v24, Assembler::v0_t);      // vfnmacc.vv  v10, v4, v24, v0.t
    __ vwmaccsu_vx(v5, x29, v5, Assembler::v0_t);      // vwmaccsu.vx  v5, x29, v5, v0.t
    __ vwmaccsu_vv(v30, v7, v0, Assembler::v0_t);      // vwmaccsu.vv  v30, v7, v0, v0.t
    __ vwmacc_vx(v27, x24, v9, Assembler::v0_t);       // vwmacc.vx  v27, x24, v9, v0.t
    __ vwmacc_vv(v9, v2, v11, Assembler::v0_t);        // vwmacc.vv  v9, v2, v11, v0.t
    __ vwmaccu_vx(v0, x20, v4, Assembler::v0_t);       // vwmaccu.vx  v0, x20, v4, v0.t
    __ vwmaccu_vv(v22, v2, v2, Assembler::v0_t);       // vwmaccu.vv  v22, v2, v2, v0.t
    __ vwmaccus_vx(v11, x30, v22, Assembler::v0_t);    // vwmaccus.vx  v11, x30, v22, v0.t
    __ vnmsub_vx(v22, x12, v1, Assembler::v0_t);       // vnmsub.vx  v22, x12, v1, v0.t
    __ vnmsub_vv(v19, v1, v12, Assembler::v0_t);       // vnmsub.vv  v19, v1, v12, v0.t
    __ vmadd_vx(v24, x12, v10, Assembler::v0_t);       // vmadd.vx  v24, x12, v10, v0.t
    __ vmadd_vv(v18, v0, v23, Assembler::v0_t);        // vmadd.vv  v18, v0, v23, v0.t
    __ vnmsac_vx(v25, x10, v19, Assembler::v0_t);      // vnmsac.vx  v25, x10, v19, v0.t
    __ vnmsac_vv(v16, v29, v6, Assembler::v0_t);       // vnmsac.vv  v16, v29, v6, v0.t
    __ vmacc_vx(v10, x12, v20, Assembler::v0_t);       // vmacc.vx  v10, x12, v20, v0.t
    __ vmacc_vv(v1, v6, v3, Assembler::v0_t);          // vmacc.vv  v1, v6, v3, v0.t
    __ vfwnmsac_vf(v1, f5, v28);                       // vfwnmsac.vf  v1, f5, v28
    __ vfwnmsac_vv(v24, v16, v27);                     // vfwnmsac.vv  v24, v16, v27
    __ vfwmsac_vf(v4, f16, v7);                        // vfwmsac.vf  v4, f16, v7
    __ vfwmsac_vv(v10, v22, v13);                      // vfwmsac.vv  v10, v22, v13
    __ vfwnmacc_vf(v14, f12, v29);                     // vfwnmacc.vf  v14, f12, v29
    __ vfwnmacc_vv(v6, v17, v25);                      // vfwnmacc.vv  v6, v17, v25
    __ vfwmacc_vf(v16, f25, v12);                      // vfwmacc.vf  v16, f25, v12
    __ vfwmacc_vv(v8, v27, v1);                        // vfwmacc.vv  v8, v27, v1
    __ vfnmsub_vf(v9, f21, v12);                       // vfnmsub.vf  v9, f21, v12
    __ vfnmsub_vv(v4, v0, v13);                        // vfnmsub.vv  v4, v0, v13
    __ vfmsub_vf(v19, f3, v12);                        // vfmsub.vf  v19, f3, v12
    __ vfmsub_vv(v9, v20, v2);                         // vfmsub.vv  v9, v20, v2
    __ vfnmadd_vf(v13, f3, v8);                        // vfnmadd.vf  v13, f3, v8
    __ vfnmadd_vv(v16, v14, v30);                      // vfnmadd.vv  v16, v14, v30
    __ vfmadd_vf(v26, f27, v31);                       // vfmadd.vf  v26, f27, v31
    __ vfmadd_vv(v16, v21, v28);                       // vfmadd.vv  v16, v21, v28
    __ vfnmsac_vf(v14, f26, v25);                      // vfnmsac.vf  v14, f26, v25
    __ vfnmsac_vv(v18, v11, v1);                       // vfnmsac.vv  v18, v11, v1
    __ vfmsac_vf(v24, f9, v16);                        // vfmsac.vf  v24, f9, v16
    __ vfmsac_vv(v2, v28, v21);                        // vfmsac.vv  v2, v28, v21
    __ vfmacc_vf(v26, f15, v30);                       // vfmacc.vf  v26, f15, v30
    __ vfmacc_vv(v31, v18, v29);                       // vfmacc.vv  v31, v18, v29
    __ vfnmacc_vf(v21, f21, v10);                      // vfnmacc.vf  v21, f21, v10
    __ vfnmacc_vv(v24, v28, v3);                       // vfnmacc.vv  v24, v28, v3
    __ vwmaccsu_vx(v13, x12, v5);                      // vwmaccsu.vx  v13, x12, v5
    __ vwmaccsu_vv(v15, v19, v21);                     // vwmaccsu.vv  v15, v19, v21
    __ vwmacc_vx(v1, x7, v27);                         // vwmacc.vx  v1, x7, v27
    __ vwmacc_vv(v16, v10, v25);                       // vwmacc.vv  v16, v10, v25
    __ vwmaccu_vx(v21, x29, v31);                      // vwmaccu.vx  v21, x29, v31
    __ vwmaccu_vv(v18, v18, v20);                      // vwmaccu.vv  v18, v18, v20
    __ vwmaccus_vx(v30, x24, v26);                     // vwmaccus.vx  v30, x24, v26
    __ vnmsub_vx(v19, x13, v26);                       // vnmsub.vx  v19, x13, v26
    __ vnmsub_vv(v21, v12, v12);                       // vnmsub.vv  v21, v12, v12
    __ vmadd_vx(v10, x22, v27);                        // vmadd.vx  v10, x22, v27
    __ vmadd_vv(v22, v10, v20);                        // vmadd.vv  v22, v10, v20
    __ vnmsac_vx(v3, x23, v3);                         // vnmsac.vx  v3, x23, v3
    __ vnmsac_vv(v2, v28, v7);                         // vnmsac.vv  v2, v28, v7
    __ vmacc_vx(v25, x17, v8);                         // vmacc.vx  v25, x17, v8
    __ vmacc_vv(v12, v16, v9);                         // vmacc.vv  v12, v16, v9

    // ResSubVecRegOp
    __ vrsub_vx(v7, x30, v26, Assembler::v0_t);        // vrsub.vx  v7, v26, x30, v0.t
    __ vrsub_vx(v19, x30, v5);                         // vrsub.vx  v19, v5, x30

    // ThreeVecRegD21Op
    __ vrgather_vx(v23, v4, x9, Assembler::v0_t);      // vrgather.vx  v23, v4, x9, v0.t
    __ vrgather_vv(v1, v23, v3, Assembler::v0_t);      // vrgather.vv  v1, v23, v3, v0.t
    __ vslide1down_vx(v31, v4, x9, Assembler::v0_t);   // vslide1down.vx  v31, v4, x9, v0.t
    __ vslidedown_vx(v16, v31, x19, Assembler::v0_t);  // vslidedown.vx  v16, v31, x19, v0.t
    __ vslide1up_vx(v26, v10, x8, Assembler::v0_t);    // vslide1up.vx  v26, v10, x8, v0.t
    __ vslideup_vx(v7, v5, x28, Assembler::v0_t);      // vslideup.vx  v7, v5, x28, v0.t
    __ vfwredsum_vs(v4, v31, v30, Assembler::v0_t);    // vfwredsum.vs  v4, v31, v30, v0.t
    __ vfwredosum_vs(v30, v8, v26, Assembler::v0_t);   // vfwredosum.vs  v30, v8, v26, v0.t
    __ vfredsum_vs(v21, v7, v18, Assembler::v0_t);     // vfredsum.vs  v21, v7, v18, v0.t
    __ vfredosum_vs(v14, v0, v6, Assembler::v0_t);     // vfredosum.vs  v14, v0, v6, v0.t
    __ vfredmin_vs(v8, v12, v0, Assembler::v0_t);      // vfredmin.vs  v8, v12, v0, v0.t
    __ vfredmax_vs(v28, v23, v13, Assembler::v0_t);    // vfredmax.vs  v28, v23, v13, v0.t
    __ vredsum_vs(v20, v29, v0, Assembler::v0_t);      // vredsum.vs  v20, v29, v0, v0.t
    __ vredand_vs(v25, v18, v20, Assembler::v0_t);     // vredand.vs  v25, v18, v20, v0.t
    __ vredor_vs(v1, v10, v25, Assembler::v0_t);       // vredor.vs  v1, v10, v25, v0.t
    __ vredxor_vs(v2, v6, v21, Assembler::v0_t);       // vredxor.vs  v2, v6, v21, v0.t
    __ vredminu_vs(v9, v2, v4, Assembler::v0_t);       // vredminu.vs  v9, v2, v4, v0.t
    __ vredmin_vs(v19, v14, v20, Assembler::v0_t);     // vredmin.vs  v19, v14, v20, v0.t
    __ vredmaxu_vs(v8, v23, v6, Assembler::v0_t);      // vredmaxu.vs  v8, v23, v6, v0.t
    __ vredmax_vs(v30, v12, v19, Assembler::v0_t);     // vredmax.vs  v30, v12, v19, v0.t
    __ vwredsumu_vs(v30, v7, v29, Assembler::v0_t);    // vwredsumu.vs  v30, v7, v29, v0.t
    __ vwredsum_vs(v20, v7, v18, Assembler::v0_t);     // vwredsum.vs  v20, v7, v18, v0.t
    __ vmfge_vf(v3, v2, f24, Assembler::v0_t);         // vmfge.vf  v3, v2, f24, v0.t
    __ vmfgt_vf(v31, v7, f28, Assembler::v0_t);        // vmfgt.vf  v31, v7, f28, v0.t
    __ vmfle_vf(v8, v23, f19, Assembler::v0_t);        // vmfle.vf  v8, v23, f19, v0.t
    __ vmfle_vv(v24, v26, v15, Assembler::v0_t);       // vmfle.vv  v24, v26, v15, v0.t
    __ vmflt_vf(v19, v8, f10, Assembler::v0_t);        // vmflt.vf  v19, v8, f10, v0.t
    __ vmflt_vv(v23, v16, v13, Assembler::v0_t);       // vmflt.vv  v23, v16, v13, v0.t
    __ vmfne_vf(v7, v4, f30, Assembler::v0_t);         // vmfne.vf  v7, v4, f30, v0.t
    __ vmfne_vv(v6, v1, v27, Assembler::v0_t);         // vmfne.vv  v6, v1, v27, v0.t
    __ vmfeq_vf(v19, v12, f21, Assembler::v0_t);       // vmfeq.vf  v19, v12, f21, v0.t
    __ vmfeq_vv(v31, v22, v26, Assembler::v0_t);       // vmfeq.vv  v31, v22, v26, v0.t
    __ vfslide1down_vf(v24, v13, f23, Assembler::v0_t); // vfslide1down.vf  v24, v13, f23, v0.t
    __ vfslide1up_vf(v23, v6, f3, Assembler::v0_t);    // vfslide1up.vf  v23, v6, f3, v0.t
    __ vfsgnjx_vf(v11, v13, f18, Assembler::v0_t);     // vfsgnjx.vf  v11, v13, f18, v0.t
    __ vfsgnjx_vv(v20, v26, v2, Assembler::v0_t);      // vfsgnjx.vv  v20, v26, v2, v0.t
    __ vfsgnjn_vf(v21, v25, f26, Assembler::v0_t);     // vfsgnjn.vf  v21, v25, f26, v0.t
    __ vfsgnjn_vv(v12, v22, v18, Assembler::v0_t);     // vfsgnjn.vv  v12, v22, v18, v0.t
    __ vfsgnj_vf(v26, v22, f5, Assembler::v0_t);       // vfsgnj.vf  v26, v22, f5, v0.t
    __ vfsgnj_vv(v30, v14, v23, Assembler::v0_t);      // vfsgnj.vv  v30, v14, v23, v0.t
    __ vfmax_vf(v3, v10, f28, Assembler::v0_t);        // vfmax.vf  v3, v10, f28, v0.t
    __ vfmax_vv(v31, v3, v19, Assembler::v0_t);        // vfmax.vv  v31, v3, v19, v0.t
    __ vfmin_vf(v4, v17, f2, Assembler::v0_t);         // vfmin.vf  v4, v17, f2, v0.t
    __ vfmin_vv(v21, v22, v27, Assembler::v0_t);       // vfmin.vv  v21, v22, v27, v0.t
    __ vfwmul_vf(v24, v27, f15, Assembler::v0_t);      // vfwmul.vf  v24, v27, f15, v0.t
    __ vfwmul_vv(v31, v17, v21, Assembler::v0_t);      // vfwmul.vv  v31, v17, v21, v0.t
    __ vfdiv_vf(v19, v22, f6, Assembler::v0_t);        // vfdiv.vf  v19, v22, f6, v0.t
    __ vfdiv_vv(v10, v21, v2, Assembler::v0_t);        // vfdiv.vv  v10, v21, v2, v0.t
    __ vfmul_vf(v1, v12, f3, Assembler::v0_t);         // vfmul.vf  v1, v12, f3, v0.t
    __ vfmul_vv(v10, v13, v20, Assembler::v0_t);       // vfmul.vv  v10, v13, v20, v0.t
    __ vfrdiv_vf(v16, v7, f15, Assembler::v0_t);       // vfrdiv.vf  v16, v7, f15, v0.t
    __ vfwsub_wf(v5, v30, f4, Assembler::v0_t);        // vfwsub.wf  v5, v30, f4, v0.t
    __ vfwsub_wv(v16, v13, v17, Assembler::v0_t);      // vfwsub.wv  v16, v13, v17, v0.t
    __ vfwsub_vf(v24, v4, f12, Assembler::v0_t);       // vfwsub.vf  v24, v4, f12, v0.t
    __ vfwsub_vv(v0, v4, v9, Assembler::v0_t);         // vfwsub.vv  v0, v4, v9, v0.t
    __ vfwadd_wf(v1, v18, f2, Assembler::v0_t);        // vfwadd.wf  v1, v18, f2, v0.t
    __ vfwadd_wv(v17, v3, v13, Assembler::v0_t);       // vfwadd.wv  v17, v3, v13, v0.t
    __ vfwadd_vf(v30, v1, f12, Assembler::v0_t);       // vfwadd.vf  v30, v1, f12, v0.t
    __ vfwadd_vv(v19, v19, v27, Assembler::v0_t);      // vfwadd.vv  v19, v19, v27, v0.t
    __ vfsub_vf(v11, v31, f5, Assembler::v0_t);        // vfsub.vf  v11, v31, f5, v0.t
    __ vfsub_vv(v16, v13, v6, Assembler::v0_t);        // vfsub.vv  v16, v13, v6, v0.t
    __ vfadd_vf(v3, v21, f6, Assembler::v0_t);         // vfadd.vf  v3, v21, f6, v0.t
    __ vfadd_vv(v10, v28, v16, Assembler::v0_t);       // vfadd.vv  v10, v28, v16, v0.t
    __ vfrsub_vf(v18, v5, f29, Assembler::v0_t);       // vfrsub.vf  v18, v5, f29, v0.t
    __ vnclip_wx(v16, v0, x11, Assembler::v0_t);       // vnclip.wx  v16, v0, x11, v0.t
    __ vnclip_wv(v3, v25, v13, Assembler::v0_t);       // vnclip.wv  v3, v25, v13, v0.t
    __ vnclipu_wx(v21, v26, x30, Assembler::v0_t);     // vnclipu.wx  v21, v26, x30, v0.t
    __ vnclipu_wv(v17, v20, v13, Assembler::v0_t);     // vnclipu.wv  v17, v20, v13, v0.t
    __ vssra_vx(v5, v23, x19, Assembler::v0_t);        // vssra.vx  v5, v23, x19, v0.t
    __ vssra_vv(v12, v24, v30, Assembler::v0_t);       // vssra.vv  v12, v24, v30, v0.t
    __ vssrl_vx(v2, v0, x20, Assembler::v0_t);         // vssrl.vx  v2, v0, x20, v0.t
    __ vssrl_vv(v29, v12, v2, Assembler::v0_t);        // vssrl.vv  v29, v12, v2, v0.t
    __ vsmul_vx(v6, v10, x16, Assembler::v0_t);        // vsmul.vx  v6, v10, x16, v0.t
    __ vsmul_vv(v28, v23, v22, Assembler::v0_t);       // vsmul.vv  v28, v23, v22, v0.t
    __ vasubu_vx(v21, v15, x23, Assembler::v0_t);      // vasubu.vx  v21, v15, x23, v0.t
    __ vasubu_vv(v1, v3, v26, Assembler::v0_t);        // vasubu.vv  v1, v3, v26, v0.t
    __ vasub_vx(v16, v15, x23, Assembler::v0_t);       // vasub.vx  v16, v15, x23, v0.t
    __ vasub_vv(v5, v11, v19, Assembler::v0_t);        // vasub.vv  v5, v11, v19, v0.t
    __ vaaddu_vx(v0, v2, x28, Assembler::v0_t);        // vaaddu.vx  v0, v2, x28, v0.t
    __ vaaddu_vv(v1, v23, v13, Assembler::v0_t);       // vaaddu.vv  v1, v23, v13, v0.t
    __ vaadd_vx(v0, v4, x21, Assembler::v0_t);         // vaadd.vx  v0, v4, x21, v0.t
    __ vaadd_vv(v1, v9, v10, Assembler::v0_t);         // vaadd.vv  v1, v9, v10, v0.t
    __ vssub_vx(v8, v26, x10, Assembler::v0_t);        // vssub.vx  v8, v26, x10, v0.t
    __ vssub_vv(v12, v17, v13, Assembler::v0_t);       // vssub.vv  v12, v17, v13, v0.t
    __ vssubu_vx(v7, v30, x10, Assembler::v0_t);       // vssubu.vx  v7, v30, x10, v0.t
    __ vssubu_vv(v23, v21, v29, Assembler::v0_t);      // vssubu.vv  v23, v21, v29, v0.t
    __ vsadd_vx(v10, v2, x25, Assembler::v0_t);        // vsadd.vx  v10, v2, x25, v0.t
    __ vsadd_vv(v8, v1, v13, Assembler::v0_t);         // vsadd.vv  v8, v1, v13, v0.t
    __ vsaddu_vx(v26, v7, x24, Assembler::v0_t);       // vsaddu.vx  v26, v7, x24, v0.t
    __ vsaddu_vv(v22, v21, v0, Assembler::v0_t);       // vsaddu.vv  v22, v21, v0, v0.t
    __ vwmul_vx(v10, v19, x18, Assembler::v0_t);       // vwmul.vx  v10, v19, x18, v0.t
    __ vwmul_vv(v6, v14, v26, Assembler::v0_t);        // vwmul.vv  v6, v14, v26, v0.t
    __ vwmulsu_vx(v28, v23, x5, Assembler::v0_t);      // vwmulsu.vx  v28, v23, x5, v0.t
    __ vwmulsu_vv(v16, v19, v30, Assembler::v0_t);     // vwmulsu.vv  v16, v19, v30, v0.t
    __ vwmulu_vx(v7, v8, x5, Assembler::v0_t);         // vwmulu.vx  v7, v8, x5, v0.t
    __ vwmulu_vv(v27, v22, v4, Assembler::v0_t);       // vwmulu.vv  v27, v22, v4, v0.t
    __ vrem_vx(v25, v1, x13, Assembler::v0_t);         // vrem.vx  v25, v1, x13, v0.t
    __ vrem_vv(v24, v1, v13, Assembler::v0_t);         // vrem.vv  v24, v1, v13, v0.t
    __ vremu_vx(v9, v15, x5, Assembler::v0_t);         // vremu.vx  v9, v15, x5, v0.t
    __ vremu_vv(v1, v16, v12, Assembler::v0_t);        // vremu.vv  v1, v16, v12, v0.t
    __ vdiv_vx(v2, v1, x18, Assembler::v0_t);          // vdiv.vx  v2, v1, x18, v0.t
    __ vdiv_vv(v0, v20, v14, Assembler::v0_t);         // vdiv.vv  v0, v20, v14, v0.t
    __ vdivu_vx(v4, v4, x11, Assembler::v0_t);         // vdivu.vx  v4, v4, x11, v0.t
    __ vdivu_vv(v2, v28, v12, Assembler::v0_t);        // vdivu.vv  v2, v28, v12, v0.t
    __ vmulhsu_vx(v24, v18, x10, Assembler::v0_t);     // vmulhsu.vx  v24, v18, x10, v0.t
    __ vmulhsu_vv(v5, v13, v13, Assembler::v0_t);      // vmulhsu.vv  v5, v13, v13, v0.t
    __ vmulhu_vx(v10, v1, x31, Assembler::v0_t);       // vmulhu.vx  v10, v1, x31, v0.t
    __ vmulhu_vv(v30, v13, v14, Assembler::v0_t);      // vmulhu.vv  v30, v13, v14, v0.t
    __ vmulh_vx(v26, v25, x6, Assembler::v0_t);        // vmulh.vx  v26, v25, x6, v0.t
    __ vmulh_vv(v21, v4, v24, Assembler::v0_t);        // vmulh.vv  v21, v4, v24, v0.t
    __ vmul_vx(v24, v23, x16, Assembler::v0_t);        // vmul.vx  v24, v23, x16, v0.t
    __ vmul_vv(v18, v14, v16, Assembler::v0_t);        // vmul.vv  v18, v14, v16, v0.t
    __ vmax_vx(v23, v20, x25, Assembler::v0_t);        // vmax.vx  v23, v20, x25, v0.t
    __ vmax_vv(v4, v8, v0, Assembler::v0_t);           // vmax.vv  v4, v8, v0, v0.t
    __ vmaxu_vx(v28, v15, x5, Assembler::v0_t);        // vmaxu.vx  v28, v15, x5, v0.t
    __ vmaxu_vv(v15, v23, v6, Assembler::v0_t);        // vmaxu.vv  v15, v23, v6, v0.t
    __ vmin_vx(v20, v10, x17, Assembler::v0_t);        // vmin.vx  v20, v10, x17, v0.t
    __ vmin_vv(v17, v15, v15, Assembler::v0_t);        // vmin.vv  v17, v15, v15, v0.t
    __ vminu_vx(v5, v10, x7, Assembler::v0_t);         // vminu.vx  v5, v10, x7, v0.t
    __ vminu_vv(v20, v16, v9, Assembler::v0_t);        // vminu.vv  v20, v16, v9, v0.t
    __ vmsgt_vx(v17, v31, x20, Assembler::v0_t);       // vmsgt.vx  v17, v31, x20, v0.t
    __ vmsgtu_vx(v17, v21, x13, Assembler::v0_t);      // vmsgtu.vx  v17, v21, x13, v0.t
    __ vmsle_vx(v13, v27, x12, Assembler::v0_t);       // vmsle.vx  v13, v27, x12, v0.t
    __ vmsle_vv(v21, v31, v31, Assembler::v0_t);       // vmsle.vv  v21, v31, v31, v0.t
    __ vmsleu_vx(v27, v16, x14, Assembler::v0_t);      // vmsleu.vx  v27, v16, x14, v0.t
    __ vmsleu_vv(v29, v14, v3, Assembler::v0_t);       // vmsleu.vv  v29, v14, v3, v0.t
    __ vmslt_vx(v15, v0, x8, Assembler::v0_t);         // vmslt.vx  v15, v0, x8, v0.t
    __ vmslt_vv(v3, v25, v25, Assembler::v0_t);        // vmslt.vv  v3, v25, v25, v0.t
    __ vmsltu_vx(v7, v18, x28, Assembler::v0_t);       // vmsltu.vx  v7, v18, x28, v0.t
    __ vmsltu_vv(v15, v24, v21, Assembler::v0_t);      // vmsltu.vv  v15, v24, v21, v0.t
    __ vmsne_vx(v31, v1, x23, Assembler::v0_t);        // vmsne.vx  v31, v1, x23, v0.t
    __ vmsne_vv(v19, v21, v8, Assembler::v0_t);        // vmsne.vv  v19, v21, v8, v0.t
    __ vmseq_vx(v3, v2, x9, Assembler::v0_t);          // vmseq.vx  v3, v2, x9, v0.t
    __ vmseq_vv(v28, v30, v10, Assembler::v0_t);       // vmseq.vv  v28, v30, v10, v0.t
    __ vnsra_wx(v26, v18, x30, Assembler::v0_t);       // vnsra.wx  v26, v18, x30, v0.t
    __ vnsra_wv(v30, v8, v31, Assembler::v0_t);        // vnsra.wv  v30, v8, v31, v0.t
    __ vnsrl_wx(v2, v18, x9, Assembler::v0_t);         // vnsrl.wx  v2, v18, x9, v0.t
    __ vnsrl_wv(v26, v14, v3, Assembler::v0_t);        // vnsrl.wv  v26, v14, v3, v0.t
    __ vsra_vx(v12, v30, x12, Assembler::v0_t);        // vsra.vx  v12, v30, x12, v0.t
    __ vsra_vv(v26, v16, v30, Assembler::v0_t);        // vsra.vv  v26, v16, v30, v0.t
    __ vsrl_vx(v4, v14, x18, Assembler::v0_t);         // vsrl.vx  v4, v14, x18, v0.t
    __ vsrl_vv(v25, v19, v4, Assembler::v0_t);         // vsrl.vv  v25, v19, v4, v0.t
    __ vsll_vx(v3, v16, x8, Assembler::v0_t);          // vsll.vx  v3, v16, x8, v0.t
    __ vsll_vv(v24, v9, v16, Assembler::v0_t);         // vsll.vv  v24, v9, v16, v0.t
    __ vxor_vx(v30, v5, x21, Assembler::v0_t);         // vxor.vx  v30, v5, x21, v0.t
    __ vxor_vv(v10, v0, v17, Assembler::v0_t);         // vxor.vv  v10, v0, v17, v0.t
    __ vor_vx(v2, v31, x11, Assembler::v0_t);          // vor.vx  v2, v31, x11, v0.t
    __ vor_vv(v11, v12, v16, Assembler::v0_t);         // vor.vv  v11, v12, v16, v0.t
    __ vand_vx(v4, v30, x15, Assembler::v0_t);         // vand.vx  v4, v30, x15, v0.t
    __ vand_vv(v25, v17, v7, Assembler::v0_t);         // vand.vv  v25, v17, v7, v0.t
    __ vwsub_wx(v3, v16, x19, Assembler::v0_t);        // vwsub.wx  v3, v16, x19, v0.t
    __ vwsub_wv(v8, v24, v18, Assembler::v0_t);        // vwsub.wv  v8, v24, v18, v0.t
    __ vwsubu_wx(v6, v3, x18, Assembler::v0_t);        // vwsubu.wx  v6, v3, x18, v0.t
    __ vwsubu_wv(v15, v29, v24, Assembler::v0_t);      // vwsubu.wv  v15, v29, v24, v0.t
    __ vwadd_wx(v21, v31, x14, Assembler::v0_t);       // vwadd.wx  v21, v31, x14, v0.t
    __ vwadd_wv(v24, v6, v8, Assembler::v0_t);         // vwadd.wv  v24, v6, v8, v0.t
    __ vwaddu_wx(v9, v4, x27, Assembler::v0_t);        // vwaddu.wx  v9, v4, x27, v0.t
    __ vwaddu_wv(v11, v0, v9, Assembler::v0_t);        // vwaddu.wv  v11, v0, v9, v0.t
    __ vwsub_vx(v10, v20, x30, Assembler::v0_t);       // vwsub.vx  v10, v20, x30, v0.t
    __ vwsub_vv(v22, v2, v13, Assembler::v0_t);        // vwsub.vv  v22, v2, v13, v0.t
    __ vwsubu_vx(v19, v7, x22, Assembler::v0_t);       // vwsubu.vx  v19, v7, x22, v0.t
    __ vwsubu_vv(v9, v30, v4, Assembler::v0_t);        // vwsubu.vv  v9, v30, v4, v0.t
    __ vwadd_vx(v13, v15, x18, Assembler::v0_t);       // vwadd.vx  v13, v15, x18, v0.t
    __ vwadd_vv(v25, v5, v18, Assembler::v0_t);        // vwadd.vv  v25, v5, v18, v0.t
    __ vwaddu_vx(v9, v12, x15, Assembler::v0_t);       // vwaddu.vx  v9, v12, x15, v0.t
    __ vwaddu_vv(v31, v7, v1, Assembler::v0_t);        // vwaddu.vv  v31, v7, v1, v0.t
    __ vsub_vx(v7, v4, x10, Assembler::v0_t);          // vsub.vx  v7, v4, x10, v0.t
    __ vsub_vv(v24, v12, v10, Assembler::v0_t);        // vsub.vv  v24, v12, v10, v0.t
    __ vadd_vx(v18, v27, x31, Assembler::v0_t);        // vadd.vx  v18, v27, x31, v0.t
    __ vadd_vv(v10, v24, v3, Assembler::v0_t);         // vadd.vv  v10, v24, v3, v0.t
    __ vcompress_vm(v13, v8, v17);                     // vcompress.vm  v13, v8, v17
    __ vmxnor_mm(v26, v4, v28);                        // vmxnor.mm  v26, v4, v28
    __ vmornot_mm(v24, v13, v1);                       // vmornot.mm  v24, v13, v1
    __ vmnor_mm(v26, v23, v23);                        // vmnor.mm  v26, v23, v23
    __ vmor_mm(v28, v30, v20);                         // vmor.mm  v28, v30, v20
    __ vmxor_mm(v21, v14, v3);                         // vmxor.mm  v21, v14, v3
    __ vmandnot_mm(v0, v10, v14);                      // vmandnot.mm  v0, v10, v14
    __ vmnand_mm(v13, v23, v10);                       // vmnand.mm  v13, v23, v10
    __ vmand_mm(v14, v3, v14);                         // vmand.mm  v14, v3, v14
    __ vrgather_vx(v16, v15, x31);                     // vrgather.vx  v16, v15, x31
    __ vrgather_vv(v5, v3, v21);                       // vrgather.vv  v5, v3, v21
    __ vslide1down_vx(v31, v25, x10);                  // vslide1down.vx  v31, v25, x10
    __ vslidedown_vx(v11, v2, x21);                    // vslidedown.vx  v11, v2, x21
    __ vslide1up_vx(v30, v21, x22);                    // vslide1up.vx  v30, v21, x22
    __ vslideup_vx(v1, v5, x13);                       // vslideup.vx  v1, v5, x13
    __ vfwredsum_vs(v15, v10, v24);                    // vfwredsum.vs  v15, v10, v24
    __ vfwredosum_vs(v10, v25, v31);                   // vfwredosum.vs  v10, v25, v31
    __ vfredsum_vs(v1, v28, v25);                      // vfredsum.vs  v1, v28, v25
    __ vfredosum_vs(v26, v15, v12);                    // vfredosum.vs  v26, v15, v12
    __ vfredmin_vs(v24, v6, v3);                       // vfredmin.vs  v24, v6, v3
    __ vfredmax_vs(v9, v0, v22);                       // vfredmax.vs  v9, v0, v22
    __ vredsum_vs(v30, v22, v20);                      // vredsum.vs  v30, v22, v20
    __ vredand_vs(v27, v24, v14);                      // vredand.vs  v27, v24, v14
    __ vredor_vs(v31, v27, v24);                       // vredor.vs  v31, v27, v24
    __ vredxor_vs(v30, v15, v22);                      // vredxor.vs  v30, v15, v22
    __ vredminu_vs(v26, v5, v9);                       // vredminu.vs  v26, v5, v9
    __ vredmin_vs(v16, v13, v14);                      // vredmin.vs  v16, v13, v14
    __ vredmaxu_vs(v9, v30, v12);                      // vredmaxu.vs  v9, v30, v12
    __ vredmax_vs(v20, v1, v19);                       // vredmax.vs  v20, v1, v19
    __ vwredsumu_vs(v1, v4, v18);                      // vwredsumu.vs  v1, v4, v18
    __ vwredsum_vs(v15, v17, v5);                      // vwredsum.vs  v15, v17, v5
    __ vmfge_vf(v25, v9, f21);                         // vmfge.vf  v25, v9, f21
    __ vmfgt_vf(v30, v29, f31);                        // vmfgt.vf  v30, v29, f31
    __ vmfle_vf(v10, v14, f14);                        // vmfle.vf  v10, v14, f14
    __ vmfle_vv(v25, v12, v30);                        // vmfle.vv  v25, v12, v30
    __ vmflt_vf(v29, v14, f23);                        // vmflt.vf  v29, v14, f23
    __ vmflt_vv(v18, v9, v11);                         // vmflt.vv  v18, v9, v11
    __ vmfne_vf(v10, v0, f21);                         // vmfne.vf  v10, v0, f21
    __ vmfne_vv(v10, v13, v19);                        // vmfne.vv  v10, v13, v19
    __ vmfeq_vf(v18, v24, f12);                        // vmfeq.vf  v18, v24, f12
    __ vmfeq_vv(v18, v28, v29);                        // vmfeq.vv  v18, v28, v29
    __ vfslide1down_vf(v27, v30, f26);                 // vfslide1down.vf  v27, v30, f26
    __ vfslide1up_vf(v10, v27, f16);                   // vfslide1up.vf  v10, v27, f16
    __ vfsgnjx_vf(v3, v16, f5);                        // vfsgnjx.vf  v3, v16, f5
    __ vfsgnjx_vv(v13, v5, v11);                       // vfsgnjx.vv  v13, v5, v11
    __ vfsgnjn_vf(v21, v9, f25);                       // vfsgnjn.vf  v21, v9, f25
    __ vfsgnjn_vv(v30, v7, v18);                       // vfsgnjn.vv  v30, v7, v18
    __ vfsgnj_vf(v7, v1, f24);                         // vfsgnj.vf  v7, v1, f24
    __ vfsgnj_vv(v28, v24, v20);                       // vfsgnj.vv  v28, v24, v20
    __ vfmax_vf(v12, v26, f23);                        // vfmax.vf  v12, v26, f23
    __ vfmax_vv(v6, v25, v23);                         // vfmax.vv  v6, v25, v23
    __ vfmin_vf(v0, v23, f11);                         // vfmin.vf  v0, v23, f11
    __ vfmin_vv(v8, v7, v15);                          // vfmin.vv  v8, v7, v15
    __ vfwmul_vf(v24, v23, f15);                       // vfwmul.vf  v24, v23, f15
    __ vfwmul_vv(v11, v13, v22);                       // vfwmul.vv  v11, v13, v22
    __ vfdiv_vf(v3, v1, f0);                           // vfdiv.vf  v3, v1, f0
    __ vfdiv_vv(v23, v26, v14);                        // vfdiv.vv  v23, v26, v14
    __ vfmul_vf(v10, v7, f14);                         // vfmul.vf  v10, v7, f14
    __ vfmul_vv(v0, v2, v9);                           // vfmul.vv  v0, v2, v9
    __ vfrdiv_vf(v13, v31, f10);                       // vfrdiv.vf  v13, v31, f10
    __ vfwsub_wf(v23, v5, f5);                         // vfwsub.wf  v23, v5, f5
    __ vfwsub_wv(v2, v10, v21);                        // vfwsub.wv  v2, v10, v21
    __ vfwsub_vf(v28, v1, f21);                        // vfwsub.vf  v28, v1, f21
    __ vfwsub_vv(v24, v23, v4);                        // vfwsub.vv  v24, v23, v4
    __ vfwadd_wf(v11, v4, f9);                         // vfwadd.wf  v11, v4, f9
    __ vfwadd_wv(v3, v23, v4);                         // vfwadd.wv  v3, v23, v4
    __ vfwadd_vf(v26, v14, f15);                       // vfwadd.vf  v26, v14, f15
    __ vfwadd_vv(v24, v31, v14);                       // vfwadd.vv  v24, v31, v14
    __ vfsub_vf(v26, v17, f24);                        // vfsub.vf  v26, v17, f24
    __ vfsub_vv(v28, v1, v21);                         // vfsub.vv  v28, v1, v21
    __ vfadd_vf(v13, v7, f17);                         // vfadd.vf  v13, v7, f17
    __ vfadd_vv(v7, v29, v8);                          // vfadd.vv  v7, v29, v8
    __ vfrsub_vf(v4, v23, f12);                        // vfrsub.vf  v4, v23, f12
    __ vnclip_wx(v7, v29, x18);                        // vnclip.wx  v7, v29, x18
    __ vnclip_wv(v28, v1, v7);                         // vnclip.wv  v28, v1, v7
    __ vnclipu_wx(v25, v16, x12);                      // vnclipu.wx  v25, v16, x12
    __ vnclipu_wv(v4, v10, v1);                        // vnclipu.wv  v4, v10, v1
    __ vssra_vx(v27, v23, x12);                        // vssra.vx  v27, v23, x12
    __ vssra_vv(v2, v29, v23);                         // vssra.vv  v2, v29, v23
    __ vssrl_vx(v2, v0, x31);                          // vssrl.vx  v2, v0, x31
    __ vssrl_vv(v0, v4, v12);                          // vssrl.vv  v0, v4, v12
    __ vsmul_vx(v18, v28, x24);                        // vsmul.vx  v18, v28, x24
    __ vsmul_vv(v11, v31, v26);                        // vsmul.vv  v11, v31, v26
    __ vasubu_vx(v2, v23, x20);                        // vasubu.vx  v2, v23, x20
    __ vasubu_vv(v29, v3, v7);                         // vasubu.vv  v29, v3, v7
    __ vasub_vx(v18, v8, x16);                         // vasub.vx  v18, v8, x16
    __ vasub_vv(v30, v16, v28);                        // vasub.vv  v30, v16, v28
    __ vaaddu_vx(v11, v16, x18);                       // vaaddu.vx  v11, v16, x18
    __ vaaddu_vv(v5, v14, v17);                        // vaaddu.vv  v5, v14, v17
    __ vaadd_vx(v5, v9, x16);                          // vaadd.vx  v5, v9, x16
    __ vaadd_vv(v8, v25, v8);                          // vaadd.vv  v8, v25, v8
    __ vssub_vx(v10, v6, x10);                         // vssub.vx  v10, v6, x10
    __ vssub_vv(v11, v27, v26);                        // vssub.vv  v11, v27, v26
    __ vssubu_vx(v14, v9, x29);                        // vssubu.vx  v14, v9, x29
    __ vssubu_vv(v31, v18, v17);                       // vssubu.vv  v31, v18, v17
    __ vsadd_vx(v28, v24, x21);                        // vsadd.vx  v28, v24, x21
    __ vsadd_vv(v3, v12, v31);                         // vsadd.vv  v3, v12, v31
    __ vsaddu_vx(v9, v19, x10);                        // vsaddu.vx  v9, v19, x10
    __ vsaddu_vv(v4, v20, v27);                        // vsaddu.vv  v4, v20, v27
    __ vwmul_vx(v6, v25, x23);                         // vwmul.vx  v6, v25, x23
    __ vwmul_vv(v5, v10, v16);                         // vwmul.vv  v5, v10, v16
    __ vwmulsu_vx(v3, v22, x7);                        // vwmulsu.vx  v3, v22, x7
    __ vwmulsu_vv(v13, v10, v3);                       // vwmulsu.vv  v13, v10, v3
    __ vwmulu_vx(v14, v25, x27);                       // vwmulu.vx  v14, v25, x27
    __ vwmulu_vv(v11, v19, v8);                        // vwmulu.vv  v11, v19, v8
    __ vrem_vx(v30, v0, x14);                          // vrem.vx  v30, v0, x14
    __ vrem_vv(v29, v31, v10);                         // vrem.vv  v29, v31, v10
    __ vremu_vx(v3, v6, x8);                           // vremu.vx  v3, v6, x8
    __ vremu_vv(v29, v27, v11);                        // vremu.vv  v29, v27, v11
    __ vdiv_vx(v31, v29, x30);                         // vdiv.vx  v31, v29, x30
    __ vdiv_vv(v18, v22, v0);                          // vdiv.vv  v18, v22, v0
    __ vdivu_vx(v0, v9, x8);                           // vdivu.vx  v0, v9, x8
    __ vdivu_vv(v9, v1, v12);                          // vdivu.vv  v9, v1, v12
    __ vmulhsu_vx(v1, v30, x29);                       // vmulhsu.vx  v1, v30, x29
    __ vmulhsu_vv(v30, v30, v29);                      // vmulhsu.vv  v30, v30, v29
    __ vmulhu_vx(v8, v12, x11);                        // vmulhu.vx  v8, v12, x11
    __ vmulhu_vv(v14, v12, v14);                       // vmulhu.vv  v14, v12, v14
    __ vmulh_vx(v24, v8, x14);                         // vmulh.vx  v24, v8, x14
    __ vmulh_vv(v6, v6, v0);                           // vmulh.vv  v6, v6, v0
    __ vmul_vx(v26, v7, x30);                          // vmul.vx  v26, v7, x30
    __ vmul_vv(v23, v5, v0);                           // vmul.vv  v23, v5, v0
    __ vmax_vx(v23, v12, x20);                         // vmax.vx  v23, v12, x20
    __ vmax_vv(v28, v18, v1);                          // vmax.vv  v28, v18, v1
    __ vmaxu_vx(v3, v21, x24);                         // vmaxu.vx  v3, v21, x24
    __ vmaxu_vv(v6, v7, v8);                           // vmaxu.vv  v6, v7, v8
    __ vmin_vx(v8, v1, x14);                           // vmin.vx  v8, v1, x14
    __ vmin_vv(v27, v18, v9);                          // vmin.vv  v27, v18, v9
    __ vminu_vx(v19, v25, x11);                        // vminu.vx  v19, v25, x11
    __ vminu_vv(v5, v12, v27);                         // vminu.vv  v5, v12, v27
    __ vmsgt_vx(v14, v23, x26);                        // vmsgt.vx  v14, v23, x26
    __ vmsgtu_vx(v25, v22, x12);                       // vmsgtu.vx  v25, v22, x12
    __ vmsle_vx(v13, v15, x26);                        // vmsle.vx  v13, v15, x26
    __ vmsle_vv(v25, v27, v15);                        // vmsle.vv  v25, v27, v15
    __ vmsleu_vx(v1, v7, x11);                         // vmsleu.vx  v1, v7, x11
    __ vmsleu_vv(v15, v26, v4);                        // vmsleu.vv  v15, v26, v4
    __ vmslt_vx(v4, v24, x25);                         // vmslt.vx  v4, v24, x25
    __ vmslt_vv(v20, v6, v7);                          // vmslt.vv  v20, v6, v7
    __ vmsltu_vx(v17, v3, x19);                        // vmsltu.vx  v17, v3, x19
    __ vmsltu_vv(v25, v7, v1);                         // vmsltu.vv  v25, v7, v1
    __ vmsne_vx(v25, v15, x12);                        // vmsne.vx  v25, v15, x12
    __ vmsne_vv(v17, v7, v20);                         // vmsne.vv  v17, v7, v20
    __ vmseq_vx(v1, v1, x27);                          // vmseq.vx  v1, v1, x27
    __ vmseq_vv(v19, v2, v24);                         // vmseq.vv  v19, v2, v24
    __ vnsra_wx(v1, v31, x10);                         // vnsra.wx  v1, v31, x10
    __ vnsra_wv(v29, v1, v19);                         // vnsra.wv  v29, v1, v19
    __ vnsrl_wx(v6, v30, x12);                         // vnsrl.wx  v6, v30, x12
    __ vnsrl_wv(v22, v13, v24);                        // vnsrl.wv  v22, v13, v24
    __ vsra_vx(v0, v21, x6);                           // vsra.vx  v0, v21, x6
    __ vsra_vv(v12, v13, v2);                          // vsra.vv  v12, v13, v2
    __ vsrl_vx(v1, v0, x6);                            // vsrl.vx  v1, v0, x6
    __ vsrl_vv(v13, v18, v9);                          // vsrl.vv  v13, v18, v9
    __ vsll_vx(v4, v29, x17);                          // vsll.vx  v4, v29, x17
    __ vsll_vv(v23, v6, v27);                          // vsll.vv  v23, v6, v27
    __ vxor_vx(v24, v3, x14);                          // vxor.vx  v24, v3, x14
    __ vxor_vv(v28, v26, v12);                         // vxor.vv  v28, v26, v12
    __ vor_vx(v17, v9, x19);                           // vor.vx  v17, v9, x19
    __ vor_vv(v11, v26, v30);                          // vor.vv  v11, v26, v30
    __ vand_vx(v3, v5, x7);                            // vand.vx  v3, v5, x7
    __ vand_vv(v7, v2, v5);                            // vand.vv  v7, v2, v5
    __ vwsub_wx(v31, v19, x19);                        // vwsub.wx  v31, v19, x19
    __ vwsub_wv(v28, v31, v24);                        // vwsub.wv  v28, v31, v24
    __ vwsubu_wx(v23, v18, x5);                        // vwsubu.wx  v23, v18, x5
    __ vwsubu_wv(v21, v15, v12);                       // vwsubu.wv  v21, v15, v12
    __ vwadd_wx(v25, v0, x5);                          // vwadd.wx  v25, v0, x5
    __ vwadd_wv(v8, v0, v3);                           // vwadd.wv  v8, v0, v3
    __ vwaddu_wx(v22, v29, x19);                       // vwaddu.wx  v22, v29, x19
    __ vwaddu_wv(v20, v19, v17);                       // vwaddu.wv  v20, v19, v17
    __ vwsub_vx(v2, v27, x30);                         // vwsub.vx  v2, v27, x30
    __ vwsub_vv(v23, v4, v8);                          // vwsub.vv  v23, v4, v8
    __ vwsubu_vx(v5, v18, x19);                        // vwsubu.vx  v5, v18, x19
    __ vwsubu_vv(v9, v2, v16);                         // vwsubu.vv  v9, v2, v16
    __ vwadd_vx(v15, v8, x14);                         // vwadd.vx  v15, v8, x14
    __ vwadd_vv(v13, v19, v6);                         // vwadd.vv  v13, v19, v6
    __ vwaddu_vx(v16, v0, x27);                        // vwaddu.vx  v16, v0, x27
    __ vwaddu_vv(v8, v23, v28);                        // vwaddu.vv  v8, v23, v28
    __ vsub_vx(v26, v1, x19);                          // vsub.vx  v26, v1, x19
    __ vsub_vv(v26, v21, v5);                          // vsub.vv  v26, v21, v5
    __ vadd_vx(v22, v18, x29);                         // vadd.vx  v22, v18, x29
    __ vadd_vv(v24, v17, v18);                         // vadd.vv  v24, v17, v18
    __ vcompress_vm(v27, v15, v9);                     // vcompress.vm  v27, v15, v9
    __ vmxnor_mm(v4, v8, v15);                         // vmxnor.mm  v4, v8, v15
    __ vmornot_mm(v0, v15, v15);                       // vmornot.mm  v0, v15, v15
    __ vmnor_mm(v8, v19, v10);                         // vmnor.mm  v8, v19, v10
    __ vmor_mm(v16, v27, v3);                          // vmor.mm  v16, v27, v3
    __ vmxor_mm(v18, v23, v18);                        // vmxor.mm  v18, v23, v18
    __ vmandnot_mm(v2, v5, v12);                       // vmandnot.mm  v2, v5, v12
    __ vmnand_mm(v16, v14, v0);                        // vmnand.mm  v16, v14, v0
    __ vmand_mm(v10, v2, v4);                          // vmand.mm  v10, v2, v4

    // TwoVecRegImmVOp
    __ vmadc_vim(v25, v2, -14, v0);                    // vmadc.vim  v25, v2, -14, v0
    __ vadc_vim(v4, v13, 3, v0);                       // vadc.vim  v4, v13, 3, v0
    __ vmerge_vim(v8, v11, -8, v0);                    // vmerge.vim  v8, v11, -8, v0

    // TwoVecRegImmOp
    __ vsadd_vi(v13, v12, -5, Assembler::v0_t);        // vsadd.vi  v13, v12, -5, v0.t
    __ vsaddu_vi(v30, v11, -9, Assembler::v0_t);       // vsaddu.vi  v30, v11, -9, v0.t
    __ vmsgt_vi(v26, v18, -16, Assembler::v0_t);       // vmsgt.vi  v26, v18, -16, v0.t
    __ vmsgtu_vi(v15, v29, 14, Assembler::v0_t);       // vmsgtu.vi  v15, v29, 14, v0.t
    __ vmsle_vi(v25, v12, 8, Assembler::v0_t);         // vmsle.vi  v25, v12, 8, v0.t
    __ vmsleu_vi(v21, v11, 12, Assembler::v0_t);       // vmsleu.vi  v21, v11, 12, v0.t
    __ vmsne_vi(v8, v15, 5, Assembler::v0_t);          // vmsne.vi  v8, v15, 5, v0.t
    __ vmseq_vi(v21, v27, -3, Assembler::v0_t);        // vmseq.vi  v21, v27, -3, v0.t
    __ vxor_vi(v31, v22, -2, Assembler::v0_t);         // vxor.vi  v31, v22, -2, v0.t
    __ vor_vi(v3, v19, -16, Assembler::v0_t);          // vor.vi  v3, v19, -16, v0.t
    __ vand_vi(v28, v13, -1, Assembler::v0_t);         // vand.vi  v28, v13, -1, v0.t
    __ vadd_vi(v10, v19, 7, Assembler::v0_t);          // vadd.vi  v10, v19, 7, v0.t
    __ vsadd_vi(v21, v11, -12);                        // vsadd.vi  v21, v11, -12
    __ vsaddu_vi(v6, v11, -1);                         // vsaddu.vi  v6, v11, -1
    __ vmsgt_vi(v30, v22, -10);                        // vmsgt.vi  v30, v22, -10
    __ vmsgtu_vi(v17, v8, 12);                         // vmsgtu.vi  v17, v8, 12
    __ vmsle_vi(v12, v7, 14);                          // vmsle.vi  v12, v7, 14
    __ vmsleu_vi(v14, v13, -11);                       // vmsleu.vi  v14, v13, -11
    __ vmsne_vi(v6, v20, -11);                         // vmsne.vi  v6, v20, -11
    __ vmseq_vi(v26, v8, 12);                          // vmseq.vi  v26, v8, 12
    __ vxor_vi(v15, v24, 9);                           // vxor.vi  v15, v24, 9
    __ vor_vi(v9, v2, -7);                             // vor.vi  v9, v2, -7
    __ vand_vi(v22, v31, 14);                          // vand.vi  v22, v31, 14
    __ vadd_vi(v19, v25, -15);                         // vadd.vi  v19, v25, -15

    // ImmSubOp
    __ vrsub_vi(v26, 15, v5, Assembler::v0_t);         // vrsub.vi  v26, v5, 15, v0.t
    __ vrsub_vi(v6, 14, v8);                           // vrsub.vi  v6, v8, 14

    // TwoVecRegUnsignedImmOp
    __ vrgather_vi(v4, v27, 31u, Assembler::v0_t);     // vrgather.vi  v4, v27, 31, v0.t
    __ vslidedown_vi(v20, v9, 21u, Assembler::v0_t);   // vslidedown.vi  v20, v9, 21, v0.t
    __ vslideup_vi(v21, v25, 19u, Assembler::v0_t);    // vslideup.vi  v21, v25, 19, v0.t
    __ vnclip_wi(v5, v13, 0u, Assembler::v0_t);        // vnclip.wi  v5, v13, 0, v0.t
    __ vnclipu_wi(v2, v1, 28u, Assembler::v0_t);       // vnclipu.wi  v2, v1, 28, v0.t
    __ vssra_vi(v19, v12, 30u, Assembler::v0_t);       // vssra.vi  v19, v12, 30, v0.t
    __ vssrl_vi(v2, v0, 1u, Assembler::v0_t);          // vssrl.vi  v2, v0, 1, v0.t
    __ vnsra_wi(v11, v7, 4u, Assembler::v0_t);         // vnsra.wi  v11, v7, 4, v0.t
    __ vnsrl_wi(v9, v4, 5u, Assembler::v0_t);          // vnsrl.wi  v9, v4, 5, v0.t
    __ vsra_vi(v11, v21, 12u, Assembler::v0_t);        // vsra.vi  v11, v21, 12, v0.t
    __ vsrl_vi(v19, v10, 21u, Assembler::v0_t);        // vsrl.vi  v19, v10, 21, v0.t
    __ vsll_vi(v17, v5, 22u, Assembler::v0_t);         // vsll.vi  v17, v5, 22, v0.t
    __ vrgather_vi(v13, v17, 1u);                      // vrgather.vi  v13, v17, 1
    __ vslidedown_vi(v10, v9, 18u);                    // vslidedown.vi  v10, v9, 18
    __ vslideup_vi(v9, v0, 0u);                        // vslideup.vi  v9, v0, 0
    __ vnclip_wi(v29, v23, 20u);                       // vnclip.wi  v29, v23, 20
    __ vnclipu_wi(v5, v29, 5u);                        // vnclipu.wi  v5, v29, 5
    __ vssra_vi(v29, v10, 15u);                        // vssra.vi  v29, v10, 15
    __ vssrl_vi(v19, v29, 24u);                        // vssrl.vi  v19, v29, 24
    __ vnsra_wi(v30, v29, 12u);                        // vnsra.wi  v30, v29, 12
    __ vnsrl_wi(v23, v26, 23u);                        // vnsrl.wi  v23, v26, 23
    __ vsra_vi(v18, v14, 9u);                          // vsra.vi  v18, v14, 9
    __ vsrl_vi(v25, v10, 11u);                         // vsrl.vi  v25, v10, 11
    __ vsll_vi(v14, v1, 31u);                          // vsll.vi  v14, v1, 31

    // CfgOp
    __ vsetvl(x6, x18, x7);                            // vsetvl  x6, x18, x7

    __ bind(forth);

/*
riscv64ops.o:     file format elf64-littleriscv


Disassembly of section .text:

0000000000000000 <back>:
   0:   00fe02b3            add t0,t3,a5
   4:   40ef0bb3            sub s7,t5,a4
   8:   011d893b            addw    s2,s11,a7
   c:   40ff853b            subw    a0,t6,a5
  10:   00ab65b3            or  a1,s6,a0
  14:   01afc733            xor a4,t6,s10
  18:   03cc0933            mul s2,s8,t3
  1c:   02e413b3            mulh    t2,s0,a4
  20:   036aa733            mulhsu  a4,s5,s6
  24:   0257ba33            mulhu   s4,a5,t0
  28:   0292c333            div t1,t0,s1
  2c:   03d8d533            divu    a0,a7,t4
  30:   0372e7b3            rem a5,t0,s7
  34:   0357ffb3            remu    t6,a5,s5
  38:   03be8a3b            mulw    s4,t4,s11
  3c:   0355ca3b            divw    s4,a1,s5
  40:   02fbd3bb            divuw   t2,s7,a5
  44:   03dce8bb            remw    a7,s9,t4
  48:   02577a3b            remuw   s4,a4,t0
  4c:   01967a33            and s4,a2,s9
  50:   5d660f93            addi    t6,a2,1494
  54:   834a831b            addiw   t1,s5,-1996
  58:   be9e6d93            ori s11,t3,-1047
  5c:   de0acb13            xori    s6,s5,-544
  60:   1efb7d13            andi    s10,s6,495
  64:   f66da993            slti    s3,s11,-154
  68:   49638a67            jalr    s4,1174(t2)
  6c:   1fd2bc93            sltiu   s9,t0,509
  70:   0000006f            j   70 <back+0x70>
  74:   f8dff06f            j   0 <back>
  78:   6a50006f            j   f1c <forth>
  7c:   000000ef            jal ra,7c <back+0x7c>
  80:   f81ff0ef            jal ra,0 <back>
  84:   699000ef            jal ra,f1c <forth>
  88:   0daf8837            lui a6,0xdaf8
  8c:   7068cb97            auipc   s7,0x7068c
  90:   000d1063            bnez    s10,90 <back+0x90>
  94:   f60d16e3            bnez    s10,0 <back>
  98:   680d12e3            bnez    s10,f1c <forth>
  9c:   000f8063            beqz    t6,9c <back+0x9c>
  a0:   f60f80e3            beqz    t6,0 <back>
  a4:   660f8ce3            beqz    t6,f1c <forth>
  a8:   01ce9063            bne t4,t3,a8 <back+0xa8>
  ac:   f5ce9ae3            bne t4,t3,0 <back>
  b0:   67ce96e3            bne t4,t3,f1c <forth>
  b4:   01ba0063            beq s4,s11,b4 <back+0xb4>
  b8:   f5ba04e3            beq s4,s11,0 <back>
  bc:   67ba00e3            beq s4,s11,f1c <forth>
  c0:   009ad063            bge s5,s1,c0 <back+0xc0>
  c4:   f29adee3            bge s5,s1,0 <back>
  c8:   649adae3            bge s5,s1,f1c <forth>
  cc:   01e67063            bgeu    a2,t5,cc <back+0xcc>
  d0:   f3e678e3            bgeu    a2,t5,0 <back>
  d4:   65e674e3            bgeu    a2,t5,f1c <forth>
  d8:   01c8c063            blt a7,t3,d8 <back+0xd8>
  dc:   f3c8c2e3            blt a7,t3,0 <back>
  e0:   63c8cee3            blt a7,t3,f1c <forth>
  e4:   019ae063            bltu    s5,s9,e4 <back+0xe4>
  e8:   f19aece3            bltu    s5,s9,0 <back>
  ec:   639ae8e3            bltu    s5,s9,f1c <forth>
  f0:   006c9fb3            sll t6,s9,t1
  f4:   012dd3b3            srl t2,s11,s2
  f8:   4156deb3            sra t4,a3,s5
  fc:   416bda3b            sraw    s4,s7,s6
 100:   016c9fbb            sllw    t6,s9,s6
 104:   0099dc3b            srlw    s8,s3,s1
 108:   00629d93            slli    s11,t0,0x6
 10c:   0049db13            srli    s6,s3,0x4
 110:   40f85293            srai    t0,a6,0xf
 114:   00a3129b            slliw   t0,t1,0xa
 118:   003eda9b            srliw   s5,t4,0x3
 11c:   4098de1b            sraiw   t3,a7,0x9
 120:   00000013            nop
 124:   00000073            ecall
 128:   00100073            ebreak
 12c:   0000100f            fence.i
 130:   01f0000f            fence   w,iorw
 134:   1d3f29af            sc.w.aq s3,s3,(t5)
 138:   0cc5a5af            amoswap.w.aq    a1,a2,(a1)
 13c:   05beab2f            amoadd.w.aq s6,s11,(t4)
 140:   246aab2f            amoxor.w.aq s6,t1,(s5)
 144:   64efab2f            amoand.w.aq s6,a4,(t6)
 148:   459ea3af            amoor.w.aq  t2,s9,(t4)
 14c:   84b3282f            amomin.w.aq a6,a1,(t1)
 150:   a51bae2f            amomax.w.aq t3,a7,(s7)
 154:   c52fa72f            amominu.w.aq    a4,s2,(t6)
 158:   e46facaf            amomaxu.w.aq    s9,t1,(t6)
 15c:   14072d2f            lr.w.aq s10,(a4)
 160:   1bd6232f            sc.w.rl t1,t4,(a2)
 164:   0af428af            amoswap.w.rl    a7,a5,(s0)
 168:   03cf2b2f            amoadd.w.rl s6,t3,(t5)
 16c:   226ca6af            amoxor.w.rl a3,t1,(s9)
 170:   63a9a82f            amoand.w.rl a6,s10,(s3)
 174:   4265adaf            amoor.w.rl  s11,t1,(a1)
 178:   8387af2f            amomin.w.rl t5,s8,(a5)
 17c:   a3a3a62f            amomax.w.rl a2,s10,(t2)
 180:   c28e2e2f            amominu.w.rl    t3,s0,(t3)
 184:   e3b8ad2f            amomaxu.w.rl    s10,s11,(a7)
 188:   120c2faf            lr.w.rl t6,(s8)
 18c:   1cadbb2f            sc.d.aq s6,a0,(s11)
 190:   0da3332f            amoswap.d.aq    t1,s10,(t1)
 194:   05eeb8af            amoadd.d.aq a7,t5,(t4)
 198:   2487b8af            amoxor.d.aq a7,s0,(a5)
 19c:   645c3faf            amoand.d.aq t6,t0,(s8)
 1a0:   45c635af            amoor.d.aq  a1,t3,(a2)
 1a4:   85aa3d2f            amomin.d.aq s10,s10,(s4)
 1a8:   a524b3af            amomax.d.aq t2,s2,(s1)
 1ac:   c4f5392f            amominu.d.aq    s2,a5,(a0)
 1b0:   e4abbaaf            amomaxu.d.aq    s5,a0,(s7)
 1b4:   14083aaf            lr.d.aq s5,(a6)
 1b8:   1b45b42f            sc.d.rl s0,s4,(a1)
 1bc:   0af63a2f            amoswap.d.rl    s4,a5,(a2)
 1c0:   026f39af            amoadd.d.rl s3,t1,(t5)
 1c4:   238eb2af            amoxor.d.rl t0,s8,(t4)
 1c8:   63d432af            amoand.d.rl t0,t4,(s0)
 1cc:   4277b42f            amoor.d.rl  s0,t2,(a5)
 1d0:   8384b82f            amomin.d.rl a6,s8,(s1)
 1d4:   a2dcbe2f            amomax.d.rl t3,a3,(s9)
 1d8:   c3ceb7af            amominu.d.rl    a5,t3,(t4)
 1dc:   e34fbe2f            amomaxu.d.rl    t3,s4,(t6)
 1e0:   12073c2f            lr.d.rl s8,(a4)
 1e4:   001023f3            frflags t2
 1e8:   00202973            frrm    s2
 1ec:   00302573            frcsr   a0
 1f0:   c01024f3            rdtime  s1
 1f4:   c0002b73            rdcycle s6
 1f8:   c02023f3            rdinstret   t2
 1fc:   000b8693            mv  a3,s7
 200:   fffc4b13            not s6,s8
 204:   40e004b3            neg s1,a4
 208:   40800f3b            negw    t5,s0
 20c:   000d099b            sext.w  s3,s10
 210:   001bbb13            seqz    s6,s7
 214:   00703b33            snez    s6,t2
 218:   00072f33            sltz    t5,a4
 21c:   00c027b3            sgtz    a5,a2
 220:   003d9673            fscsr   a2,s11
 224:   002c9973            fsrm    s2,s9
 228:   001b98f3            fsflags a7,s7
 22c:   00deacb3            slt s9,t4,a3
 230:   00beb4b3            sltu    s1,t4,a1
 234:   ef7a14f3            csrrw   s1,0xef7,s4
 238:   5cbda2f3            csrrs   t0,0x5cb,s11
 23c:   ff483df3            csrrc   s11,0xff4,a6
 240:   72b85df3            csrrwi  s11,0x72b,16
 244:   cd7ce473            csrrsi  s0,0xcd7,25
 248:   4cd979f3            csrrci  s3,0x4cd,18
 24c:   a1c02d73            csrr    s10,0xa1c
 250:   e0341073            csrw    0xe03,s0
 254:   8908a073            csrs    0x890,a7
 258:   db9f3073            csrc    0xdb9,t5
 25c:   9ff75073            csrwi   0x9ff,14
 260:   4f4be073            csrsi   0x4f4,23
 264:   8206f073            csrci   0x820,13
 268:   86ca3503            ld  a0,-1940(s4)
 26c:   b1de2b03            lw  s6,-1251(t3)
 270:   cdece803            lwu a6,-802(s9)
 274:   c9949883            lh  a7,-871(s1)
 278:   ad755c83            lhu s9,-1321(a0)
 27c:   b6c60d03            lb  s10,-1172(a2)
 280:   f5fdcc03            lbu s8,-161(s11)
 284:   b3493423            sd  s4,-1240(s2)
 288:   10a92423            sw  a0,264(s2)
 28c:   f6669ba3            sh  t1,-137(a3)
 290:   07368223            sb  s3,100(a3)
 294:   2af9bf87            fld ft11,687(s3)
 298:   d9bdab87            flw fs7,-613(s11)
 29c:   077f3627            fsd fs7,108(t5)
 2a0:   ef89a327            fsw fs8,-282(s3)
 2a4:   580ba4d3            fsqrt.s fs1,fs7,rdn
 2a8:   5a022153            fsqrt.d ft2,ft4,rdn
 2ac:   00f2b253            fadd.s  ft4,ft5,fa5,rup
 2b0:   08073cd3            fsub.s  fs9,fa4,ft0,rup
 2b4:   025c38d3            fadd.d  fa7,fs8,ft5,rup
 2b8:   0bc9b853            fsub.d  fa6,fs3,ft8,rup
 2bc:   11403b53            fmul.s  fs6,ft0,fs4,rup
 2c0:   19283953            fdiv.s  fs2,fa6,fs2,rup
 2c4:   13e0bfd3            fmul.d  ft11,ft1,ft10,rup
 2c8:   1bfe31d3            fdiv.d  ft3,ft8,ft11,rup
 2cc:   1002bdc3            fmadd.s fs11,ft5,ft0,ft2,rup
 2d0:   e8c69cc7            fmsub.s fs9,fa3,fa2,ft9,rtz
 2d4:   dbba3e43            fmadd.d ft8,fs4,fs11,fs11,rup
 2d8:   92189747            fmsub.d fa4,fa7,ft1,fs2,rtz
 2dc:   70f0c24b            fnmsub.s    ft4,ft1,fa5,fa4,rmm
 2e0:   41251bcf            fnmadd.s    fs7,fa0,fs2,fs0,rtz
 2e4:   1bae434b            fnmsub.d    ft6,ft8,fs10,ft3,rmm
 2e8:   827616cf            fnmadd.d    fa3,fa2,ft7,fa6,rtz
 2ec:   e0019bd3            fclass.s    s7,ft3
 2f0:   21080153            fmv.s   ft2,fa6
 2f4:   e20c99d3            fclass.d    s3,fs9
 2f8:   22318b53            fmv.d   fs6,ft3
 2fc:   20c62cd3            fabs.s  fs9,fa2
 300:   20a517d3            fneg.s  fa5,fa0
 304:   23ce2b53            fabs.d  fs6,ft8
 308:   233993d3            fneg.d  ft7,fs3
 30c:   e0018bd3            fmv.x.w s7,ft3
 310:   e20982d3            fmv.x.d t0,fs3
 314:   200904d3            fsgnj.s fs1,fs2,ft0
 318:   20cb9cd3            fsgnjn.s    fs9,fs7,fa2
 31c:   220801d3            fsgnj.d ft3,fa6,ft0
 320:   22909b53            fsgnjn.d    fs6,ft1,fs1
 324:   20562953            fsgnjx.s    fs2,fa2,ft5
 328:   29cf0953            fmin.s  fs2,ft10,ft8
 32c:   2209aed3            fsgnjx.d    ft9,fs3,ft0
 330:   2bc904d3            fmin.d  fs1,fs2,ft8
 334:   29519253            fmax.s  ft4,ft3,fs5
 338:   a170a553            feq.s   a0,ft1,fs7
 33c:   2bab18d3            fmax.d  fa7,fs6,fs10
 340:   a3b0ad53            feq.d   s10,ft1,fs11
 344:   a15e1f53            flt.s   t5,ft8,fs5
 348:   a1b30cd3            fle.s   s9,ft6,fs11
 34c:   a2e01ad3            flt.d   s5,ft0,fa4
 350:   a3308e53            fle.d   t3,ft1,fs3
 354:   c007b5d3            fcvt.w.s    a1,fa5,rup
 358:   c01686d3            fcvt.wu.s   a3,fa3,rne
 35c:   d00da8d3            fcvt.s.w    fa7,s11,rdn
 360:   d01d95d3            fcvt.s.wu   fa1,s11,rtz
 364:   c02d0b53            fcvt.l.s    s6,fs10,rne
 368:   c032ca53            fcvt.lu.s   s4,ft5,rmm
 36c:   d026bad3            fcvt.s.l    fs5,a3,rup
 370:   d03c9e53            fcvt.s.lu   ft8,s9,rtz
 374:   4013a653            fcvt.s.d    fa2,ft7,rdn
 378:   420587d3            fcvt.d.s    fa5,fa1
 37c:   c20a2dd3            fcvt.w.d    s11,fs4,rdn
 380:   c2172b53            fcvt.wu.d   s6,fa4,rdn
 384:   d20c8553            fcvt.d.w    fa0,s9
 388:   d21a04d3            fcvt.d.wu   fs1,s4
 38c:   c22cad53            fcvt.l.d    s10,fs9,rdn
 390:   c23da553            fcvt.lu.d   a0,fs11,rdn
 394:   d22ea053            fcvt.d.l    ft0,t4,rdn
 398:   d23b2753            fcvt.d.lu   fa4,s6,rdn
 39c:   0d6b8caf            vamoswapei8.v   v25,(s7),v22,v25,v0.t
 3a0:   045c8aaf            vamoaddei8.v    v21,(s9),v5,v21,v0.t
 3a4:   249f092f            vamoxorei8.v    v18,(t5),v9,v18,v0.t
 3a8:   64ae03af            vamoandei8.v    v7,(t3),v10,v7,v0.t
 3ac:   44b30c2f            vamoorei8.v v24,(t1),v11,v24,v0.t
 3b0:   85e983af            vamominei8.v    v7,(s3),v30,v7,v0.t
 3b4:   a4598e2f            vamomaxei8.v    v28,(s3),v5,v28,v0.t
 3b8:   c44685af            vamominuei8.v   v11,(a3),v4,v11,v0.t
 3bc:   e555062f            vamomaxuei8.v   v12,(a0),v21,v12,v0.t
 3c0:   0ecb0e2f            vamoswapei8.v   v28,(s6),v12,v28
 3c4:   069b8eaf            vamoaddei8.v    v29,(s7),v9,v29
 3c8:   263e02af            vamoxorei8.v    v5,(t3),v3,v5
 3cc:   66c385af            vamoandei8.v    v11,(t2),v12,v11
 3d0:   460f082f            vamoorei8.v v16,(t5),v0,v16
 3d4:   876f0a2f            vamominei8.v    v20,(t5),v22,v20
 3d8:   a68a832f            vamomaxei8.v    v6,(s5),v8,v6
 3dc:   c61c802f            vamominuei8.v   v0,(s9),v1,v0
 3e0:   e7e30b2f            vamomaxuei8.v   v22,(t1),v30,v22
 3e4:   096285af            vamoswapei8.v   zero,(t0),v22,v11,v0.t
 3e8:   013801af            vamoaddei8.v    zero,(a6),v19,v3,v0.t
 3ec:   206f0c2f            vamoxorei8.v    zero,(t5),v6,v24,v0.t
 3f0:   60b307af            vamoandei8.v    zero,(t1),v11,v15,v0.t
 3f4:   40e487af            vamoorei8.v zero,(s1),v14,v15,v0.t
 3f8:   81bb822f            vamominei8.v    zero,(s7),v27,v4,v0.t
 3fc:   a13a8c2f            vamomaxei8.v    zero,(s5),v19,v24,v0.t
 400:   c1c6062f            vamominuei8.v   zero,(a2),v28,v12,v0.t
 404:   e16904af            vamomaxuei8.v   zero,(s2),v22,v9,v0.t
 408:   0bcd082f            vamoswapei8.v   zero,(s10),v28,v16
 40c:   03f709af            vamoaddei8.v    zero,(a4),v31,v19
 410:   223885af            vamoxorei8.v    zero,(a7),v3,v11
 414:   63a408af            vamoandei8.v    zero,(s0),v26,v17
 418:   42e38aaf            vamoorei8.v zero,(t2),v14,v21
 41c:   83ae02af            vamominei8.v    zero,(t3),v26,v5
 420:   a39d022f            vamomaxei8.v    zero,(s10),v25,v4
 424:   c2cc86af            vamominuei8.v   zero,(s9),v12,v13
 428:   e31e80af            vamomaxuei8.v   zero,(t4),v17,v1
 42c:   0d4e51af            vamoswapei16.v  v3,(t3),v20,v3,v0.t
 430:   04d659af            vamoaddei16.v   v19,(a2),v13,v19,v0.t
 434:   24b2d72f            vamoxorei16.v   v14,(t0),v11,v14,v0.t
 438:   641352af            vamoandei16.v   v5,(t1),v1,v5,v0.t
 43c:   4544d42f            vamoorei16.v    v8,(s1),v20,v8,v0.t
 440:   84e2dbaf            vamominei16.v   v23,(t0),v14,v23,v0.t
 444:   a4add82f            vamomaxei16.v   v16,(s11),v10,v16,v0.t
 448:   c4c9da2f            vamominuei16.v  v20,(s3),v12,v20,v0.t
 44c:   e447d52f            vamomaxuei16.v  v10,(a5),v4,v10,v0.t
 450:   0fad5aaf            vamoswapei16.v  v21,(s10),v26,v21
 454:   06addd2f            vamoaddei16.v   v26,(s11),v10,v26
 458:   260ad5af            vamoxorei16.v   v11,(s5),v0,v11
 45c:   67ead6af            vamoandei16.v   v13,(s5),v30,v13
 460:   468653af            vamoorei16.v    v7,(a2),v8,v7
 464:   8643df2f            vamominei16.v   v30,(t2),v4,v30
 468:   a7a45c2f            vamomaxei16.v   v24,(s0),v26,v24
 46c:   c75956af            vamominuei16.v  v13,(s2),v21,v13
 470:   e644d42f            vamomaxuei16.v  v8,(s1),v4,v8
 474:   0929dbaf            vamoswapei16.v  zero,(s3),v18,v23,v0.t
 478:   010cd9af            vamoaddei16.v   zero,(s9),v16,v19,v0.t
 47c:   2156dbaf            vamoxorei16.v   zero,(a3),v21,v23,v0.t
 480:   6139d5af            vamoandei16.v   zero,(s3),v19,v11,v0.t
 484:   40cb52af            vamoorei16.v    zero,(s6),v12,v5,v0.t
 488:   807c522f            vamominei16.v   zero,(s8),v7,v4,v0.t
 48c:   a1ea5f2f            vamomaxei16.v   zero,(s4),v30,v30,v0.t
 490:   c0b4d1af            vamominuei16.v  zero,(s1),v11,v3,v0.t
 494:   e1f3ddaf            vamomaxuei16.v  zero,(t2),v31,v27,v0.t
 498:   0aa3d1af            vamoswapei16.v  zero,(t2),v10,v3
 49c:   021b562f            vamoaddei16.v   zero,(s6),v1,v12
 4a0:   22a3532f            vamoxorei16.v   zero,(t1),v10,v6
 4a4:   63cddb2f            vamoandei16.v   zero,(s11),v28,v22
 4a8:   438fd22f            vamoorei16.v    zero,(t6),v24,v4
 4ac:   82dd55af            vamominei16.v   zero,(s10),v13,v11
 4b0:   a20edaaf            vamomaxei16.v   zero,(t4),v0,v21
 4b4:   c22b5c2f            vamominuei16.v  zero,(s6),v2,v24
 4b8:   e3cbd8af            vamomaxuei16.v  zero,(s7),v28,v17
 4bc:   0de2eaaf            vamoswapei32.v  v21,(t0),v30,v21,v0.t
 4c0:   05936baf            vamoaddei32.v   v23,(t1),v25,v23,v0.t
 4c4:   240d652f            vamoxorei32.v   v10,(s10),v0,v10,v0.t
 4c8:   65c7e32f            vamoandei32.v   v6,(a5),v28,v6,v0.t
 4cc:   4449ea2f            vamoorei32.v    v20,(s3),v4,v20,v0.t
 4d0:   844b6daf            vamominei32.v   v27,(s6),v4,v27,v0.t
 4d4:   a4bcedaf            vamomaxei32.v   v27,(s9),v11,v27,v0.t
 4d8:   c416ed2f            vamominuei32.v  v26,(a3),v1,v26,v0.t
 4dc:   e4de652f            vamomaxuei32.v  v10,(t3),v13,v10,v0.t
 4e0:   0edf672f            vamoswapei32.v  v14,(t5),v13,v14
 4e4:   0755662f            vamoaddei32.v   v12,(a0),v21,v12
 4e8:   2678e8af            vamoxorei32.v   v17,(a7),v7,v17
 4ec:   67bbe9af            vamoandei32.v   v19,(s7),v27,v19
 4f0:   46a2e42f            vamoorei32.v    v8,(t0),v10,v8
 4f4:   860c60af            vamominei32.v   v1,(s8),v0,v1
 4f8:   a74b6c2f            vamomaxei32.v   v24,(s6),v20,v24
 4fc:   c60fe9af            vamominuei32.v  v19,(t6),v0,v19
 500:   e607eaaf            vamomaxuei32.v  v21,(a5),v0,v21
 504:   085be22f            vamoswapei32.v  zero,(s7),v5,v4,v0.t
 508:   016c6e2f            vamoaddei32.v   zero,(s8),v22,v28,v0.t
 50c:   2177ed2f            vamoxorei32.v   zero,(a5),v23,v26,v0.t
 510:   60efeeaf            vamoandei32.v   zero,(t6),v14,v29,v0.t
 514:   406b6caf            vamoorei32.v    zero,(s6),v6,v25,v0.t
 518:   810ae92f            vamominei32.v   zero,(s5),v16,v18,v0.t
 51c:   a0b4e5af            vamomaxei32.v   zero,(s1),v11,v11,v0.t
 520:   c00760af            vamominuei32.v  zero,(a4),v0,v1,v0.t
 524:   e1676e2f            vamomaxuei32.v  zero,(a4),v22,v28,v0.t
 528:   0b5ce3af            vamoswapei32.v  zero,(s9),v21,v7
 52c:   0353602f            vamoaddei32.v   zero,(t1),v21,v0
 530:   2347e6af            vamoxorei32.v   zero,(a5),v20,v13
 534:   63a4ee2f            vamoandei32.v   zero,(s1),v26,v28
 538:   4238e1af            vamoorei32.v    zero,(a7),v3,v3
 53c:   83eae6af            vamominei32.v   zero,(s5),v30,v13
 540:   a25ce82f            vamomaxei32.v   zero,(s9),v5,v16
 544:   c396692f            vamominuei32.v  zero,(a2),v25,v18
 548:   e26c61af            vamomaxuei32.v  zero,(s8),v6,v3
 54c:   48832ed7            vzext.vf2   v29,v8,v0.t
 550:   490220d7            vzext.vf4   v1,v16,v0.t
 554:   49a12657            vzext.vf8   v12,v26,v0.t
 558:   4893a4d7            vsext.vf2   v9,v9,v0.t
 55c:   49b2a0d7            vsext.vf4   v1,v27,v0.t
 560:   4891ae57            vsext.vf8   v28,v9,v0.t
 564:   40582c57            vpopc.m s8,v5,v0.t
 568:   40b8a457            vfirst.m    s0,v11,v0.t
 56c:   5110af57            vmsbf.m v30,v17,v0.t
 570:   5081a6d7            vmsif.m v13,v8,v0.t
 574:   519125d7            vmsof.m v11,v25,v0.t
 578:   510821d7            viota.m v3,v16,v0.t
 57c:   489010d7            vfcvt.xu.f.v    v1,v9,v0.t
 580:   495094d7            vfcvt.x.f.v v9,v21,v0.t
 584:   48111fd7            vfcvt.f.xu.v    v31,v1,v0.t
 588:   48d196d7            vfcvt.f.x.v v13,v13,v0.t
 58c:   49d31557            vfcvt.rtz.xu.f.v    v10,v29,v0.t
 590:   488390d7            vfcvt.rtz.x.f.v v1,v8,v0.t
 594:   49c41657            vfwcvt.xu.f.v   v12,v28,v0.t
 598:   492496d7            vfwcvt.x.f.v    v13,v18,v0.t
 59c:   494519d7            vfwcvt.f.xu.v   v19,v20,v0.t
 5a0:   48a59ad7            vfwcvt.f.x.v    v21,v10,v0.t
 5a4:   48761dd7            vfwcvt.f.f.v    v27,v7,v0.t
 5a8:   48371dd7            vfwcvt.rtz.xu.f.v   v27,v3,v0.t
 5ac:   481799d7            vfwcvt.rtz.x.f.v    v19,v1,v0.t
 5b0:   48181a57            vfncvt.xu.f.w   v20,v1,v0.t
 5b4:   48a898d7            vfncvt.x.f.w    v17,v10,v0.t
 5b8:   49d91c57            vfncvt.f.xu.w   v24,v29,v0.t
 5bc:   49c99dd7            vfncvt.f.x.w    v27,v28,v0.t
 5c0:   49da1fd7            vfncvt.f.f.w    v31,v29,v0.t
 5c4:   49fa94d7            vfncvt.rod.f.f.w    v9,v31,v0.t
 5c8:   48bb19d7            vfncvt.rtz.xu.f.w   v19,v11,v0.t
 5cc:   49bb9fd7            vfncvt.rtz.x.f.w    v31,v27,v0.t
 5d0:   4df01f57            vfsqrt.v    v30,v31,v0.t
 5d4:   4cd810d7            vfclass.v   v1,v13,v0.t
 5d8:   4a232757            vzext.vf2   v14,v2
 5dc:   4a8227d7            vzext.vf4   v15,v8
 5e0:   4b012457            vzext.vf8   v8,v16
 5e4:   4b93aed7            vsext.vf2   v29,v25
 5e8:   4b72a6d7            vsext.vf4   v13,v23
 5ec:   4bd1af57            vsext.vf8   v30,v29
 5f0:   42582ed7            vpopc.m t4,v5
 5f4:   42e8abd7            vfirst.m    s7,v14
 5f8:   5320a857            vmsbf.m v16,v18
 5fc:   5241a0d7            vmsif.m v1,v4
 600:   533123d7            vmsof.m v7,v19
 604:   53c824d7            viota.m v9,v28
 608:   4a9016d7            vfcvt.xu.f.v    v13,v9
 60c:   4a209ad7            vfcvt.x.f.v v21,v2
 610:   4b311d57            vfcvt.f.xu.v    v26,v19
 614:   4ba19bd7            vfcvt.f.x.v v23,v26
 618:   4a9314d7            vfcvt.rtz.xu.f.v    v9,v9
 61c:   4bd390d7            vfcvt.rtz.x.f.v v1,v29
 620:   4a3411d7            vfwcvt.xu.f.v   v3,v3
 624:   4a249f57            vfwcvt.x.f.v    v30,v2
 628:   4b051857            vfwcvt.f.xu.v   v16,v16
 62c:   4b4591d7            vfwcvt.f.x.v    v3,v20
 630:   4a361c57            vfwcvt.f.f.v    v24,v3
 634:   4a671a57            vfwcvt.rtz.xu.f.v   v20,v6
 638:   4b6798d7            vfwcvt.rtz.x.f.v    v17,v22
 63c:   4a5815d7            vfncvt.xu.f.w   v11,v5
 640:   4ad89c57            vfncvt.x.f.w    v24,v13
 644:   4bf91257            vfncvt.f.xu.w   v4,v31
 648:   4b3994d7            vfncvt.f.x.w    v9,v19
 64c:   4bea19d7            vfncvt.f.f.w    v19,v30
 650:   4aba9157            vfncvt.rod.f.f.w    v2,v11
 654:   4b5b1957            vfncvt.rtz.xu.f.w   v18,v21
 658:   4bfb9357            vfncvt.rtz.x.f.w    v6,v31
 65c:   4e401dd7            vfsqrt.v    v27,v4
 660:   4ee81957            vfclass.v   v18,v14
 664:   9e503ed7            vmv1r.v v29,v5
 668:   9ee0bfd7            vmv2r.v v31,v14
 66c:   9ee1b6d7            vmv4r.v v13,v14
 670:   9e43bcd7            vmv8r.v v25,v4
 674:   43a01f57            vfmv.f.s    ft10,v26
 678:   435025d7            vmv.x.s a1,v21
 67c:   420650d7            vfmv.s.f    v1,fa2
 680:   42036dd7            vmv.s.x v27,t1
 684:   5e07dc57            vfmv.v.f    v24,fa5
 688:   5e064ad7            vmv.v.x v21,a2
 68c:   5e070157            vmv.v.v v2,v14
 690:   5e03bb57            vmv.v.i v22,7
 694:   5208add7            vid.v   v27
 698:   028c0e07            vl1r.v  v28,(s8)
 69c:   028402a7            vs1r.v  v5,(s0)
 6a0:   000c8007            vle8.v  v0,(s9),v0.t
 6a4:   000e5107            vle16.v v2,(t3),v0.t
 6a8:   0005e687            vle32.v v13,(a1),v0.t
 6ac:   00087587            vle64.v v11,(a6),v0.t
 6b0:   00088ea7            vse8.v  v29,(a7),v0.t
 6b4:   00085ca7            vse16.v v25,(a6),v0.t
 6b8:   000b6da7            vse32.v v27,(s6),v0.t
 6bc:   00057927            vse64.v v18,(a0),v0.t
 6c0:   010b8607            vle8ff.v    v12,(s7),v0.t
 6c4:   010f5d87            vle16ff.v   v27,(t5),v0.t
 6c8:   0105e387            vle32ff.v   v7,(a1),v0.t
 6cc:   0105fb87            vle64ff.v   v23,(a1),v0.t
 6d0:   02040b07            vle8.v  v22,(s0)
 6d4:   0209d707            vle16.v v14,(s3)
 6d8:   02076e87            vle32.v v29,(a4)
 6dc:   020cf787            vle64.v v15,(s9)
 6e0:   020e0c27            vse8.v  v24,(t3)
 6e4:   02035927            vse16.v v18,(t1)
 6e8:   020769a7            vse32.v v19,(a4)
 6ec:   020f7ba7            vse64.v v23,(t5)
 6f0:   03048407            vle8ff.v    v8,(s1)
 6f4:   030ad587            vle16ff.v   v11,(s5)
 6f8:   030e6307            vle32ff.v   v6,(t3)
 6fc:   030c7707            vle64ff.v   v14,(s8)
 700:   09a78407            vlse8.v v8,(a5),s10,v0.t
 704:   08ea5f87            vlse16.v    v31,(s4),a4,v0.t
 708:   08ce6507            vlse32.v    v10,(t3),a2,v0.t
 70c:   09dafe07            vlse64.v    v28,(s5),t4,v0.t
 710:   08d283a7            vsse8.v v7,(t0),a3,v0.t
 714:   0966dfa7            vsse16.v    v31,(a3),s6,v0.t
 718:   093667a7            vsse32.v    v15,(a2),s3,v0.t
 71c:   09877ea7            vsse64.v    v29,(a4),s8,v0.t
 720:   0b688c07            vlse8.v v24,(a7),s6
 724:   0b49d707            vlse16.v    v14,(s3),s4
 728:   0b48ed07            vlse32.v    v26,(a7),s4
 72c:   0b25fb07            vlse64.v    v22,(a1),s2
 730:   0bbd01a7            vsse8.v v3,(s10),s11
 734:   0a555da7            vsse16.v    v27,(a0),t0
 738:   0b66ea27            vsse32.v    v20,(a3),s6
 73c:   0aaff027            vsse64.v    v0,(t6),a0
 740:   5da25fd7            vfmerge.vfm v31,v26,ft4,v0
 744:   5d1ac4d7            vmerge.vxm  v9,v17,s5,v0
 748:   5c500dd7            vmerge.vvm  v27,v5,v0,v0
 74c:   482bcdd7            vsbc.vxm    v27,v2,s7,v0
 750:   49a688d7            vsbc.vvm    v17,v26,v13,v0
 754:   406cca57            vadc.vxm    v20,v6,s9,v0
 758:   41788057            vadc.vvm    v0,v23,v17,v0
 75c:   451b4457            vmadc.vxm   v8,v17,s6,v0
 760:   459406d7            vmadc.vvm   v13,v25,v8,v0
 764:   4cc4c557            vmsbc.vxm   v10,v12,s1,v0
 768:   4c6b8f57            vmsbc.vvm   v30,v6,v23,v0
 76c:   fcb6d4d7            vfwnmsac.vf v9,fa3,v11,v0.t
 770:   fc9e9657            vfwnmsac.vv v12,v29,v9,v0.t
 774:   f80ed757            vfwmsac.vf  v14,ft9,v0,v0.t
 778:   f8ec1757            vfwmsac.vv  v14,v24,v14,v0.t
 77c:   f4035857            vfwnmacc.vf v16,ft6,v0,v0.t
 780:   f5fc9cd7            vfwnmacc.vv v25,v25,v31,v0.t
 784:   f0715d57            vfwmacc.vf  v26,ft2,v7,v0.t
 788:   f1799357            vfwmacc.vv  v6,v19,v23,v0.t
 78c:   ad9955d7            vfnmsub.vf  v11,fs2,v25,v0.t
 790:   ac889ed7            vfnmsub.vv  v29,v17,v8,v0.t
 794:   a8145457            vfmsub.vf   v8,fs0,v1,v0.t
 798:   a9981457            vfmsub.vv   v8,v16,v25,v0.t
 79c:   a4a75757            vfnmadd.vf  v14,fa4,v10,v0.t
 7a0:   a4a39e57            vfnmadd.vv  v28,v7,v10,v0.t
 7a4:   a07855d7            vfmadd.vf   v11,fa6,v7,v0.t
 7a8:   a0a119d7            vfmadd.vv   v19,v2,v10,v0.t
 7ac:   bd5bdf57            vfnmsac.vf  v30,fs7,v21,v0.t
 7b0:   bdf31a57            vfnmsac.vv  v20,v6,v31,v0.t
 7b4:   b939dc57            vfmsac.vf   v24,fs3,v19,v0.t
 7b8:   b9a61757            vfmsac.vv   v14,v12,v26,v0.t
 7bc:   b034dbd7            vfmacc.vf   v23,fs1,v3,v0.t
 7c0:   b1f513d7            vfmacc.vv   v7,v10,v31,v0.t
 7c4:   b436ddd7            vfnmacc.vf  v27,fa3,v3,v0.t
 7c8:   b5821557            vfnmacc.vv  v10,v4,v24,v0.t
 7cc:   fc5ee2d7            vwmaccsu.vx v5,t4,v5,v0.t
 7d0:   fc03af57            vwmaccsu.vv v30,v7,v0,v0.t
 7d4:   f49c6dd7            vwmacc.vx   v27,s8,v9,v0.t
 7d8:   f4b124d7            vwmacc.vv   v9,v2,v11,v0.t
 7dc:   f04a6057            vwmaccu.vx  v0,s4,v4,v0.t
 7e0:   f0212b57            vwmaccu.vv  v22,v2,v2,v0.t
 7e4:   f96f65d7            vwmaccus.vx v11,t5,v22,v0.t
 7e8:   ac166b57            vnmsub.vx   v22,a2,v1,v0.t
 7ec:   acc0a9d7            vnmsub.vv   v19,v1,v12,v0.t
 7f0:   a4a66c57            vmadd.vx    v24,a2,v10,v0.t
 7f4:   a5702957            vmadd.vv    v18,v0,v23,v0.t
 7f8:   bd356cd7            vnmsac.vx   v25,a0,v19,v0.t
 7fc:   bc6ea857            vnmsac.vv   v16,v29,v6,v0.t
 800:   b5466557            vmacc.vx    v10,a2,v20,v0.t
 804:   b43320d7            vmacc.vv    v1,v6,v3,v0.t
 808:   ffc2d0d7            vfwnmsac.vf v1,ft5,v28
 80c:   ffb81c57            vfwnmsac.vv v24,v16,v27
 810:   fa785257            vfwmsac.vf  v4,fa6,v7
 814:   fadb1557            vfwmsac.vv  v10,v22,v13
 818:   f7d65757            vfwnmacc.vf v14,fa2,v29
 81c:   f7989357            vfwnmacc.vv v6,v17,v25
 820:   f2ccd857            vfwmacc.vf  v16,fs9,v12
 824:   f21d9457            vfwmacc.vv  v8,v27,v1
 828:   aecad4d7            vfnmsub.vf  v9,fs5,v12
 82c:   aed01257            vfnmsub.vv  v4,v0,v13
 830:   aac1d9d7            vfmsub.vf   v19,ft3,v12
 834:   aa2a14d7            vfmsub.vv   v9,v20,v2
 838:   a681d6d7            vfnmadd.vf  v13,ft3,v8
 83c:   a7e71857            vfnmadd.vv  v16,v14,v30
 840:   a3fddd57            vfmadd.vf   v26,fs11,v31
 844:   a3ca9857            vfmadd.vv   v16,v21,v28
 848:   bf9d5757            vfnmsac.vf  v14,fs10,v25
 84c:   be159957            vfnmsac.vv  v18,v11,v1
 850:   bb04dc57            vfmsac.vf   v24,fs1,v16
 854:   bb5e1157            vfmsac.vv   v2,v28,v21
 858:   b3e7dd57            vfmacc.vf   v26,fa5,v30
 85c:   b3d91fd7            vfmacc.vv   v31,v18,v29
 860:   b6aadad7            vfnmacc.vf  v21,fs5,v10
 864:   b63e1c57            vfnmacc.vv  v24,v28,v3
 868:   fe5666d7            vwmaccsu.vx v13,a2,v5
 86c:   ff59a7d7            vwmaccsu.vv v15,v19,v21
 870:   f7b3e0d7            vwmacc.vx   v1,t2,v27
 874:   f7952857            vwmacc.vv   v16,v10,v25
 878:   f3feead7            vwmaccu.vx  v21,t4,v31
 87c:   f3492957            vwmaccu.vv  v18,v18,v20
 880:   fbac6f57            vwmaccus.vx v30,s8,v26
 884:   afa6e9d7            vnmsub.vx   v19,a3,v26
 888:   aec62ad7            vnmsub.vv   v21,v12,v12
 88c:   a7bb6557            vmadd.vx    v10,s6,v27
 890:   a7452b57            vmadd.vv    v22,v10,v20
 894:   be3be1d7            vnmsac.vx   v3,s7,v3
 898:   be7e2157            vnmsac.vv   v2,v28,v7
 89c:   b688ecd7            vmacc.vx    v25,a7,v8
 8a0:   b6982657            vmacc.vv    v12,v16,v9
 8a4:   0daf43d7            vrsub.vx    v7,v26,t5,v0.t
 8a8:   0e5f49d7            vrsub.vx    v19,v5,t5
 8ac:   3044cbd7            vrgather.vx v23,v4,s1,v0.t
 8b0:   317180d7            vrgather.vv v1,v23,v3,v0.t
 8b4:   3c44efd7            vslide1down.vx  v31,v4,s1,v0.t
 8b8:   3df9c857            vslidedown.vx   v16,v31,s3,v0.t
 8bc:   38a46d57            vslide1up.vx    v26,v10,s0,v0.t
 8c0:   385e43d7            vslideup.vx v7,v5,t3,v0.t
 8c4:   c5ff1257            vfwredsum.vs    v4,v31,v30,v0.t
 8c8:   cc8d1f57            vfwredosum.vs   v30,v8,v26,v0.t
 8cc:   04791ad7            vfredsum.vs v21,v7,v18,v0.t
 8d0:   0c031757            vfredosum.vs    v14,v0,v6,v0.t
 8d4:   14c01457            vfredmin.vs v8,v12,v0,v0.t
 8d8:   1d769e57            vfredmax.vs v28,v23,v13,v0.t
 8dc:   01d02a57            vredsum.vs  v20,v29,v0,v0.t
 8e0:   052a2cd7            vredand.vs  v25,v18,v20,v0.t
 8e4:   08aca0d7            vredor.vs   v1,v10,v25,v0.t
 8e8:   0c6aa157            vredxor.vs  v2,v6,v21,v0.t
 8ec:   102224d7            vredminu.vs v9,v2,v4,v0.t
 8f0:   14ea29d7            vredmin.vs  v19,v14,v20,v0.t
 8f4:   19732457            vredmaxu.vs v8,v23,v6,v0.t
 8f8:   1cc9af57            vredmax.vs  v30,v12,v19,v0.t
 8fc:   c07e8f57            vwredsumu.vs    v30,v7,v29,v0.t
 900:   c4790a57            vwredsum.vs v20,v7,v18,v0.t
 904:   7c2c51d7            vmfge.vf    v3,v2,fs8,v0.t
 908:   747e5fd7            vmfgt.vf    v31,v7,ft8,v0.t
 90c:   6579d457            vmfle.vf    v8,v23,fs3,v0.t
 910:   65a79c57            vmfle.vv    v24,v26,v15,v0.t
 914:   6c8559d7            vmflt.vf    v19,v8,fa0,v0.t
 918:   6d069bd7            vmflt.vv    v23,v16,v13,v0.t
 91c:   704f53d7            vmfne.vf    v7,v4,ft10,v0.t
 920:   701d9357            vmfne.vv    v6,v1,v27,v0.t
 924:   60cad9d7            vmfeq.vf    v19,v12,fs5,v0.t
 928:   616d1fd7            vmfeq.vv    v31,v22,v26,v0.t
 92c:   3cdbdc57            vfslide1down.vf v24,v13,fs7,v0.t
 930:   3861dbd7            vfslide1up.vf   v23,v6,ft3,v0.t
 934:   28d955d7            vfsgnjx.vf  v11,v13,fs2,v0.t
 938:   29a11a57            vfsgnjx.vv  v20,v26,v2,v0.t
 93c:   259d5ad7            vfsgnjn.vf  v21,v25,fs10,v0.t
 940:   25691657            vfsgnjn.vv  v12,v22,v18,v0.t
 944:   2162dd57            vfsgnj.vf   v26,v22,ft5,v0.t
 948:   20eb9f57            vfsgnj.vv   v30,v14,v23,v0.t
 94c:   18ae51d7            vfmax.vf    v3,v10,ft8,v0.t
 950:   18399fd7            vfmax.vv    v31,v3,v19,v0.t
 954:   11115257            vfmin.vf    v4,v17,ft2,v0.t
 958:   116d9ad7            vfmin.vv    v21,v22,v27,v0.t
 95c:   e1b7dc57            vfwmul.vf   v24,v27,fa5,v0.t
 960:   e11a9fd7            vfwmul.vv   v31,v17,v21,v0.t
 964:   816359d7            vfdiv.vf    v19,v22,ft6,v0.t
 968:   81511557            vfdiv.vv    v10,v21,v2,v0.t
 96c:   90c1d0d7            vfmul.vf    v1,v12,ft3,v0.t
 970:   90da1557            vfmul.vv    v10,v13,v20,v0.t
 974:   8477d857            vfrdiv.vf   v16,v7,fa5,v0.t
 978:   d9e252d7            vfwsub.wf   v5,v30,ft4,v0.t
 97c:   d8d89857            vfwsub.wv   v16,v13,v17,v0.t
 980:   c8465c57            vfwsub.vf   v24,v4,fa2,v0.t
 984:   c8449057            vfwsub.vv   v0,v4,v9,v0.t
 988:   d12150d7            vfwadd.wf   v1,v18,ft2,v0.t
 98c:   d03698d7            vfwadd.wv   v17,v3,v13,v0.t
 990:   c0165f57            vfwadd.vf   v30,v1,fa2,v0.t
 994:   c13d99d7            vfwadd.vv   v19,v19,v27,v0.t
 998:   09f2d5d7            vfsub.vf    v11,v31,ft5,v0.t
 99c:   08d31857            vfsub.vv    v16,v13,v6,v0.t
 9a0:   015351d7            vfadd.vf    v3,v21,ft6,v0.t
 9a4:   01c81557            vfadd.vv    v10,v28,v16,v0.t
 9a8:   9c5ed957            vfrsub.vf   v18,v5,ft9,v0.t
 9ac:   bc05c857            vnclip.wx   v16,v0,a1,v0.t
 9b0:   bd9681d7            vnclip.wv   v3,v25,v13,v0.t
 9b4:   b9af4ad7            vnclipu.wx  v21,v26,t5,v0.t
 9b8:   b94688d7            vnclipu.wv  v17,v20,v13,v0.t
 9bc:   ad79c2d7            vssra.vx    v5,v23,s3,v0.t
 9c0:   ad8f0657            vssra.vv    v12,v24,v30,v0.t
 9c4:   a80a4157            vssrl.vx    v2,v0,s4,v0.t
 9c8:   a8c10ed7            vssrl.vv    v29,v12,v2,v0.t
 9cc:   9ca84357            vsmul.vx    v6,v10,a6,v0.t
 9d0:   9d7b0e57            vsmul.vv    v28,v23,v22,v0.t
 9d4:   28fbead7            vasubu.vx   v21,v15,s7,v0.t
 9d8:   283d20d7            vasubu.vv   v1,v3,v26,v0.t
 9dc:   2cfbe857            vasub.vx    v16,v15,s7,v0.t
 9e0:   2cb9a2d7            vasub.vv    v5,v11,v19,v0.t
 9e4:   202e6057            vaaddu.vx   v0,v2,t3,v0.t
 9e8:   2176a0d7            vaaddu.vv   v1,v23,v13,v0.t
 9ec:   244ae057            vaadd.vx    v0,v4,s5,v0.t
 9f0:   249520d7            vaadd.vv    v1,v9,v10,v0.t
 9f4:   8da54457            vssub.vx    v8,v26,a0,v0.t
 9f8:   8d168657            vssub.vv    v12,v17,v13,v0.t
 9fc:   89e543d7            vssubu.vx   v7,v30,a0,v0.t
 a00:   895e8bd7            vssubu.vv   v23,v21,v29,v0.t
 a04:   842cc557            vsadd.vx    v10,v2,s9,v0.t
 a08:   84168457            vsadd.vv    v8,v1,v13,v0.t
 a0c:   807c4d57            vsaddu.vx   v26,v7,s8,v0.t
 a10:   81500b57            vsaddu.vv   v22,v21,v0,v0.t
 a14:   ed396557            vwmul.vx    v10,v19,s2,v0.t
 a18:   eced2357            vwmul.vv    v6,v14,v26,v0.t
 a1c:   e972ee57            vwmulsu.vx  v28,v23,t0,v0.t
 a20:   e93f2857            vwmulsu.vv  v16,v19,v30,v0.t
 a24:   e082e3d7            vwmulu.vx   v7,v8,t0,v0.t
 a28:   e1622dd7            vwmulu.vv   v27,v22,v4,v0.t
 a2c:   8c16ecd7            vrem.vx v25,v1,a3,v0.t
 a30:   8c16ac57            vrem.vv v24,v1,v13,v0.t
 a34:   88f2e4d7            vremu.vx    v9,v15,t0,v0.t
 a38:   890620d7            vremu.vv    v1,v16,v12,v0.t
 a3c:   84196157            vdiv.vx v2,v1,s2,v0.t
 a40:   85472057            vdiv.vv v0,v20,v14,v0.t
 a44:   8045e257            vdivu.vx    v4,v4,a1,v0.t
 a48:   81c62157            vdivu.vv    v2,v28,v12,v0.t
 a4c:   99256c57            vmulhsu.vx  v24,v18,a0,v0.t
 a50:   98d6a2d7            vmulhsu.vv  v5,v13,v13,v0.t
 a54:   901fe557            vmulhu.vx   v10,v1,t6,v0.t
 a58:   90d72f57            vmulhu.vv   v30,v13,v14,v0.t
 a5c:   9d936d57            vmulh.vx    v26,v25,t1,v0.t
 a60:   9c4c2ad7            vmulh.vv    v21,v4,v24,v0.t
 a64:   95786c57            vmul.vx v24,v23,a6,v0.t
 a68:   94e82957            vmul.vv v18,v14,v16,v0.t
 a6c:   1d4ccbd7            vmax.vx v23,v20,s9,v0.t
 a70:   1c800257            vmax.vv v4,v8,v0,v0.t
 a74:   18f2ce57            vmaxu.vx    v28,v15,t0,v0.t
 a78:   197307d7            vmaxu.vv    v15,v23,v6,v0.t
 a7c:   14a8ca57            vmin.vx v20,v10,a7,v0.t
 a80:   14f788d7            vmin.vv v17,v15,v15,v0.t
 a84:   10a3c2d7            vminu.vx    v5,v10,t2,v0.t
 a88:   11048a57            vminu.vv    v20,v16,v9,v0.t
 a8c:   7dfa48d7            vmsgt.vx    v17,v31,s4,v0.t
 a90:   7956c8d7            vmsgtu.vx   v17,v21,a3,v0.t
 a94:   75b646d7            vmsle.vx    v13,v27,a2,v0.t
 a98:   75ff8ad7            vmsle.vv    v21,v31,v31,v0.t
 a9c:   71074dd7            vmsleu.vx   v27,v16,a4,v0.t
 aa0:   70e18ed7            vmsleu.vv   v29,v14,v3,v0.t
 aa4:   6c0447d7            vmslt.vx    v15,v0,s0,v0.t
 aa8:   6d9c81d7            vmslt.vv    v3,v25,v25,v0.t
 aac:   692e43d7            vmsltu.vx   v7,v18,t3,v0.t
 ab0:   698a87d7            vmsltu.vv   v15,v24,v21,v0.t
 ab4:   641bcfd7            vmsne.vx    v31,v1,s7,v0.t
 ab8:   655409d7            vmsne.vv    v19,v21,v8,v0.t
 abc:   6024c1d7            vmseq.vx    v3,v2,s1,v0.t
 ac0:   61e50e57            vmseq.vv    v28,v30,v10,v0.t
 ac4:   b52f4d57            vnsra.wx    v26,v18,t5,v0.t
 ac8:   b48f8f57            vnsra.wv    v30,v8,v31,v0.t
 acc:   b124c157            vnsrl.wx    v2,v18,s1,v0.t
 ad0:   b0e18d57            vnsrl.wv    v26,v14,v3,v0.t
 ad4:   a5e64657            vsra.vx v12,v30,a2,v0.t
 ad8:   a50f0d57            vsra.vv v26,v16,v30,v0.t
 adc:   a0e94257            vsrl.vx v4,v14,s2,v0.t
 ae0:   a1320cd7            vsrl.vv v25,v19,v4,v0.t
 ae4:   950441d7            vsll.vx v3,v16,s0,v0.t
 ae8:   94980c57            vsll.vv v24,v9,v16,v0.t
 aec:   2c5acf57            vxor.vx v30,v5,s5,v0.t
 af0:   2c088557            vxor.vv v10,v0,v17,v0.t
 af4:   29f5c157            vor.vx  v2,v31,a1,v0.t
 af8:   28c805d7            vor.vv  v11,v12,v16,v0.t
 afc:   25e7c257            vand.vx v4,v30,a5,v0.t
 b00:   25138cd7            vand.vv v25,v17,v7,v0.t
 b04:   dd09e1d7            vwsub.wx    v3,v16,s3,v0.t
 b08:   dd892457            vwsub.wv    v8,v24,v18,v0.t
 b0c:   d8396357            vwsubu.wx   v6,v3,s2,v0.t
 b10:   d9dc27d7            vwsubu.wv   v15,v29,v24,v0.t
 b14:   d5f76ad7            vwadd.wx    v21,v31,a4,v0.t
 b18:   d4642c57            vwadd.wv    v24,v6,v8,v0.t
 b1c:   d04de4d7            vwaddu.wx   v9,v4,s11,v0.t
 b20:   d004a5d7            vwaddu.wv   v11,v0,v9,v0.t
 b24:   cd4f6557            vwsub.vx    v10,v20,t5,v0.t
 b28:   cc26ab57            vwsub.vv    v22,v2,v13,v0.t
 b2c:   c87b69d7            vwsubu.vx   v19,v7,s6,v0.t
 b30:   c9e224d7            vwsubu.vv   v9,v30,v4,v0.t
 b34:   c4f966d7            vwadd.vx    v13,v15,s2,v0.t
 b38:   c4592cd7            vwadd.vv    v25,v5,v18,v0.t
 b3c:   c0c7e4d7            vwaddu.vx   v9,v12,a5,v0.t
 b40:   c070afd7            vwaddu.vv   v31,v7,v1,v0.t
 b44:   084543d7            vsub.vx v7,v4,a0,v0.t
 b48:   08c50c57            vsub.vv v24,v12,v10,v0.t
 b4c:   01bfc957            vadd.vx v18,v27,t6,v0.t
 b50:   01818557            vadd.vv v10,v24,v3,v0.t
 b54:   5e88a6d7            vcompress.vm    v13,v8,v17
 b58:   7e4e2d57            vmxnor.mm   v26,v4,v28
 b5c:   72d0ac57            vmornot.mm  v24,v13,v1
 b60:   7b7bad57            vmnor.mm    v26,v23,v23
 b64:   6bea2e57            vmor.mm v28,v30,v20
 b68:   6ee1aad7            vmxor.mm    v21,v14,v3
 b6c:   62a72057            vmandnot.mm v0,v10,v14
 b70:   777526d7            vmnand.mm   v13,v23,v10
 b74:   66372757            vmand.mm    v14,v3,v14
 b78:   32ffc857            vrgather.vx v16,v15,t6
 b7c:   323a82d7            vrgather.vv v5,v3,v21
 b80:   3f956fd7            vslide1down.vx  v31,v25,a0
 b84:   3e2ac5d7            vslidedown.vx   v11,v2,s5
 b88:   3b5b6f57            vslide1up.vx    v30,v21,s6
 b8c:   3a56c0d7            vslideup.vx v1,v5,a3
 b90:   c6ac17d7            vfwredsum.vs    v15,v10,v24
 b94:   cf9f9557            vfwredosum.vs   v10,v25,v31
 b98:   07cc90d7            vfredsum.vs v1,v28,v25
 b9c:   0ef61d57            vfredosum.vs    v26,v15,v12
 ba0:   16619c57            vfredmin.vs v24,v6,v3
 ba4:   1e0b14d7            vfredmax.vs v9,v0,v22
 ba8:   036a2f57            vredsum.vs  v30,v22,v20
 bac:   07872dd7            vredand.vs  v27,v24,v14
 bb0:   0bbc2fd7            vredor.vs   v31,v27,v24
 bb4:   0efb2f57            vredxor.vs  v30,v15,v22
 bb8:   1254ad57            vredminu.vs v26,v5,v9
 bbc:   16d72857            vredmin.vs  v16,v13,v14
 bc0:   1be624d7            vredmaxu.vs v9,v30,v12
 bc4:   1e19aa57            vredmax.vs  v20,v1,v19
 bc8:   c24900d7            vwredsumu.vs    v1,v4,v18
 bcc:   c71287d7            vwredsum.vs v15,v17,v5
 bd0:   7e9adcd7            vmfge.vf    v25,v9,fs5
 bd4:   77dfdf57            vmfgt.vf    v30,v29,ft11
 bd8:   66e75557            vmfle.vf    v10,v14,fa4
 bdc:   66cf1cd7            vmfle.vv    v25,v12,v30
 be0:   6eebded7            vmflt.vf    v29,v14,fs7
 be4:   6e959957            vmflt.vv    v18,v9,v11
 be8:   720ad557            vmfne.vf    v10,v0,fs5
 bec:   72d99557            vmfne.vv    v10,v13,v19
 bf0:   63865957            vmfeq.vf    v18,v24,fa2
 bf4:   63ce9957            vmfeq.vv    v18,v28,v29
 bf8:   3fed5dd7            vfslide1down.vf v27,v30,fs10
 bfc:   3bb85557            vfslide1up.vf   v10,v27,fa6
 c00:   2b02d1d7            vfsgnjx.vf  v3,v16,ft5
 c04:   2a5596d7            vfsgnjx.vv  v13,v5,v11
 c08:   269cdad7            vfsgnjn.vf  v21,v9,fs9
 c0c:   26791f57            vfsgnjn.vv  v30,v7,v18
 c10:   221c53d7            vfsgnj.vf   v7,v1,fs8
 c14:   238a1e57            vfsgnj.vv   v28,v24,v20
 c18:   1babd657            vfmax.vf    v12,v26,fs7
 c1c:   1b9b9357            vfmax.vv    v6,v25,v23
 c20:   1375d057            vfmin.vf    v0,v23,fa1
 c24:   12779457            vfmin.vv    v8,v7,v15
 c28:   e377dc57            vfwmul.vf   v24,v23,fa5
 c2c:   e2db15d7            vfwmul.vv   v11,v13,v22
 c30:   821051d7            vfdiv.vf    v3,v1,ft0
 c34:   83a71bd7            vfdiv.vv    v23,v26,v14
 c38:   92775557            vfmul.vf    v10,v7,fa4
 c3c:   92249057            vfmul.vv    v0,v2,v9
 c40:   87f556d7            vfrdiv.vf   v13,v31,fa0
 c44:   da52dbd7            vfwsub.wf   v23,v5,ft5
 c48:   daaa9157            vfwsub.wv   v2,v10,v21
 c4c:   ca1ade57            vfwsub.vf   v28,v1,fs5
 c50:   cb721c57            vfwsub.vv   v24,v23,v4
 c54:   d244d5d7            vfwadd.wf   v11,v4,fs1
 c58:   d37211d7            vfwadd.wv   v3,v23,v4
 c5c:   c2e7dd57            vfwadd.vf   v26,v14,fa5
 c60:   c3f71c57            vfwadd.vv   v24,v31,v14
 c64:   0b1c5d57            vfsub.vf    v26,v17,fs8
 c68:   0a1a9e57            vfsub.vv    v28,v1,v21
 c6c:   0278d6d7            vfadd.vf    v13,v7,fa7
 c70:   03d413d7            vfadd.vv    v7,v29,v8
 c74:   9f765257            vfrsub.vf   v4,v23,fa2
 c78:   bfd943d7            vnclip.wx   v7,v29,s2
 c7c:   be138e57            vnclip.wv   v28,v1,v7
 c80:   bb064cd7            vnclipu.wx  v25,v16,a2
 c84:   baa08257            vnclipu.wv  v4,v10,v1
 c88:   af764dd7            vssra.vx    v27,v23,a2
 c8c:   afdb8157            vssra.vv    v2,v29,v23
 c90:   aa0fc157            vssrl.vx    v2,v0,t6
 c94:   aa460057            vssrl.vv    v0,v4,v12
 c98:   9fcc4957            vsmul.vx    v18,v28,s8
 c9c:   9ffd05d7            vsmul.vv    v11,v31,v26
 ca0:   2b7a6157            vasubu.vx   v2,v23,s4
 ca4:   2a33aed7            vasubu.vv   v29,v3,v7
 ca8:   2e886957            vasub.vx    v18,v8,a6
 cac:   2f0e2f57            vasub.vv    v30,v16,v28
 cb0:   230965d7            vaaddu.vx   v11,v16,s2
 cb4:   22e8a2d7            vaaddu.vv   v5,v14,v17
 cb8:   269862d7            vaadd.vx    v5,v9,a6
 cbc:   27942457            vaadd.vv    v8,v25,v8
 cc0:   8e654557            vssub.vx    v10,v6,a0
 cc4:   8fbd05d7            vssub.vv    v11,v27,v26
 cc8:   8a9ec757            vssubu.vx   v14,v9,t4
 ccc:   8b288fd7            vssubu.vv   v31,v18,v17
 cd0:   878ace57            vsadd.vx    v28,v24,s5
 cd4:   86cf81d7            vsadd.vv    v3,v12,v31
 cd8:   833544d7            vsaddu.vx   v9,v19,a0
 cdc:   834d8257            vsaddu.vv   v4,v20,v27
 ce0:   ef9be357            vwmul.vx    v6,v25,s7
 ce4:   eea822d7            vwmul.vv    v5,v10,v16
 ce8:   eb63e1d7            vwmulsu.vx  v3,v22,t2
 cec:   eaa1a6d7            vwmulsu.vv  v13,v10,v3
 cf0:   e39de757            vwmulu.vx   v14,v25,s11
 cf4:   e33425d7            vwmulu.vv   v11,v19,v8
 cf8:   8e076f57            vrem.vx v30,v0,a4
 cfc:   8ff52ed7            vrem.vv v29,v31,v10
 d00:   8a6461d7            vremu.vx    v3,v6,s0
 d04:   8bb5aed7            vremu.vv    v29,v27,v11
 d08:   87df6fd7            vdiv.vx v31,v29,t5
 d0c:   87602957            vdiv.vv v18,v22,v0
 d10:   82946057            vdivu.vx    v0,v9,s0
 d14:   821624d7            vdivu.vv    v9,v1,v12
 d18:   9beee0d7            vmulhsu.vx  v1,v30,t4
 d1c:   9beeaf57            vmulhsu.vv  v30,v30,v29
 d20:   92c5e457            vmulhu.vx   v8,v12,a1
 d24:   92c72757            vmulhu.vv   v14,v12,v14
 d28:   9e876c57            vmulh.vx    v24,v8,a4
 d2c:   9e602357            vmulh.vv    v6,v6,v0
 d30:   967f6d57            vmul.vx v26,v7,t5
 d34:   96502bd7            vmul.vv v23,v5,v0
 d38:   1eca4bd7            vmax.vx v23,v12,s4
 d3c:   1f208e57            vmax.vv v28,v18,v1
 d40:   1b5c41d7            vmaxu.vx    v3,v21,s8
 d44:   1a740357            vmaxu.vv    v6,v7,v8
 d48:   16174457            vmin.vx v8,v1,a4
 d4c:   17248dd7            vmin.vv v27,v18,v9
 d50:   1395c9d7            vminu.vx    v19,v25,a1
 d54:   12cd82d7            vminu.vv    v5,v12,v27
 d58:   7f7d4757            vmsgt.vx    v14,v23,s10
 d5c:   7b664cd7            vmsgtu.vx   v25,v22,a2
 d60:   76fd46d7            vmsle.vx    v13,v15,s10
 d64:   77b78cd7            vmsle.vv    v25,v27,v15
 d68:   7275c0d7            vmsleu.vx   v1,v7,a1
 d6c:   73a207d7            vmsleu.vv   v15,v26,v4
 d70:   6f8cc257            vmslt.vx    v4,v24,s9
 d74:   6e638a57            vmslt.vv    v20,v6,v7
 d78:   6a39c8d7            vmsltu.vx   v17,v3,s3
 d7c:   6a708cd7            vmsltu.vv   v25,v7,v1
 d80:   66f64cd7            vmsne.vx    v25,v15,a2
 d84:   667a08d7            vmsne.vv    v17,v7,v20
 d88:   621dc0d7            vmseq.vx    v1,v1,s11
 d8c:   622c09d7            vmseq.vv    v19,v2,v24
 d90:   b7f540d7            vnsra.wx    v1,v31,a0
 d94:   b6198ed7            vnsra.wv    v29,v1,v19
 d98:   b3e64357            vnsrl.wx    v6,v30,a2
 d9c:   b2dc0b57            vnsrl.wv    v22,v13,v24
 da0:   a7534057            vsra.vx v0,v21,t1
 da4:   a6d10657            vsra.vv v12,v13,v2
 da8:   a20340d7            vsrl.vx v1,v0,t1
 dac:   a32486d7            vsrl.vv v13,v18,v9
 db0:   97d8c257            vsll.vx v4,v29,a7
 db4:   966d8bd7            vsll.vv v23,v6,v27
 db8:   2e374c57            vxor.vx v24,v3,a4
 dbc:   2fa60e57            vxor.vv v28,v26,v12
 dc0:   2a99c8d7            vor.vx  v17,v9,s3
 dc4:   2baf05d7            vor.vv  v11,v26,v30
 dc8:   2653c1d7            vand.vx v3,v5,t2
 dcc:   262283d7            vand.vv v7,v2,v5
 dd0:   df39efd7            vwsub.wx    v31,v19,s3
 dd4:   dffc2e57            vwsub.wv    v28,v31,v24
 dd8:   db22ebd7            vwsubu.wx   v23,v18,t0
 ddc:   daf62ad7            vwsubu.wv   v21,v15,v12
 de0:   d602ecd7            vwadd.wx    v25,v0,t0
 de4:   d601a457            vwadd.wv    v8,v0,v3
 de8:   d3d9eb57            vwaddu.wx   v22,v29,s3
 dec:   d338aa57            vwaddu.wv   v20,v19,v17
 df0:   cfbf6157            vwsub.vx    v2,v27,t5
 df4:   ce442bd7            vwsub.vv    v23,v4,v8
 df8:   cb29e2d7            vwsubu.vx   v5,v18,s3
 dfc:   ca2824d7            vwsubu.vv   v9,v2,v16
 e00:   c68767d7            vwadd.vx    v15,v8,a4
 e04:   c73326d7            vwadd.vv    v13,v19,v6
 e08:   c20de857            vwaddu.vx   v16,v0,s11
 e0c:   c37e2457            vwaddu.vv   v8,v23,v28
 e10:   0a19cd57            vsub.vx v26,v1,s3
 e14:   0b528d57            vsub.vv v26,v21,v5
 e18:   032ecb57            vadd.vx v22,v18,t4
 e1c:   03190c57            vadd.vv v24,v17,v18
 e20:   5ef4add7            vcompress.vm    v27,v15,v9
 e24:   7e87a257            vmxnor.mm   v4,v8,v15
 e28:   72f7a057            vmornot.mm  v0,v15,v15
 e2c:   7b352457            vmnor.mm    v8,v19,v10
 e30:   6bb1a857            vmor.mm v16,v27,v3
 e34:   6f792957            vmxor.mm    v18,v23,v18
 e38:   62562157            vmandnot.mm v2,v5,v12
 e3c:   76e02857            vmnand.mm   v16,v14,v0
 e40:   66222557            vmand.mm    v10,v2,v4
 e44:   44293cd7            vmadc.vim   v25,v2,-14,v0
 e48:   40d1b257            vadc.vim    v4,v13,3,v0
 e4c:   5cbc3457            vmerge.vim  v8,v11,-8,v0
 e50:   84cdb6d7            vsadd.vi    v13,v12,-5,v0.t
 e54:   80bbbf57            vsaddu.vi   v30,v11,-9,v0.t
 e58:   7d283d57            vmsgt.vi    v26,v18,-16,v0.t
 e5c:   79d737d7            vmsgtu.vi   v15,v29,14,v0.t
 e60:   74c43cd7            vmsle.vi    v25,v12,8,v0.t
 e64:   70b63ad7            vmsleu.vi   v21,v11,12,v0.t
 e68:   64f2b457            vmsne.vi    v8,v15,5,v0.t
 e6c:   61bebad7            vmseq.vi    v21,v27,-3,v0.t
 e70:   2d6f3fd7            vxor.vi v31,v22,-2,v0.t
 e74:   293831d7            vor.vi  v3,v19,-16,v0.t
 e78:   24dfbe57            vand.vi v28,v13,-1,v0.t
 e7c:   0133b557            vadd.vi v10,v19,7,v0.t
 e80:   86ba3ad7            vsadd.vi    v21,v11,-12
 e84:   82bfb357            vsaddu.vi   v6,v11,-1
 e88:   7f6b3f57            vmsgt.vi    v30,v22,-10
 e8c:   7a8638d7            vmsgtu.vi   v17,v8,12
 e90:   76773657            vmsle.vi    v12,v7,14
 e94:   72dab757            vmsleu.vi   v14,v13,-11
 e98:   674ab357            vmsne.vi    v6,v20,-11
 e9c:   62863d57            vmseq.vi    v26,v8,12
 ea0:   2f84b7d7            vxor.vi v15,v24,9
 ea4:   2a2cb4d7            vor.vi  v9,v2,-7
 ea8:   27f73b57            vand.vi v22,v31,14
 eac:   0398b9d7            vadd.vi v19,v25,-15
 eb0:   0c57bd57            vrsub.vi    v26,v5,15,v0.t
 eb4:   0e873357            vrsub.vi    v6,v8,14
 eb8:   31bfb257            vrgather.vi v4,v27,31,v0.t
 ebc:   3c9aba57            vslidedown.vi   v20,v9,21,v0.t
 ec0:   3999bad7            vslideup.vi v21,v25,19,v0.t
 ec4:   bcd032d7            vnclip.wi   v5,v13,0,v0.t
 ec8:   b81e3157            vnclipu.wi  v2,v1,28,v0.t
 ecc:   accf39d7            vssra.vi    v19,v12,30,v0.t
 ed0:   a800b157            vssrl.vi    v2,v0,1,v0.t
 ed4:   b47235d7            vnsra.wi    v11,v7,4,v0.t
 ed8:   b042b4d7            vnsrl.wi    v9,v4,5,v0.t
 edc:   a55635d7            vsra.vi v11,v21,12,v0.t
 ee0:   a0aab9d7            vsrl.vi v19,v10,21,v0.t
 ee4:   945b38d7            vsll.vi v17,v5,22,v0.t
 ee8:   3310b6d7            vrgather.vi v13,v17,1
 eec:   3e993557            vslidedown.vi   v10,v9,18
 ef0:   3a0034d7            vslideup.vi v9,v0,0
 ef4:   bf7a3ed7            vnclip.wi   v29,v23,20
 ef8:   bbd2b2d7            vnclipu.wi  v5,v29,5
 efc:   aea7bed7            vssra.vi    v29,v10,15
 f00:   abdc39d7            vssrl.vi    v19,v29,24
 f04:   b7d63f57            vnsra.wi    v30,v29,12
 f08:   b3abbbd7            vnsrl.wi    v23,v26,23
 f0c:   a6e4b957            vsra.vi v18,v14,9
 f10:   a2a5bcd7            vsrl.vi v25,v10,11
 f14:   961fb757            vsll.vi v14,v1,31
 f18:   80797357            vsetvl  t1,s2,t2
 */

  static const unsigned int insns[] =
  {
    0x00fe02b3,     0x40ef0bb3,     0x011d893b,     0x40ff853b,
    0x00ab65b3,     0x01afc733,     0x03cc0933,     0x02e413b3,
    0x036aa733,     0x0257ba33,     0x0292c333,     0x03d8d533,
    0x0372e7b3,     0x0357ffb3,     0x03be8a3b,     0x0355ca3b,
    0x02fbd3bb,     0x03dce8bb,     0x02577a3b,     0x01967a33,
    0x5d660f93,     0x834a831b,     0xbe9e6d93,     0xde0acb13,
    0x1efb7d13,     0xf66da993,     0x49638a67,     0x1fd2bc93,
    0x0000006f,     0xf8dff06f,     0x6a50006f,     0x000000ef,
    0xf81ff0ef,     0x699000ef,     0x0daf8837,     0x7068cb97,
    0x000d1063,     0xf60d16e3,     0x680d12e3,     0x000f8063,
    0xf60f80e3,     0x660f8ce3,     0x01ce9063,     0xf5ce9ae3,
    0x67ce96e3,     0x01ba0063,     0xf5ba04e3,     0x67ba00e3,
    0x009ad063,     0xf29adee3,     0x649adae3,     0x01e67063,
    0xf3e678e3,     0x65e674e3,     0x01c8c063,     0xf3c8c2e3,
    0x63c8cee3,     0x019ae063,     0xf19aece3,     0x639ae8e3,
    0x006c9fb3,     0x012dd3b3,     0x4156deb3,     0x416bda3b,
    0x016c9fbb,     0x0099dc3b,     0x00629d93,     0x0049db13,
    0x40f85293,     0x00a3129b,     0x003eda9b,     0x4098de1b,
    0x00000013,     0x00000073,     0x00100073,     0x0000100f,
    0x01f0000f,     0x1d3f29af,     0x0cc5a5af,     0x05beab2f,
    0x246aab2f,     0x64efab2f,     0x459ea3af,     0x84b3282f,
    0xa51bae2f,     0xc52fa72f,     0xe46facaf,     0x14072d2f,
    0x1bd6232f,     0x0af428af,     0x03cf2b2f,     0x226ca6af,
    0x63a9a82f,     0x4265adaf,     0x8387af2f,     0xa3a3a62f,
    0xc28e2e2f,     0xe3b8ad2f,     0x120c2faf,     0x1cadbb2f,
    0x0da3332f,     0x05eeb8af,     0x2487b8af,     0x645c3faf,
    0x45c635af,     0x85aa3d2f,     0xa524b3af,     0xc4f5392f,
    0xe4abbaaf,     0x14083aaf,     0x1b45b42f,     0x0af63a2f,
    0x026f39af,     0x238eb2af,     0x63d432af,     0x4277b42f,
    0x8384b82f,     0xa2dcbe2f,     0xc3ceb7af,     0xe34fbe2f,
    0x12073c2f,     0x001023f3,     0x00202973,     0x00302573,
    0xc01024f3,     0xc0002b73,     0xc02023f3,     0x000b8693,
    0xfffc4b13,     0x40e004b3,     0x40800f3b,     0x000d099b,
    0x001bbb13,     0x00703b33,     0x00072f33,     0x00c027b3,
    0x003d9673,     0x002c9973,     0x001b98f3,     0x00deacb3,
    0x00beb4b3,     0xef7a14f3,     0x5cbda2f3,     0xff483df3,
    0x72b85df3,     0xcd7ce473,     0x4cd979f3,     0xa1c02d73,
    0xe0341073,     0x8908a073,     0xdb9f3073,     0x9ff75073,
    0x4f4be073,     0x8206f073,     0x86ca3503,     0xb1de2b03,
    0xcdece803,     0xc9949883,     0xad755c83,     0xb6c60d03,
    0xf5fdcc03,     0xb3493423,     0x10a92423,     0xf6669ba3,
    0x07368223,     0x2af9bf87,     0xd9bdab87,     0x077f3627,
    0xef89a327,     0x580ba4d3,     0x5a022153,     0x00f2b253,
    0x08073cd3,     0x025c38d3,     0x0bc9b853,     0x11403b53,
    0x19283953,     0x13e0bfd3,     0x1bfe31d3,     0x1002bdc3,
    0xe8c69cc7,     0xdbba3e43,     0x92189747,     0x70f0c24b,
    0x41251bcf,     0x1bae434b,     0x827616cf,     0xe0019bd3,
    0x21080153,     0xe20c99d3,     0x22318b53,     0x20c62cd3,
    0x20a517d3,     0x23ce2b53,     0x233993d3,     0xe0018bd3,
    0xe20982d3,     0x200904d3,     0x20cb9cd3,     0x220801d3,
    0x22909b53,     0x20562953,     0x29cf0953,     0x2209aed3,
    0x2bc904d3,     0x29519253,     0xa170a553,     0x2bab18d3,
    0xa3b0ad53,     0xa15e1f53,     0xa1b30cd3,     0xa2e01ad3,
    0xa3308e53,     0xc007b5d3,     0xc01686d3,     0xd00da8d3,
    0xd01d95d3,     0xc02d0b53,     0xc032ca53,     0xd026bad3,
    0xd03c9e53,     0x4013a653,     0x420587d3,     0xc20a2dd3,
    0xc2172b53,     0xd20c8553,     0xd21a04d3,     0xc22cad53,
    0xc23da553,     0xd22ea053,     0xd23b2753,     0x0d6b8caf,
    0x045c8aaf,     0x249f092f,     0x64ae03af,     0x44b30c2f,
    0x85e983af,     0xa4598e2f,     0xc44685af,     0xe555062f,
    0x0ecb0e2f,     0x069b8eaf,     0x263e02af,     0x66c385af,
    0x460f082f,     0x876f0a2f,     0xa68a832f,     0xc61c802f,
    0xe7e30b2f,     0x096285af,     0x013801af,     0x206f0c2f,
    0x60b307af,     0x40e487af,     0x81bb822f,     0xa13a8c2f,
    0xc1c6062f,     0xe16904af,     0x0bcd082f,     0x03f709af,
    0x223885af,     0x63a408af,     0x42e38aaf,     0x83ae02af,
    0xa39d022f,     0xc2cc86af,     0xe31e80af,     0x0d4e51af,
    0x04d659af,     0x24b2d72f,     0x641352af,     0x4544d42f,
    0x84e2dbaf,     0xa4add82f,     0xc4c9da2f,     0xe447d52f,
    0x0fad5aaf,     0x06addd2f,     0x260ad5af,     0x67ead6af,
    0x468653af,     0x8643df2f,     0xa7a45c2f,     0xc75956af,
    0xe644d42f,     0x0929dbaf,     0x010cd9af,     0x2156dbaf,
    0x6139d5af,     0x40cb52af,     0x807c522f,     0xa1ea5f2f,
    0xc0b4d1af,     0xe1f3ddaf,     0x0aa3d1af,     0x021b562f,
    0x22a3532f,     0x63cddb2f,     0x438fd22f,     0x82dd55af,
    0xa20edaaf,     0xc22b5c2f,     0xe3cbd8af,     0x0de2eaaf,
    0x05936baf,     0x240d652f,     0x65c7e32f,     0x4449ea2f,
    0x844b6daf,     0xa4bcedaf,     0xc416ed2f,     0xe4de652f,
    0x0edf672f,     0x0755662f,     0x2678e8af,     0x67bbe9af,
    0x46a2e42f,     0x860c60af,     0xa74b6c2f,     0xc60fe9af,
    0xe607eaaf,     0x085be22f,     0x016c6e2f,     0x2177ed2f,
    0x60efeeaf,     0x406b6caf,     0x810ae92f,     0xa0b4e5af,
    0xc00760af,     0xe1676e2f,     0x0b5ce3af,     0x0353602f,
    0x2347e6af,     0x63a4ee2f,     0x4238e1af,     0x83eae6af,
    0xa25ce82f,     0xc396692f,     0xe26c61af,     0x48832ed7,
    0x490220d7,     0x49a12657,     0x4893a4d7,     0x49b2a0d7,
    0x4891ae57,     0x40582c57,     0x40b8a457,     0x5110af57,
    0x5081a6d7,     0x519125d7,     0x510821d7,     0x489010d7,
    0x495094d7,     0x48111fd7,     0x48d196d7,     0x49d31557,
    0x488390d7,     0x49c41657,     0x492496d7,     0x494519d7,
    0x48a59ad7,     0x48761dd7,     0x48371dd7,     0x481799d7,
    0x48181a57,     0x48a898d7,     0x49d91c57,     0x49c99dd7,
    0x49da1fd7,     0x49fa94d7,     0x48bb19d7,     0x49bb9fd7,
    0x4df01f57,     0x4cd810d7,     0x4a232757,     0x4a8227d7,
    0x4b012457,     0x4b93aed7,     0x4b72a6d7,     0x4bd1af57,
    0x42582ed7,     0x42e8abd7,     0x5320a857,     0x5241a0d7,
    0x533123d7,     0x53c824d7,     0x4a9016d7,     0x4a209ad7,
    0x4b311d57,     0x4ba19bd7,     0x4a9314d7,     0x4bd390d7,
    0x4a3411d7,     0x4a249f57,     0x4b051857,     0x4b4591d7,
    0x4a361c57,     0x4a671a57,     0x4b6798d7,     0x4a5815d7,
    0x4ad89c57,     0x4bf91257,     0x4b3994d7,     0x4bea19d7,
    0x4aba9157,     0x4b5b1957,     0x4bfb9357,     0x4e401dd7,
    0x4ee81957,     0x9e503ed7,     0x9ee0bfd7,     0x9ee1b6d7,
    0x9e43bcd7,     0x43a01f57,     0x435025d7,     0x420650d7,
    0x42036dd7,     0x5e07dc57,     0x5e064ad7,     0x5e070157,
    0x5e03bb57,     0x5208add7,     0x028c0e07,     0x028402a7,
    0x000c8007,     0x000e5107,     0x0005e687,     0x00087587,
    0x00088ea7,     0x00085ca7,     0x000b6da7,     0x00057927,
    0x010b8607,     0x010f5d87,     0x0105e387,     0x0105fb87,
    0x02040b07,     0x0209d707,     0x02076e87,     0x020cf787,
    0x020e0c27,     0x02035927,     0x020769a7,     0x020f7ba7,
    0x03048407,     0x030ad587,     0x030e6307,     0x030c7707,
    0x09a78407,     0x08ea5f87,     0x08ce6507,     0x09dafe07,
    0x08d283a7,     0x0966dfa7,     0x093667a7,     0x09877ea7,
    0x0b688c07,     0x0b49d707,     0x0b48ed07,     0x0b25fb07,
    0x0bbd01a7,     0x0a555da7,     0x0b66ea27,     0x0aaff027,
    0x5da25fd7,     0x5d1ac4d7,     0x5c500dd7,     0x482bcdd7,
    0x49a688d7,     0x406cca57,     0x41788057,     0x451b4457,
    0x459406d7,     0x4cc4c557,     0x4c6b8f57,     0xfcb6d4d7,
    0xfc9e9657,     0xf80ed757,     0xf8ec1757,     0xf4035857,
    0xf5fc9cd7,     0xf0715d57,     0xf1799357,     0xad9955d7,
    0xac889ed7,     0xa8145457,     0xa9981457,     0xa4a75757,
    0xa4a39e57,     0xa07855d7,     0xa0a119d7,     0xbd5bdf57,
    0xbdf31a57,     0xb939dc57,     0xb9a61757,     0xb034dbd7,
    0xb1f513d7,     0xb436ddd7,     0xb5821557,     0xfc5ee2d7,
    0xfc03af57,     0xf49c6dd7,     0xf4b124d7,     0xf04a6057,
    0xf0212b57,     0xf96f65d7,     0xac166b57,     0xacc0a9d7,
    0xa4a66c57,     0xa5702957,     0xbd356cd7,     0xbc6ea857,
    0xb5466557,     0xb43320d7,     0xffc2d0d7,     0xffb81c57,
    0xfa785257,     0xfadb1557,     0xf7d65757,     0xf7989357,
    0xf2ccd857,     0xf21d9457,     0xaecad4d7,     0xaed01257,
    0xaac1d9d7,     0xaa2a14d7,     0xa681d6d7,     0xa7e71857,
    0xa3fddd57,     0xa3ca9857,     0xbf9d5757,     0xbe159957,
    0xbb04dc57,     0xbb5e1157,     0xb3e7dd57,     0xb3d91fd7,
    0xb6aadad7,     0xb63e1c57,     0xfe5666d7,     0xff59a7d7,
    0xf7b3e0d7,     0xf7952857,     0xf3feead7,     0xf3492957,
    0xfbac6f57,     0xafa6e9d7,     0xaec62ad7,     0xa7bb6557,
    0xa7452b57,     0xbe3be1d7,     0xbe7e2157,     0xb688ecd7,
    0xb6982657,     0x0daf43d7,     0x0e5f49d7,     0x3044cbd7,
    0x317180d7,     0x3c44efd7,     0x3df9c857,     0x38a46d57,
    0x385e43d7,     0xc5ff1257,     0xcc8d1f57,     0x04791ad7,
    0x0c031757,     0x14c01457,     0x1d769e57,     0x01d02a57,
    0x052a2cd7,     0x08aca0d7,     0x0c6aa157,     0x102224d7,
    0x14ea29d7,     0x19732457,     0x1cc9af57,     0xc07e8f57,
    0xc4790a57,     0x7c2c51d7,     0x747e5fd7,     0x6579d457,
    0x65a79c57,     0x6c8559d7,     0x6d069bd7,     0x704f53d7,
    0x701d9357,     0x60cad9d7,     0x616d1fd7,     0x3cdbdc57,
    0x3861dbd7,     0x28d955d7,     0x29a11a57,     0x259d5ad7,
    0x25691657,     0x2162dd57,     0x20eb9f57,     0x18ae51d7,
    0x18399fd7,     0x11115257,     0x116d9ad7,     0xe1b7dc57,
    0xe11a9fd7,     0x816359d7,     0x81511557,     0x90c1d0d7,
    0x90da1557,     0x8477d857,     0xd9e252d7,     0xd8d89857,
    0xc8465c57,     0xc8449057,     0xd12150d7,     0xd03698d7,
    0xc0165f57,     0xc13d99d7,     0x09f2d5d7,     0x08d31857,
    0x015351d7,     0x01c81557,     0x9c5ed957,     0xbc05c857,
    0xbd9681d7,     0xb9af4ad7,     0xb94688d7,     0xad79c2d7,
    0xad8f0657,     0xa80a4157,     0xa8c10ed7,     0x9ca84357,
    0x9d7b0e57,     0x28fbead7,     0x283d20d7,     0x2cfbe857,
    0x2cb9a2d7,     0x202e6057,     0x2176a0d7,     0x244ae057,
    0x249520d7,     0x8da54457,     0x8d168657,     0x89e543d7,
    0x895e8bd7,     0x842cc557,     0x84168457,     0x807c4d57,
    0x81500b57,     0xed396557,     0xeced2357,     0xe972ee57,
    0xe93f2857,     0xe082e3d7,     0xe1622dd7,     0x8c16ecd7,
    0x8c16ac57,     0x88f2e4d7,     0x890620d7,     0x84196157,
    0x85472057,     0x8045e257,     0x81c62157,     0x99256c57,
    0x98d6a2d7,     0x901fe557,     0x90d72f57,     0x9d936d57,
    0x9c4c2ad7,     0x95786c57,     0x94e82957,     0x1d4ccbd7,
    0x1c800257,     0x18f2ce57,     0x197307d7,     0x14a8ca57,
    0x14f788d7,     0x10a3c2d7,     0x11048a57,     0x7dfa48d7,
    0x7956c8d7,     0x75b646d7,     0x75ff8ad7,     0x71074dd7,
    0x70e18ed7,     0x6c0447d7,     0x6d9c81d7,     0x692e43d7,
    0x698a87d7,     0x641bcfd7,     0x655409d7,     0x6024c1d7,
    0x61e50e57,     0xb52f4d57,     0xb48f8f57,     0xb124c157,
    0xb0e18d57,     0xa5e64657,     0xa50f0d57,     0xa0e94257,
    0xa1320cd7,     0x950441d7,     0x94980c57,     0x2c5acf57,
    0x2c088557,     0x29f5c157,     0x28c805d7,     0x25e7c257,
    0x25138cd7,     0xdd09e1d7,     0xdd892457,     0xd8396357,
    0xd9dc27d7,     0xd5f76ad7,     0xd4642c57,     0xd04de4d7,
    0xd004a5d7,     0xcd4f6557,     0xcc26ab57,     0xc87b69d7,
    0xc9e224d7,     0xc4f966d7,     0xc4592cd7,     0xc0c7e4d7,
    0xc070afd7,     0x084543d7,     0x08c50c57,     0x01bfc957,
    0x01818557,     0x5e88a6d7,     0x7e4e2d57,     0x72d0ac57,
    0x7b7bad57,     0x6bea2e57,     0x6ee1aad7,     0x62a72057,
    0x777526d7,     0x66372757,     0x32ffc857,     0x323a82d7,
    0x3f956fd7,     0x3e2ac5d7,     0x3b5b6f57,     0x3a56c0d7,
    0xc6ac17d7,     0xcf9f9557,     0x07cc90d7,     0x0ef61d57,
    0x16619c57,     0x1e0b14d7,     0x036a2f57,     0x07872dd7,
    0x0bbc2fd7,     0x0efb2f57,     0x1254ad57,     0x16d72857,
    0x1be624d7,     0x1e19aa57,     0xc24900d7,     0xc71287d7,
    0x7e9adcd7,     0x77dfdf57,     0x66e75557,     0x66cf1cd7,
    0x6eebded7,     0x6e959957,     0x720ad557,     0x72d99557,
    0x63865957,     0x63ce9957,     0x3fed5dd7,     0x3bb85557,
    0x2b02d1d7,     0x2a5596d7,     0x269cdad7,     0x26791f57,
    0x221c53d7,     0x238a1e57,     0x1babd657,     0x1b9b9357,
    0x1375d057,     0x12779457,     0xe377dc57,     0xe2db15d7,
    0x821051d7,     0x83a71bd7,     0x92775557,     0x92249057,
    0x87f556d7,     0xda52dbd7,     0xdaaa9157,     0xca1ade57,
    0xcb721c57,     0xd244d5d7,     0xd37211d7,     0xc2e7dd57,
    0xc3f71c57,     0x0b1c5d57,     0x0a1a9e57,     0x0278d6d7,
    0x03d413d7,     0x9f765257,     0xbfd943d7,     0xbe138e57,
    0xbb064cd7,     0xbaa08257,     0xaf764dd7,     0xafdb8157,
    0xaa0fc157,     0xaa460057,     0x9fcc4957,     0x9ffd05d7,
    0x2b7a6157,     0x2a33aed7,     0x2e886957,     0x2f0e2f57,
    0x230965d7,     0x22e8a2d7,     0x269862d7,     0x27942457,
    0x8e654557,     0x8fbd05d7,     0x8a9ec757,     0x8b288fd7,
    0x878ace57,     0x86cf81d7,     0x833544d7,     0x834d8257,
    0xef9be357,     0xeea822d7,     0xeb63e1d7,     0xeaa1a6d7,
    0xe39de757,     0xe33425d7,     0x8e076f57,     0x8ff52ed7,
    0x8a6461d7,     0x8bb5aed7,     0x87df6fd7,     0x87602957,
    0x82946057,     0x821624d7,     0x9beee0d7,     0x9beeaf57,
    0x92c5e457,     0x92c72757,     0x9e876c57,     0x9e602357,
    0x967f6d57,     0x96502bd7,     0x1eca4bd7,     0x1f208e57,
    0x1b5c41d7,     0x1a740357,     0x16174457,     0x17248dd7,
    0x1395c9d7,     0x12cd82d7,     0x7f7d4757,     0x7b664cd7,
    0x76fd46d7,     0x77b78cd7,     0x7275c0d7,     0x73a207d7,
    0x6f8cc257,     0x6e638a57,     0x6a39c8d7,     0x6a708cd7,
    0x66f64cd7,     0x667a08d7,     0x621dc0d7,     0x622c09d7,
    0xb7f540d7,     0xb6198ed7,     0xb3e64357,     0xb2dc0b57,
    0xa7534057,     0xa6d10657,     0xa20340d7,     0xa32486d7,
    0x97d8c257,     0x966d8bd7,     0x2e374c57,     0x2fa60e57,
    0x2a99c8d7,     0x2baf05d7,     0x2653c1d7,     0x262283d7,
    0xdf39efd7,     0xdffc2e57,     0xdb22ebd7,     0xdaf62ad7,
    0xd602ecd7,     0xd601a457,     0xd3d9eb57,     0xd338aa57,
    0xcfbf6157,     0xce442bd7,     0xcb29e2d7,     0xca2824d7,
    0xc68767d7,     0xc73326d7,     0xc20de857,     0xc37e2457,
    0x0a19cd57,     0x0b528d57,     0x032ecb57,     0x03190c57,
    0x5ef4add7,     0x7e87a257,     0x72f7a057,     0x7b352457,
    0x6bb1a857,     0x6f792957,     0x62562157,     0x76e02857,
    0x66222557,     0x44293cd7,     0x40d1b257,     0x5cbc3457,
    0x84cdb6d7,     0x80bbbf57,     0x7d283d57,     0x79d737d7,
    0x74c43cd7,     0x70b63ad7,     0x64f2b457,     0x61bebad7,
    0x2d6f3fd7,     0x293831d7,     0x24dfbe57,     0x0133b557,
    0x86ba3ad7,     0x82bfb357,     0x7f6b3f57,     0x7a8638d7,
    0x76773657,     0x72dab757,     0x674ab357,     0x62863d57,
    0x2f84b7d7,     0x2a2cb4d7,     0x27f73b57,     0x0398b9d7,
    0x0c57bd57,     0x0e873357,     0x31bfb257,     0x3c9aba57,
    0x3999bad7,     0xbcd032d7,     0xb81e3157,     0xaccf39d7,
    0xa800b157,     0xb47235d7,     0xb042b4d7,     0xa55635d7,
    0xa0aab9d7,     0x945b38d7,     0x3310b6d7,     0x3e993557,
    0x3a0034d7,     0xbf7a3ed7,     0xbbd2b2d7,     0xaea7bed7,
    0xabdc39d7,     0xb7d63f57,     0xb3abbbd7,     0xa6e4b957,
    0xa2a5bcd7,     0x961fb757,     0x80797357, 
  };
// END  Generated code -- do not edit

  asm_check((unsigned int *)entry, insns, sizeof insns / sizeof insns[0]);

#endif
}

int AbstractAssembler::code_fill_byte() {
  return 0;
}

void Assembler::add(Register Rd, Register Rn, int64_t increment, Register temp) {
  if (is_imm_in_range(increment, 12, 0)) {
    addi(Rd, Rn, increment);
  } else {
    assert_different_registers(Rn, temp);
    li(temp, increment);
    add(Rd, Rn, temp);
  }
}

void Assembler::addw(Register Rd, Register Rn, int64_t increment, Register temp) {
  if (is_imm_in_range(increment, 12, 0)) {
    addiw(Rd, Rn, increment);
  } else {
    assert_different_registers(Rn, temp);
    li(temp, increment);
    addw(Rd, Rn, temp);
  }
}

void Assembler::sub(Register Rd, Register Rn, int64_t decrement, Register temp) {
  if (is_imm_in_range(-decrement, 12, 0)) {
    addi(Rd, Rn, -decrement);
  } else {
    assert_different_registers(Rn, temp);
    li(temp, decrement);
    sub(Rd, Rn, temp);
  }
}

void Assembler::subw(Register Rd, Register Rn, int64_t decrement, Register temp) {
  if (is_imm_in_range(-decrement, 12, 0)) {
    addiw(Rd, Rn, -decrement);
  } else {
    assert_different_registers(Rn, temp);
    li(temp, decrement);
    subw(Rd, Rn, temp);
  }
}

void Assembler::li(Register Rd, int64_t imm) {
  // int64_t is in range 0x8000 0000 0000 0000 ~ 0x7fff ffff ffff ffff
  int shift = 12;
  int64_t upper = imm, lower = imm;
  // Split imm to a lower 12-bit sign-extended part and the remainder, because addi will sign-extend the lower imm.
  lower = ((int32_t)imm << 20) >> 20;
  upper -= lower;

  // Test whether imm is a 32-bit integer.
  if (!(((imm) & ~(int64_t)0x7fffffff) == 0 ||
        (((imm) & ~(int64_t)0x7fffffff) == ~(int64_t)0x7fffffff))) {
    while (((upper >> shift) & 1) == 0) { shift++; }
    upper >>= shift;
    li(Rd, upper);
    slli(Rd, Rd, shift);
    if (lower != 0) {
      addi(Rd, Rd, lower);
    }
  }
  else {
    // 32-bit integer
    Register hi_Rd = zr;
    if (upper != 0) {
      lui(Rd, (int32_t)upper);
      hi_Rd = Rd;
    }
    if (lower != 0 || hi_Rd == zr) {
      addiw(Rd, hi_Rd, lower);
    }
  }
}

void Assembler::li64(Register Rd, int64_t imm) {
   // Load upper 32 bits. Upper = imm[63:32], but if imm[31] = 1 or (imm[31:28] == 0x7ff && imm[19] == 1),
   // upper = imm[63:32] + 1.
   int64_t lower = imm & 0xffffffff;
   lower -= ((lower << 44) >> 44);
   int64_t tmp_imm = ((uint64_t)(imm & 0xffffffff00000000)) + (uint64_t)lower;
   int32_t upper = (tmp_imm - (int32_t)lower) >> 32;

   // Load upper 32 bits
   int64_t up = upper, lo = upper;
   lo = (lo << 52) >> 52;
   up -= lo;
   up = (int32_t)up;
   lui(Rd, up);
   addi(Rd, Rd, lo);

   // Load the rest 32 bits.
   slli(Rd, Rd, 12);
   addi(Rd, Rd, (int32_t)lower >> 20);
   slli(Rd, Rd, 12);
   lower = ((int32_t)imm << 12) >> 20;
   addi(Rd, Rd, lower);
   slli(Rd, Rd, 8);
   lower = imm & 0xff;
   addi(Rd, Rd, lower);
}

void Assembler::li32(Register Rd, int32_t imm) {
  // int32_t is in range 0x8000 0000 ~ 0x7fff ffff, and imm[31] is the sign bit
  int64_t upper = imm, lower = imm;
  lower = (imm << 20) >> 20;
  upper -= lower;
  upper = (int32_t)upper;
  // lui Rd, imm[31:12] + imm[11]
  lui(Rd, upper);
  // use addiw to distinguish li32 to li64
  addiw(Rd, Rd, lower);
}

#define INSN(NAME, REGISTER)                                       \
  void Assembler::NAME(const address &dest, Register temp) {       \
    assert_cond(dest != NULL);                                     \
    int64_t distance = dest - pc();                                \
    if (is_imm_in_range(distance, 20, 1)) {                        \
      jal(REGISTER, distance);                                     \
    } else {                                                       \
      assert(temp != noreg, "temp must not be empty register!");   \
      int32_t offset = 0;                                          \
      movptr_with_offset(temp, dest, offset);                      \
      jalr(REGISTER, temp, offset);                                \
    }                                                              \
  }                                                                \
  void Assembler::NAME(Label &l, Register temp) {                  \
    jal(REGISTER, l, temp);                                        \
  }                                                                \

  INSN(j,   x0);
  INSN(jal, x1);

#undef INSN

#define INSN(NAME, REGISTER)                                       \
  void Assembler::NAME(Register Rs) {                              \
    jalr(REGISTER, Rs, 0);                                         \
  }

  INSN(jr,   x0);
  INSN(jalr, x1);

#undef INSN

void Assembler::ret() {
  jalr(x0, x1, 0);
}

#define INSN(NAME, REGISTER)                                      \
  void Assembler::NAME(const address &dest, Register temp) {      \
    assert_cond(dest != NULL);                                    \
    assert(temp != noreg, "temp must not be empty register!");    \
    int64_t distance = dest - pc();                               \
    if (is_offset_in_range(distance, 32)) {                       \
      auipc(temp, distance + 0x800);                              \
      jalr(REGISTER, temp, ((int32_t)distance << 20) >> 20);      \
    } else {                                                      \
      int32_t offset = 0;                                         \
      movptr_with_offset(temp, dest, offset);                     \
      jalr(REGISTER, temp, offset);                               \
    }                                                             \
  }

  INSN(call, x1);
  INSN(tail, x0);

#undef INSN

#define INSN(NAME, REGISTER)                                   \
  void Assembler::NAME(const Address &adr, Register temp) {    \
    switch(adr.getMode()) {                                    \
      case Address::literal: {                                 \
        code_section()->relocate(pc(), adr.rspec());           \
        NAME(adr.target(), temp);                              \
        break;                                                 \
      }                                                        \
      case Address::base_plus_offset:{                         \
        int32_t offset = 0;                                    \
        baseOffset(temp, adr, offset);                         \
        jalr(REGISTER, temp, offset);                          \
        break;                                                 \
      }                                                        \
      default:                                                 \
        ShouldNotReachHere();                                  \
    }                                                          \
  }

  INSN(j,    x0);
  INSN(jal,  x1);
  INSN(call, x1);
  INSN(tail, x0);

#undef INSN

void Assembler::wrap_label(Register r1, Register r2, Label &L, compare_and_branch_insn insn,
                           compare_and_branch_label_insn neg_insn, bool is_far) {
  if (is_far) {
    Label done;
    (this->*neg_insn)(r1, r2, done, /* is_far */ false);
    j(L);
    bind(done);
  } else {
    if (L.is_bound()) {
      (this->*insn)(r1, r2, target(L));
    } else {
      L.add_patch_at(code(), locator());
      (this->*insn)(r1, r2, pc());
    }
  }
}

void Assembler::wrap_label(Register Rt, Label &L, Register tmp, load_insn_by_temp insn) {
  if (L.is_bound()) {
    (this->*insn)(Rt, target(L), tmp);
  } else {
    L.add_patch_at(code(), locator());
    (this->*insn)(Rt, pc(), tmp);
  }
}

void Assembler::wrap_label(Register Rt, Label &L, jal_jalr_insn insn) {
  if (L.is_bound()) {
    (this->*insn)(Rt, target(L));
  } else {
    L.add_patch_at(code(), locator());
    (this->*insn)(Rt, pc());
  }
}

void Assembler::movptr_with_offset(Register Rd, address addr, int32_t &offset) {
  uintptr_t imm64 = (uintptr_t)addr;
#ifndef PRODUCT
  {
    char buffer[64];
    snprintf(buffer, sizeof(buffer), "0x%" PRIx64, imm64);
    block_comment(buffer);
  }
#endif
  assert(is_unsigned_imm_in_range(imm64, 47, 0) || (imm64 == (uintptr_t)-1), "48-bit overflow in address constant");
  // Load upper 32 bits
  int32_t imm = imm64 >> 16;
  int64_t upper = imm, lower = imm;
  lower = (lower << 52) >> 52;
  upper -= lower;
  upper = (int32_t)upper;
  lui(Rd, upper);
  addi(Rd, Rd, lower);

  // Load the rest 16 bits.
  slli(Rd, Rd, 11);
  addi(Rd, Rd, (imm64 >> 5) & 0x7ff);
  slli(Rd, Rd, 5);

  // Here, remove the addi instruct and return the offset directly. This offset will be used by following jalr/ld.
  offset = imm64 & 0x1f;
}

void Assembler::movptr(Register Rd, uintptr_t imm64) {
  movptr(Rd, (address)imm64);
}

void Assembler::movptr(Register Rd, address addr) {
  int offset = 0;
  movptr_with_offset(Rd, addr, offset);
  addi(Rd, Rd, offset);
}

void Assembler::ifence() {
  fence_i();
  if (UseConservativeFence) {
    fence(ir, ir);
  }
}

#define INSN(NAME, NEG_INSN)                                                         \
  void Assembler::NAME(Register Rs, Register Rt, const address &dest) {              \
    NEG_INSN(Rt, Rs, dest);                                                          \
  }                                                                                  \
  void Assembler::NAME(Register Rs, Register Rt, Label &l, bool is_far) {            \
    NEG_INSN(Rt, Rs, l, is_far);                                                     \
  }

  INSN(bgt,  blt);
  INSN(ble,  bge);
  INSN(bgtu, bltu);
  INSN(bleu, bgeu);
#undef INSN

#undef __

Address::Address(address target, relocInfo::relocType rtype) : _base(noreg), _offset(0), _mode(literal) {
  _target = target;
  switch (rtype) {
    case relocInfo::oop_type:
    case relocInfo::metadata_type:
      // Oops are a special case. Normally they would be their own section
      // but in cases like icBuffer they are literals in the code stream that
      // we don't have a section for. We use none so that we get a literal address
      // which is always patchable.
      break;
    case relocInfo::external_word_type:
      _rspec = external_word_Relocation::spec(target);
      break;
    case relocInfo::internal_word_type:
      _rspec = internal_word_Relocation::spec(target);
      break;
    case relocInfo::opt_virtual_call_type:
      _rspec = opt_virtual_call_Relocation::spec();
      break;
    case relocInfo::static_call_type:
      _rspec = static_call_Relocation::spec();
      break;
    case relocInfo::runtime_call_type:
      _rspec = runtime_call_Relocation::spec();
      break;
    case relocInfo::poll_type:
    case relocInfo::poll_return_type:
      _rspec = Relocation::spec_simple(rtype);
      break;
    case relocInfo::none:
      _rspec = RelocationHolder::none;
      break;
    default:
      ShouldNotReachHere();
  }
}
